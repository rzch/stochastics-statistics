% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "report" class.

\documentclass[11pt]{report} % use larger type; default would be 10pt

\usepackage[dark, book]{mystyle}

%%% END Article customizations

%%% The "real" document content comes below...
\lhead{\chaptertitle}\chead{}\rhead{\thetitle}
\lfoot{}\cfoot{\thepage}\rfoot{}

\pretitle{\begin{center}\Huge\bf}
\posttitle{\normalfont\end{center}}
\title{Stochastics \& Statistics}
\author{R. C.}
%\date{\vspace{-5ex}} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

%for PDF authoring
\hypersetup{
    pdftitle={\thetitle},
    pdfauthor={\theauthor},
%    pdfsubject={},
%    pdfkeywords={keyword1, keyword2},
    bookmarksnumbered=true,     
    bookmarksopen=false,         
    bookmarksopenlevel=1,       
	hidelinks=true,         
    pdfstartview=,           
    pdfpagemode=UseNone,
%    pdfpagelayout=TwoPageRight
}


\begin{document}
\maketitle
\thispagestyle{fancy}

\tableofcontents

\part{Fundamentals}

\chapter{Introductory Probability}

\section{Probability Laws}

\subsection{Events}

When expressing probabilities, we express probabilities of occurrences of events. Notionally, we can think of some random `experiment' which leads to outcomes. Certain outcomes may be associated with a particular event, so we can represent an event as a set which contains as its elements the outcomes associated with it.

\subsubsection{Union}

Given events $A$ and $B$, the event of $A$ or $B$ occurring is given by the set union $A \cup B$.

\subsubsection{Intersection}

Given events $A$ and $B$, the event of $A$ and $B$ both occurring is given by the set intersection $A \cap B$.

\subsubsection{Complement}

The complementary event of $A$ is denoted $\overline{A}$. This is the event that $A$ does not happen. \\

The relative complement of event $A$ with respect to event $B$ is denoted $B\setminus A$. This is the event of $B$ occurring, but not $A$ occurring. We have the relation
\begin{equation}
B\setminus A = B\cap \overline{A}
\end{equation}

\subsubsection{Venn Diagrams}

Venn diagrams provide a way to visualise sets and events.

\begin{center}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,228.46875); %set diagram left start at 0, and has height of 228.46875

\draw    (153, 22.47) rectangle (466.5, 199.47)   ;
\draw    (270.7, 109.07) circle [x radius= 68.49, y radius= 63.6]  ;
\draw    (346.75, 111.97) circle [x radius= 68.49, y radius= 63.6]  ;

\draw (247,111) node   {$A$};
\draw (375,108) node   {$B$};
\draw (309,110) node   {$A\cap B$};
\draw (421,44) node   {$\overline{\left(A\cup B\right)}$};
\end{tikzpicture}
\end{center}


\subsection{Classical Definition of Probability}

In an experiment with outcomes given by the set $\Omega$, the outcomes associated with event $A$ is a subset of $\Omega$ (ie. $A \subseteq \Omega$). Suppose we have a way to count these outcomes. Let $\left|\Omega\right|$ be the number of outcomes in $\Omega$, and similarly let $\left|A\right|$ be the number of outcomes in $A$. Then by the classical definition of probability, the probability of event $A$, denoted $\operatorname{Pr}\left(A\right)$, is
\begin{equation}
\operatorname{Pr}\left(A\right) = \dfrac{\left|A\right|}{\left|\Omega\right|}
\end{equation}

This means that probabilities are always between $0$ and $1$ (inclusive). We can interpret this probability by saying if we were able to conduct an infinite number of experiments, then the proportion of experiments which resulted in the occurrence of $A$ would be $\operatorname{Pr}\left(A\right)$.

\subsection{Addition Rule of Probability}

The probability of the intersection of $A$ and $B$ is written $\operatorname{Pr}\left(A \cap B\right)$, or alternatively may be denoted $\operatorname{Pr}\left(A, B\right)$. To obtain the probability of the union between $A$ and $B$, we have the formula (called the addition rule of probability):
\begin{equation}
\operatorname{Pr}\left(A \cup B\right) = \operatorname{Pr}\left(A\right) + \operatorname{Pr}\left( B\right) - \operatorname{Pr}\left(A \cap B\right) 
\end{equation}

\subsection{Complementary Probabilities}

The complementary probability of $\operatorname{Pr}\left(A\right)$ is denoted $\operatorname{Pr}\left(\overline{A}\right)$. We have that
\begin{equation}
\operatorname{Pr}\left(\overline{A}\right) = 1 - \operatorname{Pr}\left(A\right)
\end{equation}

\subsection{Mutual Exclusivity}

Events $A$ and $B$ are mutually exclusive if they cannot both happen together. That means
\begin{equation}
\operatorname{Pr}\left(A \cap B \right) = 0
\end{equation}
Hence the addition rule of probability in the case of mutual exclusivity simplifies to
\begin{equation}
\operatorname{Pr}\left(A \cup B\right) = \operatorname{Pr}\left(A\right) + \operatorname{Pr}\left( B\right)
\end{equation}

\subsection{Conditional Probability}

The probability of event $A$ occurring given event $B$ has occurred is denoted by $\operatorname{Pr}\left(A\middle|B\right)$ and is known as a conditional probability. The conditional probability can be calculated by
\begin{equation}
\operatorname{Pr}\left(A\middle|B\right) = \dfrac{\operatorname{Pr}\left(A \cap B\right)}{\operatorname{Pr}\left(B\right)}
\end{equation}
The term $\operatorname{Pr}\left(B\right)$ can be thought of as a `normalising constant' for $\operatorname{Pr}\left(A \cap B\right)$, as we only want to consider the space of outcomes where $B$ has occurred.

\subsection{Chain Rule of Probability}

Given the conditional probability $\operatorname{Pr}\left(A\middle|B\right)$ and the marginal probability $\operatorname{Pr}\left(B\right)$, the chain rule of probability says that the `joint' probability $\operatorname{Pr}\left(A\cap B\right)$ is equal to
\begin{equation}
\operatorname{Pr}\left(A\cap B\right) = \operatorname{Pr}\left(A\middle|B\right)\operatorname{Pr}\left(B\right)
\end{equation}

\subsection{Law of Total Probability}

The law of total probability gives a way to write the probability of an event $B$ in terms of its conditional probabilities, using the chain rule of probability. For events $A$ and $B$, the law of total probability says that
\begin{equation}
\operatorname{Pr}\left(B\right) = \operatorname{Pr}\left(A\right)\operatorname{Pr}\left(B\middle|A\right) + \operatorname{Pr}\left(\overline{A}\right)\operatorname{Pr}\left(B\middle|\overline{A}\right)
\end{equation}

\subsubsection{Probability Tree Diagrams}
A probability tree diagram for two events $A$, $B$ is given below.
\begin{center}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,228.84375); %set diagram left start at 0, and has height of 228.84375
\draw    (119.5,117.84) -- (231.5,64.84) ;

\draw    (119.5,117.84) -- (229.5,175.84) ;

\draw    (273.5,56.84) -- (389.5,26.84) ;

\draw    (273.5,56.84) -- (388.5,79.84) ;

\draw    (272.5,177.84) -- (388.5,147.84) ;
\draw    (272.5,177.84) -- (387.5,200.84) ;

\draw (253,58) node   {$A$};
\draw (253,177) node   {$\overline{A}$};
\draw (161,80) node [rotate=-334.26]  {$\operatorname{Pr}\left(A\right)$};
\draw (159,155) node [rotate=-28.56]  {$\operatorname{Pr}\left(\overline{A}\right)$};
\draw (403,200) node   {$\overline{B}$};
\draw (403,148) node   {$B$};
\draw (403,80) node   {$\overline{B}$};
\draw (403,28) node   {$B$};
\draw (329,28) node [rotate=-343.54]  {$\operatorname{Pr}\left(B\middle|A\right)$};
\draw (329,80) node [rotate=-11.43]  {$\operatorname{Pr}\left(\overline{B}\middle|A\right)$};
\draw (332,150) node [rotate=-343.54]  {$\operatorname{Pr}\left(B\middle|\overline{A}\right)$};
\draw (328,202) node [rotate=-11.43]  {$\operatorname{Pr}\left(\overline{B}\middle|\overline{A}\right)$};
\draw (530,28) node   {$\operatorname{Pr}\left(A\cap B\right) = \operatorname{Pr}\left(A\right)\operatorname{Pr}\left(B\middle|A\right)$};
\draw (530,80) node   {$\operatorname{Pr}\left(A\cap \overline{B}\right) = \operatorname{Pr}\left(A\right)\operatorname{Pr}\left(\overline{B}\middle|A\right)$};
\draw (530,148) node   {$\operatorname{Pr}\left(\overline{A}\cap B\right) = \operatorname{Pr}\left(\overline{A}\right)\operatorname{Pr}\left(B\middle|\overline{A}\right)$};
\draw (530,200) node   {$\operatorname{Pr}\left(\overline{A}\cap \overline{B}\right) = \operatorname{Pr}\left(\overline{A}\right)\operatorname{Pr}\left(\overline{B}\middle|\overline{A}\right)$};
\end{tikzpicture}

\end{center}
Probability tree diagrams allow for the law of total probability to be visualised. One can think of obtaining $\operatorname{Pr}\left(B\right)$ across all branches to the tree to find the joint probabilities, then summing over the probabilities which are favourable to event $B$.

\subsection{Bayes' Theorem}

By definitions of conditional probability, we have for two events $A$, $B$:
\begin{gather}
\operatorname{Pr}\left(A\middle|B\right) = \dfrac{\operatorname{Pr}\left(A\cap B\right)}{\operatorname{Pr}\left(B\right)} \\
\operatorname{Pr}\left(B\middle|A\right) = \dfrac{\operatorname{Pr}\left(A\cap B\right)}{\operatorname{Pr}\left(A\right)}
\end{gather}
Rearranging both gives
\begin{gather}
\operatorname{Pr}\left(A\cap B\right) = \operatorname{Pr}\left(A\middle|B\right)\operatorname{Pr}\left(B\right) \\
\operatorname{Pr}\left(A\cap B\right) = \operatorname{Pr}\left(B\middle|A\right)\operatorname{Pr}\left(A\right)
\end{gather}
Then equating them gives Bayes' theorem
\begin{gather}
\operatorname{Pr}\left(A\middle|B\right)\operatorname{Pr}\left(B\right) = \operatorname{Pr}\left(B\middle|A\right)\operatorname{Pr}\left(A\right) \\
\operatorname{Pr}\left(A\middle|B\right) = \dfrac{\operatorname{Pr}\left(B\middle|A\right)\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(B\right)}
\end{gather}
where $\operatorname{Pr}\left(A\middle|B\right)$ is known as the posterior probability, $\operatorname{Pr}\left(A\right)$ is known as the prior probability, $\operatorname{Pr}\left(B\middle|A\right)$ is known as the likelihood and $\operatorname{Pr}\left(B\right)$ is known as the marginal likelihood.

\subsection{DeMorgan's Laws}

DeMorgan's laws can be arrived at via some logical postulates.
\begin{itemize}
\item The event of $A$ or $B$ not happening is the same as neither $A$ happening nor $B$ happening.
\item The event that $A$ and $B$ both do not happen is the same as either $A$ not happening or $B$ not happening. 
\end{itemize}
This can be written down using set notation:
\begin{gather}
\overline{A \cup B} = \overline{A} \cap \overline{B} \\
\overline{A \cap B} = \overline{A} \cup \overline{B}
\end{gather}
And in terms of probability:
\begin{gather}
\operatorname{Pr}\left(\overline{A \cup B}\right) = \operatorname{Pr}\left(\overline{A} \cap \overline{B}\right) \\
\operatorname{Pr}\left(\overline{A \cap B}\right) = \operatorname{Pr}\left(\overline{A} \cup \overline{B}\right)
\end{gather}
Or by using complements:
\begin{gather}
1 - \operatorname{Pr}\left(A \cup B\right) = \operatorname{Pr}\left(\overline{A} \cap \overline{B}\right) \\
1 - \operatorname{Pr}\left(A \cap B\right) = \operatorname{Pr}\left(\overline{A} \cup \overline{B}\right)
\end{gather}

\section{Introductory Combinatorics}

In the context of probability, combinatorics allows us to count the number of outcomes, relating to both the number of outcomes in an experiment, and the number of outcomes favourable to an event.

\subsection{Arrangements}

\subsubsection{Homogeneous Sequences}
Consider a sequence of `symbols' that can be made up of an `alphabet' of size $n$. Then for a sequence of length $m$, there are $n^{m}$ different ways to form such sequences without restriction.

\subsubsection{Inhomogeneous Sequences}
Consider a sequence of symbols where the $j$\textsuperscript{th} symbol in the sequences comes from an alphabet of size $n_{j}$. Then for a sequence of length $m$, there are $n_{1}\times n_{2}\times\dots\times n_{m}$ different ways to form such sequences without restriction. This can be used to calculate the total number of outcomes at the end of a probability tree for example.

\subsubsection{Arrangements in a Line}
Consider the number of different ways to order $n$ different symbols. There are $n$ possibilities for the first symbol, $n - 1$ possibilities for the second symbol, etc. Hence there are
\begin{equation}
n\times\left(n - 1\right)\times \dots\times 2\times 1 = n!
\end{equation}
different ways.

\subsubsection{Arrangements in a Circle}

Consider the number of different ways to arrange $n$ distinct objects in a circle (where it only matters which object is next to which). The first object can be placed in any arbitrary position. There are $n - 1$ possibilities for the second object placed next to the first object, and $n - 2$ possibilities for the third object places next to the second object, etc. Hence there are
\begin{equation}
\left(n - 1\right)\times \dots\times 2\times 1 = \left(n - 1\right)!
\end{equation}
different ways. Note that if it did matter which position in the circle each object was placed, the number of arrangements becomes the same as the number of arrangements in a line.

\subsection{Permutations}

Consider the number of different ways to pick $k$ objects from a pool of $n$ distinct objects, where the order in which they are picked matters. There are $n$ possibilities for the first pick, $n - 1$ possibilities for the second pick, up to $n - k + 1$ possibilities for the $k$\textsuperscript{th} pick. Hence there are
\begin{equation}
n\times\left(n - 1\right)\times \dots\times \left(n - k + 1\right) = \dfrac{n!}{\left(n - k\right)!}
\end{equation}
different ways. This is called the number of permutations, and may be denoted as $\mathstrut^{n}\mathsf{P}_{k}$. This also gives the number of different ways to arrange $k$ distinct objects among $\left(n - k\right)$ other homogeneous objects in a line (ie. there are $n$ objects total). This is because there are $n$ possible locations for the first object, up to $n - k + 1$ possible locations for the $k$\textsuperscript{th} object.

\subsubsection{Permutations in a Circle}

With $n$ total objects, consider the number of different ways to arrange $k$ distinct objects among $\left(n - k\right)$ other homogeneous objects in a circle (where it only matters which object is next to which). The first object can be placed in any arbitrary position. There are $n - 1$ possible positions for where the second object can be placed relative to first object, $n - 2$ possible positions for where the third object can be placed relative to first object, up to $n - k + 1$ possible positions for where the $k$\textsuperscript{th} object can be placed relative to first object. Hence there are
\begin{equation}
\left(n - 1\right)\times \dots\times \left(n - k + 1\right) = \dfrac{\left(n - 1\right)!}{\left(n - k\right)!}
\end{equation}
different ways. Note that we can arrive at this number by counting $\mathstrut^{n}\mathsf{P}_{k}$ permutations in a circle where absolute position 
does matter (equivalent to permutations in a line), but then dividing by the possible number of rotations in a circle without affecting relative positions, which is $n$.

\subsection{Combinations}

\subsubsection{Binomial Coefficient}

Consider the number of different ways to pick $k$ objects from a pool of $n$ distinct objects, where the order in which they are picked does not matter (ie. it only matters which group of $k$ are picked). We know that there are $\mathstrut^{n}\mathsf{P}_{k}$ different ways to pick $k$ objects when order does matter, but this number counts all the $k!$ different ways for every possible grouping of $k$ objects from the pool. So dividing the number of permutations by $k!$ gives
\begin{equation}
\dfrac{\mathstrut^{n}\mathsf{P}_{k}}{k!} = \dfrac{n!}{k!\left(n - k\right)!}
\end{equation}
which is the number of different groups of $k$ can be picked from $n$ distinct objects. This is known as the binomial coefficient, and denoted $\mathstrut^{n}\mathsf{C}_{k}$, which may also be read as "$n$ choose $k$". It is also sometimes denoted
\begin{equation}
\mathstrut^{n}\mathsf{C}_{k} = \begin{pmatrix}n\\k\end{pmatrix}
\end{equation}
This number can also be thought of as the number of different ways to arrange $k$ homogeneous objects among $\left(n - k\right)$ other homogeneous objects in a line. Note that the following properties hold for the binomial coefficient:
\begin{equation}
\mathstrut^{n}\mathsf{C}_{k} = \mathstrut^{n}\mathsf{C}_{n - k}
\end{equation}
which says that asking for the number of ways to choose $k$ from $n$ is no different to asking for the number of ways to choose $n - k$ from $n$. Also
\begin{equation}
\mathstrut^{n}\mathsf{C}_{0} + \mathstrut^{n}\mathsf{C}_{1} + \dots + \mathstrut^{n}\mathsf{C}_{n} = 2^{n}
\end{equation}
since there are $2^{n}$ different binary sequences of length $n$, and the above sum exhaustively enumerates through all combinations of binary sequences from when there are zero `$1$'s up to $n$ `$1$'s.

\subsubsection{Multinomial Coefficient}

As a generalisation to the binomial coefficient, suppose there are $n$ objects and $m$ different groups, with the numbers of homogeneous objects in each group given by $k_{1}, k_{2}, \dots, k_{m}$ and so we have
\begin{equation}
k_{1} + k_{2} + \dots + k_{m} = n
\end{equation}
To derive the number of different ways these objects can be arranged, first consider the number of ways to arrange only the first group (which has $k_{1}$ members), without regard for the other groups (call it the `remainder' group). By the binomial coefficient, there are
\begin{equation}
\mathstrut^{n}\mathsf{C}_{k_{1}} = \dfrac{n!}{k_{1}!\left(n - k_{1}\right)!}
\end{equation}
ways. Then for every one of these arrangements, there are $\mathstrut^{n - k_{1}}\mathsf{C}_{k_{2}}$ ways to arrange members of group $2$ among the remainder. Hence there are
\begin{equation}
\mathstrut^{n}\mathsf{C}_{k_{1}}\times\mathstrut^{n - k_{1}}\mathsf{C}_{k_{2}} = \dfrac{n!}{k_{1}!\left(n - k_{1}\right)!}\times\dfrac{\left(n - k_{1}\right)!}{k_{2}!\left(n - k_{1} - k_{2}\right)!}
\end{equation}
different ways to arrange objects in groups $1$ and $2$, without regard for the others. By continuing this process of induction, we find that
\begin{multline}
\mathstrut^{n}\mathsf{C}_{k_{1}}\times\mathstrut^{n - k_{1}}\mathsf{C}_{k_{2}}\times\dots\times\mathstrut^{n - k_{1} - \dots - k_{m - 1}}\mathsf{C}_{k_{m}} = \dfrac{n!}{k_{1}!\left(n - k_{1}\right)!}\times\dfrac{\left(n - k_{1}\right)!}{k_{2}!\left(n - k_{1} - k_{2}\right)!}\times\dots \\
\times \dfrac{\left(n - k_{1} - \dots - k_{m - 2}\right)!}{k_{m - 1}!\left(n - k_{1} - \dots - k_{m - 1}\right)!}\times\dfrac{\left(n - k_{1} - \dots - k_{m - 1}\right)!}{k_{m}!0!}
\end{multline}
gives the number of different ways to arrange all objects in all groups. By cancellation of terms, the multinomial coefficient can be denoted as
\begin{equation}
\begin{pmatrix} n \\ k_{1}, \dots, k_{m}\end{pmatrix} = \dfrac{n!}{k_{1}!k_{2}!\dots k_{m}!}
\end{equation}
This number can also be thought of as the number of ways of depositing $n$ distinct objects into $m$ bins, where the $j$\textsuperscript{th} bin requires $k_{j}$ objects.

\subsubsection{Combinations in a Circle}

With $n$ total objects, consider the number of different ways to arrange $k$ homogeneous objects among $\left(n - k\right)$ other homogeneous objects in a circle (where it only matters which object is next to which). Like with arrangements and permutations in a circle, we can find this number by dividing the total number of combinations in a line by the number of possible rotations without affecting relative position, which is $n$. This gives
\begin{equation}
\dfrac{\mathstrut^{n}\mathsf{C}_{k}}{n} = \dfrac{\left(n - 1\right)!}{k!\left(n - k\right)!}
\end{equation}

\section{Probability Distributions}

\subsection{Random Variables}

Random variables are variables which are determined as a result of a random experiment, ie. they are not fixed and will depend on the outcome of the random experiment. This is in contrast to `deterministic' variables, which are always fixed regardless of the outcome. Deterministic variables can even be thought to be a special case of random variables, which are determined at the end of the random experiment but always take on the same value. \\

The value of a random variable after the experiment has occurred is said to have been `realised'.

\subsubsection{Degenerate Random Variables}

A degenerate random variable takes on a single value with probability $1$, and thus can be regarded as a deterministic variable.

\subsubsection{Binary Random Variables}

Binary random variables are random variables which can take on either of two possible values, usually chosen to be $0$ and $1$.

\subsubsection{Indicator Random Variables}

An indicator random variable for an event $A$ is a binary random variable that takes on the value of $1$ if $A$ occurred, and $0$ if $A$ did not occur. An indicator for $A$ can be denoted by $\mathbb{I}_{A}$.

\subsubsection{Simple Random Variables}

A simple random variable is a weighted sum of indicator random variables for mutually exclusive events. Suppose $A_{1}, \dots, A_{n}$ are $n$ mutually exclusive events. Then a simple random variable $X$ is
\begin{equation}
X = \sum_{i = 1}^{n}a_{i}\mathbb{I}_{A_{i}}
\end{equation}
where $a_{1}, \dots, a_{n}$ are the weights.

\subsection{Distribution Functions}

The possible values which a random variable can take, as well as probabilities of random variables taking on certain values, is represented using a distribution function.

\subsubsection{Discrete Distributions}

For a random variable $X$ that takes on discrete values (eg. $0, 1, 2, \dots$), the probability mass function describes the probability of $X$ being equal to a particular value, denoted $\operatorname{Pr}\left(X = x\right)$. Note that for random experiments with discrete outcomes which do not explicitly result in the realisation of a random variable, a discrete random variable can still be defined by assigning each outcome to the set of integers (and if an integer $x$ is not assigned to any outcome, this simply means $\operatorname{Pr}\left(X = x\right) = 0$). In this way, a probability mass function can still be defined for any random experiment with discrete outcomes. A probability mass function satisfies the following properties:
\begin{equation}
\sum_{x=-\infty}^{\infty}\operatorname{Pr}\left(X = x\right) = 1
\end{equation}
and
\begin{equation}
0 \leq \operatorname{Pr}\left(X = x\right) \leq 1
\end{equation}
for all integers $x$.

\subsubsection{Continuous Distributions}

In some random experiments, a random variable can take on a continuum of values (eg. $\left[0, 1\right]$ or $\left(-\infty, \infty\right)$). These are called continuous random variables. A continuous random variable $X$ has a probability density function, denoted $f_{X}\left(x\right)$. Integrating this function over regions gives the probability that $X$ will lie in that region. For example, the probability that $a \leq X \leq b$ is given by
\begin{equation}
\operatorname{Pr}\left(a \leq X \leq b\right) = \int_{a}^{b}f_{X}\left(x\right)dx
\end{equation}
Note that is does not matter if this is integrated with open intervals or closed intervals. Hence for a continuous random variable, $\operatorname{Pr}\left(a \leq X \leq b\right) = \operatorname{Pr}\left(a < X < B\right)$. Also, the probability that $X$ takes on any one particular value is zero, ie. $\operatorname{Pr}\left(X = x\right) = 0$. That means it only makes sense to talk about $X$ lying within regions, rather than for specific points. A probability density function satisfies the following properties:
\begin{equation}
\int_{-\infty}^{\infty}f_{X}\left(x\right)dx = 1
\end{equation}
and
\begin{equation}
f_{X}\left(x\right) \geq 0
\end{equation}
for all $x\in\left(-\infty, \infty\right)$. Note that unlike probability masses, probability densities are allowed to be greater than $1$.

\subsubsection{Cumulative Distributions}

A cumulative distribution function $F_{X}\left(x\right)$ for a random variable $X$ is defined as a function for the `cumulative' probability up to $x$:
\begin{equation}
F_{X}\left(x\right) = \operatorname{Pr}\left(X \leq x\right)
\end{equation}
Thus if $X$ is a discrete random variable, then the relation between the cumulative distribution function and the probability mass function is given by
\begin{equation}
F_{X}\left(a\right) = \sum_{x = -\infty}^{a}\operatorname{Pr}\left(X = x\right)
\end{equation}
Whereas if $X$ is a continuous random variable, then the relation between the cumulative distribution function and the probability density function is given by
\begin{equation}
F_{X}\left(a\right) = \int_{-\infty}^{a}f_{X}\left(x\right)dx
\end{equation}
The cumulative distribution satisfies the following properties:
\begin{equation}
0 \leq F_{X}\left(x\right) \leq 1
\end{equation}
for all $x$, and
\begin{gather}
\lim_{x\to -\infty}F_{X}\left(x\right) = 0 \\
\lim_{x\to \infty}F_{X}\left(x\right) = 1
\end{gather}
Furthermore, the probability density function can be obtained from the cumulative density function by
\begin{equation}
f_{X}\left(x\right) = \dfrac{dF_{X}\left(x\right)}{dx}
\end{equation}
(wherever the cumulative density function is differentiable) and if given the probability density function, then
\begin{equation}
F_{X}\left(x\right) = \int f_{X}\left(x\right)dx
\end{equation}
with a constant of integration such that $\lim_{x\to \infty}F_{X}\left(x\right) = 1$.

\subsubsection{Quantile Function}

The quantile function can roughly be thought of as the inverse of the cumulative distribution function. However, the cumulative distribution may not always be invertible. In that case, we can define the quantile function $Q\left(p\right)$ which takes $p\in\left[0, 1\right]$ as the smallest $x$ such that $F_{X}\left(x\right) = \operatorname{Pr}\left(X \leq x\right) \geq p$.

\subsection{Joint Distributions}

Consider a random experiment which produces two random variables, $X$ and $Y$. If these random variables are discrete (taking on integer values), we can express a joint probability mass function over all combinations of $X$ and $Y$, given by $\operatorname{Pr}\left(X = x, Y = y\right)$. This joint distribution has the property that
\begin{equation}
\sum_{x = -\infty}^{\infty}\sum_{y = -\infty}^{\infty}\operatorname{Pr}\left(X = x, Y = y\right) = 1
\end{equation}
We call $\operatorname{Pr}\left(X = x\right)$ and $\operatorname{Pr}\left(Y = y\right)$ the marginal distributions, which can be calculated from the joint distribution by a summation:
\begin{gather}
\operatorname{Pr}\left(X = x\right) = \sum_{y = -\infty}^{\infty}\operatorname{Pr}\left(X = x, Y = y\right) \\
\operatorname{Pr}\left(Y = y\right) = \sum_{x = -\infty}^{\infty}\operatorname{Pr}\left(X = x, Y = y\right)
\end{gather}
Analogously, if $X$ and $Y$ are continuous, then there exists a joint density function denoted $f_{XY}\left(x, y\right)$ with the relation
\begin{equation}
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{XY}\left(x, y\right)dxdy = 1
\end{equation}
and the marginal densities $f_{X}\left(x\right)$, $f_{Y}\left(y\right)$ given by the integrals
\begin{gather}
f_{X}\left(x\right) = \int_{-\infty}^{\infty}f_{XY}\left(x, y\right)dy \\
f_{Y}\left(y\right) = \int_{-\infty}^{\infty}f_{XY}\left(x, y\right)dx
\end{gather}
Note that while it is possible to obtain marginal distributions from the joint distribution (via the process known as `marginalisation'), it is generally not possible to obtain the joint distribution from the marginal distributions (with the exception of special cases), as there may be more than one joint distribution while would yield those marginal distributions.

\subsection{Conditional Distributions}

By the definition of the conditional probability, we know (for discrete $X$ and $Y$):
\begin{equation}
\operatorname{Pr}\left(X = x\middle|Y = y\right) = \dfrac{\operatorname{Pr}\left(X = x, Y = y\right)}{\operatorname{Pr}\left(Y = y\right)}
\end{equation}
this is known as the conditional distribution of $X$ given $Y$, and is itself a probability mass function which is dependent on the value of $Y$. Notice that it is the quotient between the joint distribution of $X, Y$ and the marginal distribution of $Y$, and is defined only when $\operatorname{Pr}\left(Y = y\right) > 0$, otherwise we can presume $\operatorname{Pr}\left(X = x\middle|Y = y\right) = 0$. Likewise, the conditional distribution of $Y$ given $X$ is
\begin{equation}
\operatorname{Pr}\left(Y = y\middle|X = x\right) = \dfrac{\operatorname{Pr}\left(X = x, Y = y\right)}{\operatorname{Pr}\left(X = x\right)}
\end{equation}
Using the chain rule of probability, we can relate the marginal distribution of $X$ to the conditional distribution of $X$ given $Y$ by
\begin{align}
\operatorname{Pr}\left(X = x\right) &= \sum_{y = -\infty}^{\infty}\operatorname{Pr}\left(X = x, Y = y\right)\\
&= \sum_{\left\{y: \operatorname{Pr}\left(Y = y\right) > 0\right\}}\operatorname{Pr}\left(X = x\middle|Y = y\right)\operatorname{Pr}\left(Y = y\right)
\end{align}
We can also show that the conditional distribution $\operatorname{Pr}\left(X = x\middle|Y = y\right)$ is a proper distribution (summing over $x$ to $1$) for any given $y$:
\begin{align}
\sum_{x = -\infty}^{\infty}\operatorname{Pr}\left(X = x\middle|Y = y\right) &= \sum_{x = -\infty}^{\infty}\dfrac{\operatorname{Pr}\left(X = x, Y = y\right)}{\operatorname{Pr}\left(Y = y\right)} \\
&= \dfrac{1}{\operatorname{Pr}\left(Y = y\right)}\sum_{x = -\infty}^{\infty}\operatorname{Pr}\left(X = x, Y = y\right) \\
&= \dfrac{\operatorname{Pr}\left(Y = y\right)}{\operatorname{Pr}\left(Y = y\right)} \\
&= 1
\end{align}
If $X$ and $Y$ are continuous, then there exists conditional densities $f_{X|Y}\left(x\middle|Y = y\right)$ and $f_{Y|X}\left(y\middle|X = x\right)$. These are similarly defined by
\begin{gather}
f_{X|Y}\left(x\middle|Y = y\right) = \dfrac{f_{XY}\left(x, y\right)}{f_{Y}\left(y\right)} \\
f_{Y|X}\left(y\middle|X = x\right) = \dfrac{f_{XY}\left(x, y\right)}{f_{X}\left(x\right)} 
\end{gather}
where we can show similar properties as with discrete distributions hold:
\begin{align}
f_{X}\left(x\right) &= \int_{-\infty}^{\infty}f_{XY}\left(x, y\right)dy \\
&= \int_{\left\{y: f_{Y}\left(y\right) > 0\right\}}f_{X|Y}\left(x\middle|Y = y\right)f_{Y}\left(y\right)dy
\end{align}
and
\begin{align}
\int_{-\infty}^{\infty}f_{X|Y}\left(x\middle|Y = y\right)dx &= \int_{-\infty}^{\infty}\dfrac{f_{XY}\left(x, y\right)}{f_{Y}\left(y\right)}dx \\
&= \dfrac{1}{f_{Y}\left(y\right)}\int_{-\infty}^{\infty}f_{XY}\left(x, y\right)dx \\
&= \dfrac{f_{Y}\left(y\right)}{f_{Y}\left(y\right)} \\
&= 1
\end{align}

\subsubsection{Conditional Random Variables}

A `conditional random variable' denoted $X|Y$ can be informally defined as the random variable which has its own distribution function the conditional distribution of $X$ given $Y$.

\subsection{Support of a Distribution}

The support of a distribution for a random variable is the set of values for which the probability density function (or probability mass function, in the case of discrete random variables) is non-zero. For example, we can denote the support of a discrete random variable $X$ as
\begin{equation}
\mathcal{X} := \left\{x: \operatorname{Pr}\left(X = x\right) > 0\right\}
\end{equation}

\section{Expectation}

The expectation (or expected value, or mean) of a random variable can be thought about in several ways:
\begin{itemize}
\item The average realisation of the random variable over an infinite number of trials.
\item The `center of mass' of the probability distribution.
\end{itemize}
The expected value of a random variable $X$ is a constant denoted $\mathbb{E}\left[X\right]$ and for a discrete random variable may be calculated as
\begin{equation}
\mathbb{E}\left[X\right] = \sum_{x = -\infty}^{\infty}x\operatorname{Pr}\left(X = x\right)
\end{equation}
If $X$ is a continuous random variable, then the expectation is calculated by
\begin{equation}
\mathbb{E}\left[X\right] = \int_{-\infty}^{\infty}xf_{X}\left(x\right)dx
\end{equation}
Expectations of functions of random variables (which are also random variables) are readily defined, for example some $g\left(X\right)$:
\begin{equation}
\mathbb{E}\left[g\left(X\right)\right] = \int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)dx
\end{equation}
If an experiment results in two (continuous) random variables $X$ and $Y$, we can also take the expectation of some function of the two random variables $g\left(X, Y\right)$ by
\begin{equation}
\mathbb{E}\left[g\left(X, Y\right)\right] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x, y\right)f_{XY}\left(x, y\right)dxdy
\end{equation}
and analogously for discrete random variables. Sometimes, particularly when there are multiple random variables present, an expectation may be taken with respect to a random variable, which expresses which marginal distribution to sum/integrate over. For example, with a continuous random variable $X$ and a function $g\left(X\right)$,the expectation of $g\left(X\right)$ with respect to $X$ is denoted with $\mathbb{E}_{X}\left[g\left(X\right)\right]$, and defined like above as
\begin{equation}
\mathbb{E}_{X}\left[g\left(X\right)\right] = \int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)dx
\end{equation}
This is more for emphasis and clarity rather than any technical difference, since $g\left(X\right)$ may be written using another symbol which does not make the connection with $X$ explicit.

\subsection{Linearity of Expectation}

Consider two random variables $X$, $Y$ and two constants $a$, $b$. The expectation of $aX + bY$ (supposing $X$ and $Y$ are discrete) is written as
\begin{align}
\mathbb{E}\left[aX + bY\right] &= \sum_{x}\sum_{y}\left(ax + by\right)\operatorname{Pr}\left(X = x, Y = y\right) \\
&= a\sum_{x}\sum_{y}x\operatorname{Pr}\left(X = x, Y = y\right) + b\sum_{x}\sum_{y}y\operatorname{Pr}\left(X = x, Y = y\right) \\
&= a\sum_{x}x\sum_{y}\operatorname{Pr}\left(X = x, Y = y\right) + b\sum_{y}y\sum_{x}\operatorname{Pr}\left(X = x, Y = y\right) \\
&= a\sum_{x}x\operatorname{Pr}\left(X = x\right) + b\sum_{y}y\operatorname{Pr}\left(Y = y\right) \\
&= a\mathbb{E}\left[X\right] + b\mathbb{E}\left[Y\right]
\end{align}
This analogously holds for continuous $X$ and $Y$. Since sums and integrals are linear operators, and the expectation is defined either in terms of sums or integrals, this gives the property known as the linearity of expectation.

\subsection{Conditional Expectation}

The conditional expectation of $X$ given $Y$ is denoted $\mathbb{E}\left[X\middle|Y\right]$, and may be interpreted in two ways:
\begin{itemize}
\item Given a particular value of $Y = y$, the conditional expectation $\mathbb{E}\left[X\middle|Y = y\right]$ is the average of $X$ over all events where $Y = y$. It may be calculated by taking the expectation of the conditional distribution of $X$ given $Y$, ie. for discrete random variables
\begin{equation}
\mathbb{E}\left[X\middle|Y = y\right] = \sum_{x}x\operatorname{Pr}\left(X = x\middle|Y = y\right)
\end{equation}
For continuous random variables,
\begin{equation}
\mathbb{E}\left[X\middle|Y = y\right] = \int_{-\infty}^{\infty}xf_{X|Y}\left(x\middle|Y = y\right)dx
\end{equation}
This form of the conditional expectation may be treated as a deterministic function in the realisation $y$.
\item The conditional expectation $\mathbb{E}\left[X\middle|Y\right]$ without being given a particular value of $Y$ is a random variable, because $Y$ is random. For discrete $X$ and $Y$, the random variable $\mathbb{E}\left[X\middle|Y\right]$ is defined to take on value $\mathbb{E}\left[X\middle|Y = y\right]$ with probability $\operatorname{Pr}\left(Y = y\right)$. For continuous $X$ and $Y$, the random variable $\mathbb{E}\left[X\middle|Y\right]$ has density $f_{X|Y}\left(x\middle|Y = y\right)$ at value $\mathbb{E}\left[X\middle|Y = y\right]$.
\end{itemize}
The conditional expectation of $X$ conditional on its own realisation $X = x$ becomes $x$, ie. $\mathbb{E}\left[X\middle|X = x\right] = x$, because given $X = x$, we expect that $X$ naturally can only be $x$. We can show this more formally (for the case of a discrete random variable $X$) by using the facts that $\operatorname{Pr}\left(X = x \middle| X = x\right) = 1$ and $\operatorname{Pr}\left(X = x' \middle| X = x\right) = 0$ when $x' \neq x$:
\begin{align}
\mathbb{E}\left[X\middle|X = x\right] &= \sum_{x'}x'\operatorname{Pr}\left(X = x' \middle| X = x\right) \\
&= x
\end{align}
It can then be reasoned that $\mathbb{E}\left[X\middle|X\right] = X$, because by the definition above $X$ is the random variable which takes on value $\mathbb{E}\left[X\middle|X = x\right] = x$ with probability $\operatorname{Pr}\left(X = x\right)$. Generalising this, we can say that for a function $f\left(\cdot\right)$, we have
\begin{equation}
\mathbb{E}\left[f\left(X\right)\middle|X = x\right] = f\left(x\right)
\end{equation}
and
\begin{equation}
\mathbb{E}\left[f\left(X\right)\middle|X\right] = f\left(X\right)
\end{equation}

\subsection{Law of Iterated Expectations}

The law of iterated expectations (also sometimes referred to the law of total expectation) states that the expectation of the random variable $\mathbb{E}\left[X\middle|Y\right]$ (over $Y$) is $\mathbb{E}\left[X\right]$. That is,
\begin{equation}
\mathbb{E}\left[\mathbb{E}\left[X\middle|Y\right]\right] = \mathbb{E}\left[X\right]
\end{equation}
Sometimes the law is written $\mathbb{E}_{Y}\left[\mathbb{E}_{X}\left[X\middle|Y\right]\right] = \mathbb{E}\left[X\right]$ to explicitly show which variables the expectations are taken with respect to. To prove the law for discrete random variables,
\begin{align}
\mathbb{E}\left[\mathbb{E}\left[X\middle|Y\right]\right] &= \sum_{y}\mathbb{E}\left[X\middle|Y = y\right]\operatorname{Pr}\left(Y = y\right) \\
&= \sum_{y}\sum_{x}x\operatorname{Pr}\left(X = x\middle|Y = y\right)\operatorname{Pr}\left(Y = y\right) \\
&= \sum_{y}\sum_{x}x\operatorname{Pr}\left(X = x, Y = y\right) \\
&= \mathbb{E}\left[X\right]
\end{align}
This is similarly shown for continuous random variables. The law of iterated expectations can also be extended to when there are more than two random variables. Some variations are:
\begin{gather}
\mathbb{E}\left[\mathbb{E}\left[X\middle|Y, Z\right]\right] = \mathbb{E}\left[X\right] \\
\mathbb{E}\left[\mathbb{E}\left[X\middle|Y, Z\right]\middle|Z\right] = \mathbb{E}\left[X\middle|Z\right]
\end{gather}

\subsection{Expectation of Indicator Random Variables}

Let $\mathbb{I}_{A}$ be an indicator random variable for event $A$. The expectation of $\mathbb{I}_{A}$ is the probability of $A$, as shown below.
\begin{align}
\mathbb{E}\left[\mathbb{I}_{A}\right] &= 0\times \operatorname{Pr}\left(\overline{A}\right) + 1\times \operatorname{Pr}\left(A\right) \\
&= \operatorname{Pr}\left(A\right)
\end{align}

\section{Variance}

The variance of a random variable $X$ measures the spread of the distribution of $X$ about its mean. Realisations of a random variable with high variance can be thought to deviate far from its mean on average, while realisations of a random variable with low variance are expected to be concentrated close to the mean. The variance of $X$ is defined as
\begin{equation}
\operatorname{Var}\left(X\right) = \mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)^{2}\right] 
\end{equation}
We can interpret $\left(X - \mathbb{E}\left[X\right]\right)^{2}$ as the squared deviation of $X$ from its mean, and so $\operatorname{Var}\left(X\right)$ is the expected squared deviation of $X$ from its mean. Another way to interpret the variance of a random variable is that it gives an idea of the `uncertainty'. That is, the more uncertain a random variable is, the more spread out its distribution becomes and so the harder it is to predict. An alternative formula for the variance is
\begin{equation}
\operatorname{Var}\left(X\right) = \mathbb{E}\left[X^{2}\right] - \mathbb{E}\left[X\right]^{2}
\end{equation}
This can be shown as follows. Starting by expanding the original definition given for the variance,
\begin{equation}
\operatorname{Var}\left(X\right) = \mathbb{E}\left[X^{2} - 2X\mathbb{E}\left[X\right] + \mathbb{E}\left[X\right]^{2}\right] 
\end{equation}
Then using the linearity of expectation:
\begin{equation}
\operatorname{Var}\left(X\right) = \mathbb{E}\left[X^{2}\right] - 2\mathbb{E}\left[X\mathbb{E}\left[X\right]\right] + \mathbb{E}\left[\mathbb{E}\left[X\right]^{2}\right] 
\end{equation}
Note that $\mathbb{E}\left[X\mathbb{E}\left[X\right]\right] = \mathbb{E}\left[X\right]^{2}$ and $\mathbb{E}\left[\mathbb{E}\left[X\right]^{2}\right] = \mathbb{E}\left[X\right]^{2}$ as $\mathbb{E}\left[X\right]$ is treated as a `constant' with respect to the distribution of $X$, and so can be taken outside of the expectation. Hence
\begin{align}
\operatorname{Var}\left(X\right) &= \mathbb{E}\left[X^{2}\right] - 2\mathbb{E}\left[X\right]^{2} + \mathbb{E}\left[X\right]^{2} \\
&= \mathbb{E}\left[X^{2}\right] - \mathbb{E}\left[X\right]^{2}
\end{align}
The variance is always non-negative, because $\left(X - \mathbb{E}\left[X\right]\right)^{2}$ is always non-negative. If $X$ is scaled by a constant $a$, then
\begin{align}
\operatorname{Var}\left(aX\right) &= \mathbb{E}\left[\left(aX - \mathbb{E}\left[aX\right]\right)^{2}\right] \\
&= a^{2}\mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)^{2}\right] \\
&= a^{2}\operatorname{Var}\left(X\right)
\end{align}

\subsection{Standard Deviation}

The units of the variance $\operatorname{Var}\left(X\right)$ is in the squared units of $X$. A quantity which measures a random variable's dispersion about its mean but in the same units as the random variable is the standard deviation. The standard deviation is defined as the square root of the variance, ie.
\begin{equation}
\operatorname{sd}\left(X\right) = \sqrt{\operatorname{Var}\left(X\right)}
\end{equation}
Like the variance, the standard deviation is also always non-negative. The standard deviation is useful because since it is in the same units as the random variable, it is more easily interpretable.

\subsection{Precision}

The precision of a random variable $X$ is defined as the inverse of the variance, ie. $\operatorname{Var}\left(X\right)^{-1} = \dfrac{1}{\operatorname{Var}\left(X\right)}$. Naturally, the more precise a random variable is, the more concentrated it is about its expected value.

\subsection{Covariance}

The covariance of a pair of random variables $X$ and $Y$ gives a measure of how closely the two random variables are related (in terms of their realisations). The covariance is defined by
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)\left(Y - \mathbb{E}\left[Y\right]\right)\right]
\end{equation}
This expectation is taken over the joint distribution of $X$ and $Y$. If a realisation of $X$ being `high' (ie. above the mean) means that a realisation of $Y$ is likely to also be high, and vice-versa ($Y$ being high means $X$ is likely to be high), and also that $X$ being low means $Y$ is likely to be low and vice-versa, then the sign of $\left(X - \mathbb{E}\left[X\right]\right)\left(Y - \mathbb{E}\left[Y\right]\right)$ is more likely to be positive and hence the covariance will be positive. We may say that the random variables `move together'. In the opposing case, ie. the sign of $\left(X - \mathbb{E}\left[X\right]\right)\left(Y - \mathbb{E}\left[Y\right]\right)$ is more likely to be negative, then the covariance will be negative and we can interpret the random variables as `moving against' each other. An alternative expression for the covariance is
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
\end{equation}
This can be shown by expanding the original definition given:
\begin{align}
\operatorname{Cov}\left(X, Y\right) &= \mathbb{E}\left[XY - \mathbb{E}\left[X\right]Y - X\mathbb{E}\left[Y\right] + \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]\right] \\
&= \mathbb{E}\left[XY\right] - \mathbb{E}\left[\mathbb{E}\left[X\right]Y\right] - \mathbb{E}\left[X\mathbb{E}\left[Y\right]\right] + \mathbb{E}\left[\mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]\right] \\
&= \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] + \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] \\
&= \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
\end{align}
where we have used the linearity of expectation and the facts that $\mathbb{E}\left[X\right]$ and $\mathbb{E}\left[Y\right]$ are treated as constant with respect to the joint distribution over $X$ and $Y$, so hence may be taken outside of the respective expectations. Also note that if we take the covariance between $X$ and itself:
\begin{align}
\operatorname{Cov}\left(X, X\right) &= \mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)^{2}\right] \\
&= \operatorname{Var}\left(X\right)
\end{align}
which becomes the variance of $X$.

\subsection{Variance of Sums}

For the sum of random variables $Z = X + Y$, the variance of $Z$ is given by
\begin{equation}
\operatorname{Var}\left(Z\right) = \operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(XY\right) + \operatorname{Var}\left(Y\right)
\end{equation}
This is shown using the properties of the expectation, and definitions of the variance and covariance.
\begin{align}
\operatorname{Var}\left(Z\right) &= \mathbb{E}\left[Z^{2}\right] - \mathbb{E}\left[Z\right]^{2} \\
&= \mathbb{E}\left[\left(X + Y\right)^{2}\right] - \mathbb{E}\left[X + Y\right]^{2} \\
&= \mathbb{E}\left[X^{2}\right] + 2\mathbb{E}\left[XY\right] + \mathbb{E}\left[Y^{2}\right] - \mathbb{E}\left[X\right]^{2} - 2\mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] - \mathbb{E}\left[Y\right]^{2} \\
&= \left(\mathbb{E}\left[X^{2}\right] - \mathbb{E}\left[X\right]^{2}\right) + 2\left(\mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]\right) + \left(\mathbb{E}\left[Y^{2}\right] - \mathbb{E}\left[Y\right]^{2}\right) \\
&= \operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(XY\right) + \operatorname{Var}\left(Y\right)
\end{align}

\subsection{Correlation}

The correlation coefficient is a `standardised' covariance. Given random variables $X$, $Y$ and their standard deviations $\operatorname{sd}\left(X\right)$ and $\operatorname{sd}\left(Y\right)$, the correlation coefficient between $X$ and $Y$ is defined as
\begin{equation}
\operatorname{Corr}\left(X, Y\right) = \dfrac{\operatorname{Cov}\left(X, Y\right)}{\operatorname{sd}\left(X\right)\operatorname{sd}\left(Y\right)}
\end{equation}

As the correlation coefficient is standardised, it always lies between $-1$ and $1$. We can show this follows for two random variables $X$ and $Y$. For ease of notation, denote $\mu_{X} := \mathbb{E}\left[X\right]$, $\mu_{Y} := \mathbb{E}\left[Y\right]$, $\sigma_{X} := \operatorname{sd}\left(X\right)$ and $\sigma_{Y} := \operatorname{sd}\left(Y\right)$. Define the `standardised' random variables
\begin{gather}
\widetilde{X} := \dfrac{X - \mu_{X}}{\sigma_{X}} \\
\widetilde{Y} := \dfrac{Y - \mu_{Y}}{\sigma_{Y}}
\end{gather}
Then start from the inequality
\begin{equation}
\mathbb{E}\left[\left(\widetilde{X}+\widetilde{Y}\right)^{2}\right]\geq 0
\end{equation}
Expanding out gives
\begin{equation}
\mathbb{E}\left[\widetilde{X}^{2}+2\widetilde{X}\widetilde{Y}+\widetilde{Y}^{2}\right]\geq0
\end{equation}
By the linearity of expectation,
\begin{equation}
\mathbb{E}\left[\widetilde{X}^{2}\right]+2\mathbb{E}\left[\widetilde{X}\widetilde{Y}\right]+\mathbb{E}\left[\widetilde{Y}^{2}\right]\geq0
\end{equation}
Note that by standardisation, $\mathbb{E}\left[\widetilde{X}^{2}\right] = \operatorname{Var}\left(\widetilde{X}\right) = 1$ and $\mathbb{E}\left[\widetilde{Y}^{2}\right] = \operatorname{Var}\left(\widetilde{Y}\right) = 1$ so
\begin{equation}
2+2\mathbb{E}\left[\widetilde{X}\widetilde{Y}\right]\geq0
\end{equation}
Using the definitions of the standardised random variables, we can show:
\begin{gather}
2+2\dfrac{\mathbb{E}\left[XY-X\mu_{Y}-Y\mu_{X}+\mu_{X}\mu_{Y}\right]}{\sigma_{X}\sigma_{Y}}\geq0 \\
2+2\dfrac{\mathbb{E}\left[XY\right]-\mathbb{E}\left[X\right]\mu_{Y}-\mathbb{E}\left[Y\right]\mu_{X}+\mu_{X}\mu_{Y}}{\sigma_{X}\sigma_{Y}}\geq0 \\
2+2\dfrac{\mathbb{E}\left[XY\right]-\mu_{X}\mu_{Y}}{\sigma_{X}\sigma_{Y}}\geq0
\end{gather}
Then using the definition of the covariance:
\begin{equation}
2+2\dfrac{\operatorname{Corr}\left(X, Y\right)}{\sigma_{X}\sigma_{Y}}\geq0
\end{equation}
Hence using the definition of the correlation coefficient
\begin{equation}
\operatorname{Corr}\left(X, Y\right) \geq -1
\end{equation}
Starting from the inequality
\begin{equation}
\mathbb{E}\left[\left(\widetilde{X}-\widetilde{Y}\right)^{2}\right]\geq0
\end{equation}
and applying the same steps, we get
\begin{equation}
\operatorname{Corr}\left(X, Y\right) \leq 1
\end{equation}
Therefore combined,
\begin{equation}
-1 \leq \operatorname{Corr}\left(X, Y\right) \leq 1
\end{equation}
The correlation coefficient allows for pairs of random variables with different units/scales to be compared.

\subsection{Subadditivity of Standard Deviations}

Suppose $X$ and $Y$ are random variables, and we form $Z = X + Y$. Then we have
\begin{equation}
\operatorname{sd}\left(Z\right) \leq \operatorname{sd}\left(X\right) + \operatorname{sd}\left(Y\right)
\end{equation}
This the known as the subadditivity of standard deviations. We can show subadditivity as follows. 
\begin{gather}
\sqrt{\operatorname{Var}\left(Z\right)} \leq \sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)} \\
\sqrt{\operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(X, Y\right) +\operatorname{Var}\left(Y\right)} \leq \sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)} \\
\operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(X, Y\right) +\operatorname{Var}\left(Y\right) \leq \left(\sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)}\right)^{2} \\
\operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(X, Y\right) +\operatorname{Var}\left(Y\right) \leq \operatorname{Var}\left(X\right) + 2\sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}
+ \operatorname{Var}\left(Y\right) \\
\operatorname{Cov}\left(X, Y\right) \leq \sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}
\end{gather}
We know the last inequality holds, because correlations are upper bounded by $1$, ie.
\begin{equation}
\operatorname{Corr}\left(X, Y\right) = \dfrac{\operatorname{Cov}\left(X, Y\right)}{\sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}} \leq 1
\end{equation}

\subsection{Conditional Variance}

The conditional variance of $X$ given $Y$, denoted $\operatorname{Var}\left(X\middle|Y\right)$ may informally be thought of as the variance of the conditional random variable $X|Y$. Formally, the definition of the conditional variance is analogous to the definition of the variance except the expectations are conditioned on $Y$:
\begin{equation}
\operatorname{Var}\left(X\middle|Y\right) = \mathbb{E}\left[\left(X - \mathbb{E}\left[X\middle|Y\right]\right)^{2}\middle| Y\right]
\end{equation}
We can show that an analogous alternative formula holds for the conditional variance:
\begin{equation}
\operatorname{Var}\left(X\middle|Y\right) = \mathbb{E}\left[X^{2}\middle|Y\right] - \mathbb{E}\left[X\middle| Y\right]^{2}
\end{equation}
This is done by first expanding and using the linearity of expectation:
\begin{align}
\operatorname{Var}\left(X\middle|Y\right) &= \mathbb{E}\left[X^{2} - 2\mathbb{E}\left[X\middle|Y\right]X + \mathbb{E}\left[X\middle|Y\right]^{2}\middle|Y\right] \\
&= \mathbb{E}\left[X^{2}\middle|Y\right] - 2\mathbb{E}\left[\mathbb{E}\left[X\middle|Y\right]X\middle|Y\right] + \mathbb{E}\left[\mathbb{E}\left[X\middle|Y\right]^{2}\middle|Y\right]
\end{align}
Now since $\mathbb{E}\left[X\middle|Y\right]$ is a random variable on the same sample space as $Y$, we can take the term out of the expectation conditioned on $Y$, giving
\begin{align}
\operatorname{Var}\left(X\middle|Y\right) &= \mathbb{E}\left[X^{2}\middle|Y\right] -2\mathbb{E}\left[X\middle| Y\right]^{2} + \mathbb{E}\left[X\middle| Y\right]^{2} \\
&= \mathbb{E}\left[X^{2}\middle|Y\right] - \mathbb{E}\left[X\middle| Y\right]^{2}
\end{align}

As with conditional expectation, the conditional variance with supplied realisation of $y$, denoted $\operatorname{Var}\left(X\middle|Y = y\right)$, may be thought of as a deterministic function in $y$. \\

The conditional variance of $X$ given $X$ can be shown to be
\begin{align}
\operatorname{Var}\left(X\middle|X\right) &= \mathbb{E}\left[X^{2}\middle|X\right] - \mathbb{E}\left[X\middle|X\right]^{2} \\
&= X^{2} - X^{2} \\
&= 0
\end{align}
Intuitively, if given a value of $X$, there should be no more uncertainty surrounding $X$, hence why the conditional variance is zero.

\subsection{Law of Total Variance}

The Law of Total Variance says that for random variables $X$ and $Y$
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}\left[\operatorname{Var}\left(Y\middle|X\right)\right] + \operatorname{Var}\left(\mathbb{E}\left[Y\middle|X\right]\right)
\end{equation}
To show this, first start with the variance of $Y$ in terms of expectations involving $Y$.
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}\left[Y^{2}\right] - \mathbb{E}\left[Y\right]^{2}
\end{equation}
By applying the Law of Iterated Expectations:
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}_{X}\left[\mathbb{E}\left[Y^{2}\middle|X\right]\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}
\end{equation}
Now use the definition of conditional variance $\operatorname{Var}\left(Y\middle|X\right) = \mathbb{E}\left[Y^{2}\middle|X\right] - \mathbb{E}\left[Y\middle|X\right]^{2}$ to get
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}_{X}\left[\operatorname{Var}\left(Y\middle|X\right) + \mathbb{E}\left[Y\middle|X\right]^{2}\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}
\end{equation}
Using the linearity of expectations:
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}_{X}\left[\operatorname{Var}\left(Y\middle|X\right)\right] + \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]^{2}\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}
\end{equation}
Then finally recognising $\operatorname{Var}\left(\mathbb{E}\left[Y\middle|X\right]\right) = \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]^{2}\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}$ yields
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}\left[\operatorname{Var}\left(Y\middle|X\right)\right] + \operatorname{Var}\left(\mathbb{E}\left[Y\middle|X\right]\right)
\end{equation}

\subsection{Conditional Covariance}

The conditional covariance between random variables $X$ and $Y$ on $Z$ is denoted $\operatorname{Cov}\left(X, Y\middle| Z\right)$. It is taken (informally) as the covariance between the conditional random variables $X|Z$ and $Y|Z$. It is defined as
\begin{equation}
\operatorname{Cov}\left(X, Y\middle| Z\right) = \mathbb{E}\left[\left(X - \mathbb{E}\left[X\middle| Z\right]\right)\left(Y - \mathbb{E}\left[Y\middle| Z\right]\right)\middle|Z\right]
\end{equation}
Expanding this out and using the Law of Iterated Expectations, we can write
\begin{align}
\operatorname{Cov}\left(X, Y\middle| Z\right) &= \mathbb{E}\left[XY - X\mathbb{E}\left[Y\middle|Z\right] - Y\mathbb{E}\left[X\middle|Z\right] + \mathbb{E}\left[X\middle| Z\right]\mathbb{E}\left[Y\middle| Z\right]\middle|Z\right] \\
&= \mathbb{E}\left[XY\middle|Z\right] - \mathbb{E}\left[X\mathbb{E}\left[Y\middle|Z\right]\middle|Z\right] - \mathbb{E}\left[Y\mathbb{E}\left[X\middle|Z\right]\middle|Z\right] +\mathbb{E}\left[\mathbb{E}\left[X\middle| Z\right]\mathbb{E}\left[Y\middle| Z\right]\middle|Z\right]
\end{align}
The same way $\mathbb{E}\left[XY\middle|X\right] = X\mathbb{E}\left[Y\middle|X\right]$, we can take out terms ${E}\left[Y\middle|Z\right]$ and ${E}\left[X\middle|Z\right]$ from their outer expectations.
\begin{align}
\operatorname{Cov}\left(X, Y\middle| Z\right) &= \mathbb{E}\left[XY\middle|Z\right] - \mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right] - \mathbb{E}\left[Y\middle|Z\right]\mathbb{E}\left[X\middle|Z\right] +\mathbb{E}\left[1\middle|Z\right]\mathbb{E}\left[X\middle| Z\right]\mathbb{E}\left[Y\middle| Z\right] \\
 &= \mathbb{E}\left[XY\middle|Z\right] - \mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right]
\end{align}
As with conditional expectations, the term $\operatorname{Cov}\left(X, Y\middle| Z\right)$ may be treated as a random variable on the same sample space as $Z$, whereas $\operatorname{Cov}\left(X, Y\middle| Z = z\right)$ is a function of the realisation $z$.

\subsection{Law of Total Covariance}
The Law of Total Covariance says that
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle|Z\right)\right] + \operatorname{Cov}\left(\mathbb{E}\left[X\middle|Z\right],\mathbb{E}\left[Y\middle|Z\right]\right)
\end{equation}
To derive this, first begin with the definition of covariance.
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
\end{equation}
Using the Law of Iterated Expectations, rewrite the expectations as
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\mathbb{E}\left[XY\middle|Z\right]\right] - \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\right]\mathbb{E}\left[\mathbb{E}\left[Y\middle|Z\right]\right]
\end{equation}
Using the definition of conditional covariance, an alternative expression for the inner term of the first expectation gives
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle| Z\right) + \mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right]\right] - \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\right]\mathbb{E}\left[\mathbb{E}\left[Y\middle|Z\right]\right]
\end{equation}
Then using the linearity of expectation
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle| Z\right)\right] + \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right]\right] - \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\right]\mathbb{E}\left[\mathbb{E}\left[Y\middle|Z\right]\right]
\end{equation}
Finally, recognise that the last two terms give the covariance between the random variables $\mathbb{E}\left[X\middle|Z\right]$ and $\mathbb{E}\left[Y\middle|Z\right]$.
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle|Z\right)\right] + \operatorname{Cov}\left(\mathbb{E}\left[X\middle|Z\right], \mathbb{E}\left[Y\middle|Z\right]\right)
\end{equation}
The Law of Total Variance is a special case of the Law of Total Covariance where $X = Y$.

\section{Independence}

\subsection{Independent Events}

Intuitively, two events $A$ and $B$ are independent if any information about the occurrence of $B$  does not affect the probability of occurrence of $A$. That is, 
\begin{equation}
\operatorname{Pr}\left(A\middle|B\right) = \operatorname{Pr}\left(A\right)
\end{equation}
The same will then be true about $B$ given $A$, because by using Bayes' rule
\begin{align}
\operatorname{Pr}\left(B\middle|A\right) &= \dfrac{\operatorname{Pr}\left(A\middle|B\right)\operatorname{Pr}\left(B\right)}{\operatorname{Pr}\left(A\right)} \\
&= \dfrac{\operatorname{Pr}\left(A\right)\operatorname{Pr}\left(B\right)}{\operatorname{Pr}\left(A\right)} \\
&= \operatorname{Pr}\left(B\right)
\end{align}

Then by using the definition of conditional probabilities,
\begin{gather}
\operatorname{Pr}\left(A\middle|B\right) = \dfrac{\operatorname{Pr}\left(A\cap B\right)}{\operatorname{Pr}\left(B\right)} \\
\operatorname{Pr}\left(A\right) = \dfrac{\operatorname{Pr}\left(A\cap B\right)}{\operatorname{Pr}\left(B\right)} \\
\operatorname{Pr}\left(A\cap B\right) = \operatorname{Pr}\left(A\right)\operatorname{Pr}\left(B\right)
\end{gather}
This says the joint probabilities of $A$ and $B$ can be found by multiplying their marginal probabilities.

\subsection{Independent Random Variables}

Extending the definition of independent events, two (discrete) random variables $X$ and $Y$ are independent if the events $X = x$ and $Y = y$ are independent for all possible combinations of realisations $x$ and $y$. Hence the joint probability is the product of the marginal probabilities:
\begin{equation}
\operatorname{Pr}\left(X = x \cap Y = y\right) = \operatorname{Pr}\left(X = x\right)\operatorname{Pr}\left(Y = y\right)
\end{equation}
for all $x$ and $y$. That is, if we can find at least one pair of $x$ and $y$ such that
\begin{equation}
\operatorname{Pr}\left(X = x \cap Y = y\right) \neq \operatorname{Pr}\left(X = x\right)\operatorname{Pr}\left(Y = y\right)
\end{equation}
then $X$ and $Y$ are dependent. For the case of continuous random variables $X$ and $Y$, the definition of independence naturally extends to being that the joint density is the product of the marginal probabilities, ie.
\begin{equation}
f_{XY}\left(x, y\right) = f_{X}\left(x\right)f_{Y}\left(y\right)
\end{equation}
for all $x$ and $y$.

\subsubsection{Independent and Identically Distributed Random Variables}

If a collection of $n$ random variables $X_{1}, X_{2}, \dots, X_{n}$ all have the same distribution and are all independent (ie. the joint distribution of $X_{1}, X_{2}, \dots, X_{n}$ is the product of the marginal distributions of $X_{1}, X_{2}, \dots, X_{n}$), then the collection of random variables is said to be independent and identically distributed (i.i.d.).

\subsection{Uncorrelatedness}

If two random variables $X$ and $Y$ have a covariance (hence also correlation) of zero, then they are said to be uncorrelated. We can show that if $X$ and $Y$ are independent, then they will always be uncorrelated. From the definition of covariance:
\begin{align}
\operatorname{Cov}\left(X, Y\right) &= \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] \\
&= \sum_{x}\sum_{y}xy\operatorname{Pr}\left(X = x \cap Y = y\right) - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
\end{align}
Using independence, then
\begin{align}
\operatorname{Cov}\left(X, Y\right) &= \sum_{x}\sum_{y}xy\operatorname{Pr}\left(X = x\right)\operatorname{Pr}\left(Y = y\right) - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] \\
&= \sum_{x}x\operatorname{Pr}\left(X = x\right)\sum_{y}y\operatorname{Pr}\left(Y = y\right) - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] \\
&= \left(\sum_{x}x\operatorname{Pr}\left(X = x\right)\right)\left(\sum_{y}y\operatorname{Pr}\left(Y = y\right)\right) - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] \\
&= \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] \\
&= 0
\end{align}
However, the reverse is not true. If $X$ and $Y$ are uncorrelated, then this does not necessarily mean that they are independent. A simple counterexample to show this is a continuous random variable $X$ with distribution
\begin{equation}
f_{X}\left(x\right) = \begin{cases} \dfrac{1}{2}, & x\in\left[-1, 1\right] \\ 0, & \mathrm{elsewhere}\end{cases}
\end{equation}
and $Y = X^{2}$. Then $X$ and $Y$ are clearly dependent. However it can be seen that $\mathbb{E}\left[X\right] = 0$ and $\mathbb{E}\left[XY\right] = 0$. The latter is reasoned by considering
\begin{align}
\mathbb{E}\left[XY\right] &= \mathbb{E}\left[X^{3}\right] \\
&= \int_{-1}^{1}\dfrac{x^{3}}{2}dx \\
&= 0
\end{align}
since $\dfrac{x^{3}}{2}$ is an odd function. Therefore
\begin{align}
\operatorname{Cov}\left(X, Y\right) &= \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] \\
&= 0
\end{align}

Subadditivity of the standard deviation is also more straightforward to show in the case where $X$ and $Y$ are uncorrelated. We firstly derive what is known as the subadditivity property of the square root function. For any two scalars $a, b \geq 0$:
\begin{align}
\left(\sqrt{a} + \sqrt{b}\right)^{2} &= a + 2\sqrt{ab} + b \\
\sqrt{a} + \sqrt{b} &= \sqrt{a + 2\sqrt{ab} + b} \\
&\geq \sqrt{a + b}
\end{align}
since $\sqrt{ab} \geq 0$. Then
\begin{equation}
\sqrt{a + b} \leq \sqrt{a} + \sqrt{b}
\end{equation}
In fact, we can tell this only holds with equality if and only if $a = 0$ or $b = 0$. The variance of $Z$ is given by $\operatorname{Var}\left(Z\right) = \operatorname{Var}\left(X\right) + \operatorname{Var}\left(Y\right)$ so
\begin{equation}
\sqrt{\operatorname{Var}\left(Z\right)} \leq \sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)}
\end{equation}
which yields the subadditivity property for the standard deviation.

\subsection{Conditional Independence}

\subsubsection{Conditionally Independent Events}
Two events $A$ and $B$ are conditionally independent on a third event $C$ if the knowledge of $C$ makes events $A$ and $B$ independent (hence their joint probability conditional on $C$ can be obtained by multiplying the marginal conditional probabilities):
\begin{equation}
\operatorname{Pr}\left(A\cap B\middle|C\right) = \operatorname{Pr}\left(A\middle|C\right)\operatorname{Pr}\left(B\middle|C\right)
\end{equation}
By rearranging this and applying the definition of conditional probability, an alternative expression can be obtained.
\begin{gather}
\dfrac{\operatorname{Pr}\left(A\cap B\middle|C\right)}{\operatorname{Pr}\left(B\middle|C\right)} = \operatorname{Pr}\left(A\middle|C\right) \\
\operatorname{Pr}\left(A\middle|B \cap C\right) = \operatorname{Pr}\left(A\middle|C\right)
\end{gather}
Note that $A$ and $B$ may or may not be independent without $C$. Also, if $A$ and $B$ are not conditionally independent on $C$, then they are conditionally dependent on $C$.

\subsubsection{Conditionally Independent Random Variables}

Similarly, two random variables $X$ and $Y$ are conditionally independent on a third random variable $Z$ if the conditional random variables $X|Z$ and $Y|Z$ are independent. Or more formally (for the case of continuous random variables),
\begin{equation}
f_{XY|Z}\left(x, y\right) = f_{X|Z}\left(x\right)f_{Y|Z}\left(y\right)
\end{equation}

\subsection{Orthogonality \cite{Yates2005}}

Two random variables $X$ and $Y$ are said to be orthogonal if $\mathbb{E}\left[XY\right] = 0$. If $X$ and $Y$ are uncorrelated and at least one of them is zero-mean, then they are also orthogonal.

\chapter{Introductory Statistics}

\section{Data Generating Processes}

In random experiments involving the collection of data, we call the random experiment the data generating process. This data generating process completely defines how observations of data are made. This includes all characteristics about the underlying `population' of interest as well as how data is sampled. The data generating process is usually not completely known, and the inherent goal of statistics to be able to deduce facts about the data generating process using data for the purposes of interpretation and prediction.

\subsection{Populations}

The population is the distribution of the particular subject under study for which data is collected.

\subsubsection{Population Parameters}

If the population distribution takes on a particular structure, then the population parameters are the structural parameters which define this distribution. For example, a commonly studied population parameter is the population mean, typically denoted $\mu$, which is the expected value of the population distribution. Another example is the population variance and population standard deviation, typically denoted $\sigma^{2}$ and $\sigma$ respectively, which are respectively the variance and standard deviation of the population distribution.

\subsubsection{Nuisance Parameters}

A nuisance parameter is any parameter not of immediate interest but still must be considered in the analysis of another parameter of interest. For example, the population mean may be of primary interest, but the population variance must be considered to conduct inference.

\subsection{Samples}

A sample of size $n$ is a collection of $n$ values sampled from population under the data generating process and may be denoted $X_{1}, X_{2}, \dots, X_{n}$ to indicate the fact that they are random variables. A realisation of a sample of size $n$ may be denoted using $x_{1}, x_{2}, \dots, x_{n}$. A single value of the sample $X_{i}$ or $x_{i}$ may be referred to as an observation.

\subsubsection{Sample Statistics}

A sample statistic (or simply statistic) is anything that is calculated via the sample. Sample statistics may be considered random variables with respect to the data generating process, because the sample itself is considered random.

\subsubsection{Estimators}

An estimator is a sample statistic that is intended to estimate some population parameter of interest. For a population parameter $\theta$, we may denote an estimator for $\theta$ by $\hat{\theta}$.

\subsubsection{Unbiasedness}

An unbiased estimator $\hat{\theta}$ for $\theta$ satisfies
\begin{equation}
\mathbb{E}\left[\hat{\theta}\right] = \theta
\end{equation}
If $\mathbb{E}\left[\hat{\theta}\right] \neq \theta$, then $\hat{\theta}$ is said to be biased.

\subsubsection{Consistency}

An estimator $\hat{\theta}$ for $\theta$ is said to be consistent if loosely speaking, the estimate $\hat{\theta}$ gets closer to the population parameter $\theta$ as the sample size $n$ grows large.

\section{Descriptive Statistics}

\subsection{Measures of Central Tendency}

\subsubsection{Sample Arithmetic Mean}

The sample arithmetic mean $\overline{x}$ (usually referred to as just the sample mean) of a sample $x_{1}, \dots, x_{n}$ is defined as
\begin{equation}
\overline{x} = \dfrac{1}{n}\sum_{i = 1}^{n}x_{i}
\end{equation}
The sample mean is the most typical estimate of the population mean. Suppose that $X_{1}, \dots, X_{n}$ is a random sample with each $X_{i}$ drawn from a probability distribution with mean $\mathbb{E}\left[X_{i}\right] = \mu$. We can then show that the estimator for the sample mean
\begin{equation}
\overline{X} = \dfrac{1}{n}\sum_{i = 1}^{n}X_{i}
\end{equation}
is an unbiased estimator for the population parameter $\mu$ as follows:
\begin{align}
\mathbb{E}\left[\overline{X}\right] &= \mathbb{E}\left[\dfrac{1}{n}\sum_{i = 1}^{n}X_{i}\right] \\
&= \dfrac{1}{n}\sum_{i = 1}^{n}\mathbb{E}\left[X_{i}\right] \\
&= \dfrac{1}{n}n\mu \\
&= \mu
\end{align}
The sample mean of a dataset also has the property that it is the value which minimises the average sum of squared deviations from that value:
\begin{equation}
\overline{x} = \argmin_{c}\dfrac{1}{n}\sum_{i = 1}^{n}\left(x_{i} - c\right)^{2}
\end{equation}
To show this, take the derivative:
\begin{equation}
\dfrac{\partial}{\partial c}\dfrac{1}{n}\sum_{i = 1}^{n}\left(x_{i} - c\right)^{2} = -\dfrac{1}{n}\sum_{i = 1}^{n}2\left(x_{i} - c\right)
\end{equation}
Setting the derivative to zero and solving gives:
\begin{gather}
-\dfrac{1}{n}\sum_{i = 1}^{n}2\left(x_{i} - c^{*}\right) = 0 \\
\dfrac{nc^{*}}{n} = \dfrac{1}{n}\sum_{i = 1}^{n}x_{i} \\
c^{*} = \overline{x}
\end{gather}

\subsubsection{Median}

Let $F\left(x\right)$ be a cumulative distribution function. The median $\widetilde{x}$ of the distribution is defined as the value at which $F\left(\widetilde{x}\right) = \dfrac{1}{2}$ (ie. the value which splits the distribution `in half'). A distribution may have more than one median (ie. if there are multiple values for which $F\left(x\right) = \dfrac{1}{2}$.

\subsubsection{Sample Median}

The sample median $\widetilde{x}$ of a sample $x_{1}, \dots, x_{n}$ is the `middle value' of the sample. To be more precise, suppose we can sort the sample so that
\begin{equation}
x_{\left(1\right)} \leq \dots \leq x_{\left(n\right)}
\end{equation}
Then if $n$ is odd, the median is defined as
\begin{equation}
\widetilde{x} = x_{\left(\frac{n + 1}{2}\right)}
\end{equation}
If $n$ is even, then the median is defined as the average of the two `middle values':
\begin{equation}
\widetilde{x} = \dfrac{x_{\left(\lfloor\frac{n + 1}{2}\rfloor\right)} + x_{\left(\lceil\frac{n + 1}{2}\rceil\right)}}{2}
\end{equation}
The sample median can be useful as a measure of central tendency when there are extreme outliers in the sample, which would otherwise have a greater effect on the sample mean than the sample median.

\subsubsection{Mode}

Let $f\left(x\right)$ be a probability density function. The mode $\breve{x}$ of the distribution is the `peak' of the distribution, that is $\breve{x} = \max_{x}f\left(x\right)$. Some distributions may have multiple local peaks, at which $\dfrac{df\left(x\right)}{dx} = 0$ (supposing that $f\left(x\right)$ is differentiable). We then say that the distribution is multimodal. If there is only a single local peak, then the distribution is said to be unimodal. This definition is analogous to discrete probability distributions, we take the value which has the highest probability mass associated to it.

\subsubsection{Sample Mode}

The sample mode of a sample $x_{1}, x_{2}, \dots, x_{n}$ is the most frequently occurring value in the sample. A sample may have more than one mode.

\subsubsection{Sample Geometric Mean}

The sample geometric mean $\overline{x}_{\mathrm{G}}$ of a sample $x_{1}, \dots, x_{n}$ is defined as
\begin{equation}
\overline{x}_{\mathrm{G}} = \left(\prod_{i = 1}^{n}x_{i}\right)^{1/n}
\end{equation}
Note that the product $\prod_{i = 1}^{n}x_{i}$ should be non-negative in order for the geometric mean to be real-valued. Hence the geometric mean is most applicable for giving a valid indication of central tendency if all values in the sample are positive. \\

The geometric mean is applicable for averaging over compounding growth rates. To illustrate, suppose we have growth rates $g_{1}, \dots, g_{n}$ such that the overall compounded growth is $\prod_{i = 1}^{n}\left(1 + g_{i}\right)$. Taking the geometric mean shall give
\begin{equation}
\left(\prod_{i = 1}^{n}\left(1 + g_{i}\right)\right)^{1/n} = 1 + \overline{g}
\end{equation}
and note that
\begin{equation}
\prod_{i = 1}^{n}\left(1 + g_{i}\right) = \left(1 + \overline{g}\right)^{n}
\end{equation}
so that a growth rate of $\overline{g}$ over $n$ periods gives an equivalent compounded growth. \\

The geometric mean is also applicable for averaging normalised results. To illustrate, suppose we have two data sets $x_{1}, \dots, x_{n}$ and $y_{1}, \dots, y_{n}$. We have a choice of constructing normalised data sets using either $x_{i}$ or $y_{i}$ as reference values (ie. $y_{1}/x_{1}, \dots, y_{n}/x_{n}$ or $x_{1}/y_{1}, \dots, x_{n}/y_{n}$ respectively). However, we have
\begin{equation}
\dfrac{\left(\prod_{i = 1}^{n}x_{i}\right)^{1/n}}{\left(\prod_{i = 1}^{n}y_{i}\right)^{1/n}} = \left(\prod_{i = 1}^{n}\dfrac{x_{i}}{y_{i}}\right)^{1/n}
\end{equation}
and
\begin{equation}
\dfrac{\left(\prod_{i = 1}^{n}y_{i}\right)^{1/n}}{\left(\prod_{i = 1}^{n}x_{i}\right)^{1/n}} = \left(\prod_{i = 1}^{n}\dfrac{y_{i}}{x_{i}}\right)^{1/n}
\end{equation}
Hence, regardless of which sample is used as the reference values, the geometric mean preserves the relative ordering of the averaged data.

\subsubsection{Sample Harmonic Mean}

The sample harmonic mean $\overline{x}_{\mathrm{H}}$ of a sample $x_{1}, \dots, x_{n}$ is defined as
\begin{equation}
\overline{x}_{\mathrm{H}} = \left(\dfrac{1}{n}\sum_{i = 1}^{n}x_{i}^{-1}\right)^{-1}
\end{equation}
Hence the sample harmonic mean is the reciprocal of the sample arithmetic mean of the reciprocals. \\

The harmonic mean can be applicable in certain cases for averaging rates and ratios. To illustrate, suppose we have a sample $x_{1}, \dots, x_{n}$ where each $x_{i} = \dfrac{a}{b_{i}}$ is a ratio of two quantities with the numerator is kept constant for each value. We desire the `true' average of the ratio, being $na/\sum_{i = 1}^{n}b_{i}$. Then taking the harmonic mean gives
\begin{align}
\left(\dfrac{1}{n}\sum_{i = 1}^{n}x_{i}^{-1}\right)^{-1} &= \left(\dfrac{1}{n}\cdot\dfrac{\sum_{i = 1}^{n}b_{i}}{a}\right)^{-1} \\
&= \left(\dfrac{\sum_{i = 1}^{n}b_{i}}{na}\right)^{-1} \\
&= \dfrac{na}{\sum_{i = 1}^{n}b_{i}}
\end{align}

\subsection{Measures of Dispersion}

\subsubsection{Sample Variance}

\subsubsection{Bessel's Correction}

Suppose we knew the value of the population mean. Then it should be clear that in order to compute an unbiased estimate of the population variance from the sample variance, one should first compute the sum of squared deviations about the population mean $\sum_{i = 1}^{n}{\left(x_{i} - \mu\right)^{2}}$ and then divide by the sample size $n$. However, typically only the sample mean $\bar{x}$ is known, which itself is computed from the sample. So this inherently introduces bias if we were to compute sample variance the same way (we are measuring squared deviations of a sample from a mean which is itself determined by the sample). So to `compensate' for this, we divide by $n - 1$ instead of $n$ to give an unbiased estimate of the sample variance.

\subsubsection{Sample Standard Deviation}

\subsubsection{Coefficient of Variation}

The coefficient of variation is a standardised measure of dispersion relative to the mean. It is the ratio of standard deviation to the mean.
\begin{equation}
\mathrm{COV} = \dfrac{\sigma}{\mu}
\end{equation}

\subsubsection{Range}

\subsubsection{Percentiles}

\subsubsection{Quartiles}

\subsubsection{Interquartile Range}

\subsubsection{Quantiles}

\subsubsection{Frequency Distribution}

\subsubsection{Signal-to-noise Ratio}

While there are many definitions of the signal-to-noise ratio, one such definition is the inverse of the coefficient of variation.
\begin{equation}
\mathrm{SNR} = \dfrac{\mu}{\sigma}
\end{equation}

\subsection{Measures of Dependence}

\subsubsection{Sample Covariance}

\subsubsection{Pearson Correlation Coefficient}

\subsubsection{Kendall Rank Correlation Coefficient}

The Kendall rank correlation coefficient (also known by the Kendall $\tau$ coefficient) measures the ordinal associated between two sets of observations. Let $\left(x_{1}, y_{1}\right), \dots, \left(x_{n}, y_{n}\right)$ be the sets of observations which are a realised sample from the distributions of $X$ and $Y$ respectively. Assume that all values in the observations are unique. A pair of observations $\left(x_{i}, y_{i}\right)$ and $\left(x_{j}, y_{j}\right)$ for any $i < j$ is said to be concordant if $x_{i} > x_{j}$ and $y_{i} > y_{j}$, or alternatively if $x_{i} < x_{j}$ and $y_{i} < y_{j}$. Let $n_{c}$ be the number of concordant pairs in the set of observations. A pair of observations $\left(x_{i}, y_{i}\right)$ and $\left(x_{j}, y_{j}\right)$ for any $i < j$ is said to be discordant if $x_{i} > x_{j}$ and $y_{i} < y_{j}$, or alternatively if $x_{i} < x_{j}$ and $y_{i} > y_{j}$. Let $n_{d}$ be the number of discordant pairs in the set of observations. The Kendall $\tau$ coefficient is defined as
\begin{equation}
\tau = \dfrac{n_{c} - n_{d}}{n\left(n - 1\right)/2}
\end{equation}
Note that
\begin{itemize}
\item The term $n\left(n - 1\right)/2$ is the number of pairs in the observations.
\item Like with the Pearson correlation coefficient, $-1 \leq \tau \leq 1$.
\end{itemize}
An explicit expression for $\tau$ is found by noticing $n_{c} - n_{d} = \sum_{i < j}\operatorname{sign}\left(x_{i} - x_{j}\right)\operatorname{sign}\left(y_{i} - y_{j}\right)$:
\begin{equation}
\tau = \dfrac{2}{n\left(n - 1\right)}\sum_{i < j}\operatorname{sign}\left(x_{i} - x_{j}\right)\operatorname{sign}\left(y_{i} - y_{j}\right)
\end{equation}

\subsubsection{Spearman's Rank Correlation Coefficient}

The Spearman's rank correlation coefficient (also known as the Spearman's $\rho$ coefficient) measures the strength of monotonic (not necessarily linear) relationship between two variables. For pairs of observations $\left(x_{1}, y_{1}\right), \dots, \left(x_{n}, y_{n}\right)$, let the ranked variables $\left(r_{xi}, r_{yi}\right)$ denote the ranking of each observation within their respective datasets (ie. $r_{xi}$ is the ranking of observation $x_{i}$ in the dataset for $x$, with $1$ being the smallest and $n$ being the largest). Assume that all values in the observations are unique. The spearman's $\rho$ coefficient is defined by
\begin{equation}
\rho = \dfrac{sr_{xy}}{sr_{x}sr_{y}}
\end{equation}
where $sr_{xy}$ is the sample covariance between the ranked observations, while $sr_{x}$ and $sr_{y}$ are the sample standard deviations of the ranked $x$ and $y$ variables respectively. Essentially, the Spearman's $\rho$ coefficient is the Pearson correlation coefficient of the ranked variables.

\subsubsection{Contingency Tables}

A contingency table can be used to present the joint probability distribution (or alternatively, relative frequency or absolute frequency distribution) of two variables, and by taking summations across rows and columns to obtain marginal distributions. The layout of a $2\times 2$ contingency table for two random variables $X$, $Y$ taking on values $\left\{0, 1\right\}$ is given below.
\begin{table}[H]\centering
\begin{tabular}{|c||c|c|c|}
\hline 
 & $Y = 0$ & $Y = 1$ & \\
\hline 
\hline 
$X = 0$ & $\operatorname{Pr}\left(X = 0, Y = 0\right)$ &  $\operatorname{Pr}\left(X = 0, Y = 1\right)$ & $\operatorname{Pr}\left(X = 0\right)$ \\
\hline 
$X = 1$ & $\operatorname{Pr}\left(X = 1, Y = 0\right)$ & $\operatorname{Pr}\left(X = 1, Y = 1\right)$ & $\operatorname{Pr}\left(X = 1\right)$ \\
\hline 
 & $\operatorname{Pr}\left(Y = 0\right)$ & $\operatorname{Pr}\left(Y = 1\right)$ & $1$ \\
 \hline
\end{tabular}
\end{table}

\subsection{Measures of Shape}

\subsubsection{Skewness}

\subsubsection{Kurtosis}

\section{Normal Statistics}

\subsection{Normal Sample Mean}

\subsubsection{Independence of Normal Sample Mean and Sample Variance}

Let $Z_{1}, \dots, Z_{n}$ be i.i.d. random variables from the standard normal distribution (ie. mean of $0$ and variance of $1$). The sample mean is defined by
\begin{equation}
\overline{Z} = \dfrac{1}{n}\sum_{i = 1}^{n}Z_{i}
\end{equation}
We first show that the random variables $Z_{i} - \overline{Z}$ and $\overline{Z}$ are independent for any $i = 1, \dots, n$. Because $Z_{i} - \overline{Z}$ and $\overline{Z}$ are jointly normally distributed, it is enough to show that their covariance is zero. Using the definition of covariance and the fact that $\mathbb{E}\left[Z_{i}\right] = \mathbb{E}\left[\overline{Z}\right] = \mathbb{E}\left[Z_{i} - \overline{Z}\right] = 0$:
\begin{align}
\operatorname{Cov}\left(\overline{Z}, Z_{i} - \overline{Z}\right) &= \mathbb{E}\left[\overline{Z}\left(Z_{i} - \overline{Z}\right)\right] - \mathbb{E}\left[\overline{Z}\right]\mathbb{E}\left[Z_{i} - \overline{Z}\right] \\
&= \mathbb{E}\left[\overline{Z}\left(Z_{i} - \overline{Z}\right)\right] \\
&= \mathbb{E}\left[\overline{Z}Z_{i} - \overline{Z}^{2}\right] \\
&= \mathbb{E}\left[\dfrac{1}{n}\sum_{j = 1}^{n}Z_{j}Z_{i} - \dfrac{1}{n^{2}}\left(\sum_{j = 1}^{n}Z_{j}\right)\left(\sum_{k = 1}^{n}Z_{k}\right)\right] \\
&= \mathbb{E}\left[\dfrac{1}{n}\sum_{j = 1}^{n}Z_{j}Z_{i} - \dfrac{1}{n^{2}}\sum_{j = 1}^{n}\sum_{j = 1}^{n}Z_{j}Z_{k}\right] \\
&= \dfrac{1}{n}\sum_{j = 1}^{n}\mathbb{E}\left[Z_{j}Z_{i}\right] - \dfrac{1}{n^{2}}\sum_{j = 1}^{n}\sum_{j = 1}^{n}\mathbb{E}\left[Z_{j}Z_{k}\right]
\end{align}
Note that for standard normal random variables $\mathbb{E}\left[Z_{j}Z_{i}\right] = \operatorname{Var}\left(Z_{i}\right) = 1$ if $i = j$ and $\mathbb{E}\left[Z_{j}Z_{i}\right] = 0$ otherwise. Also $\mathbb{E}\left[Z_{j}Z_{k}\right] = 1$ if $j = k$ and zero otherwise. Hence
\begin{align}
\operatorname{Cov}\left(\overline{Z}, Z_{i} - \overline{Z}\right) &= \dfrac{1}{n} - \dfrac{n}{n^{2}} \\
&= 0
\end{align}
Since the sample variance given by
\begin{equation}
S = \frac{1}{n - 1}\sum_{i = 1}^{n}\left(Z_{i} - \overline{Z}\right)^{2}
\end{equation}
contains a sum over $Z_{i} - \overline{Z}$ but each $Z_{i} - \overline{Z}$ is independent with $\overline{Z}$, it follows that $\overline{Z}$ is independent with $S$. Then for any i.i.d. normal sample $X_{1}, \dots, X_{n}$ from a population with mean $\mu$ and variance $\sigma^{2}$, the sample mean $\overline{X}$ is independent with the sample variance $S_{X}$ because the same argument as above can be applied to the standardised $\dfrac{X_{i} - \mu}{\sigma}$ and $\dfrac{\overline{X} - \mu}{\sigma}$.

\section{Inferential Statistics}

\subsection{Confidence Intervals}

Suppose for a random variable $X$, we know it has a population mean $\mathbb{E}\left[X\right] = \mu$ (we may not actually know the mean itself), however we do know the standard deviation $\operatorname{sd}\left(X\right) = \sigma$. Also suppose that we do know enough such that the probability of $X$ being with $t > 0$ standard deviations within the mean is given by $1 - \alpha$, where $\alpha \in \left[0, 1\right]$. That is,
\begin{equation}
\operatorname{Pr}\left(\mu - t\sigma \leq X \leq \mu + t\sigma\right) = 1 - \alpha
\end{equation}
We can rearrange the above event as follows.
\begin{gather}
\mu - t\sigma \leq X \leq \mu + t\sigma \\
\mu \leq X + t\sigma \cap X - t\sigma \leq \mu \\
X - t\sigma \leq \mu \leq X + t\sigma
\end{gather}
Hence
\begin{equation}
\operatorname{Pr}\left(X - t\sigma \leq \mu \leq X + t\sigma\right) = 1 - \alpha
\end{equation}
That is, if we obtain a realisation of $X$ and construct the interval $\left[X - t\sigma, X + t\sigma\right]$, then there is $1 - \alpha$ probability that the interval will contain the population mean $\mu$. Note that this probability refers to before the random variable $X$ is realised. Suppose we have already obtained a realisation (call this $x$), then the matter of whether the interval 
$\left[x - t\sigma, x + t\sigma\right]$ contains $\mu$ is no longer down to probability (it either will contain $\mu$ or it wont). Hence the interval $\left[x - t\sigma, x + t\sigma\right]$ is considered a $\left(1 - \alpha\right)\times 100\%$ confidence interval.

\subsection{Hypothesis Testing}

\subsubsection{Statistical Power}

If we knew all the information about the population and data-generating process, then in principle it is possible to compute power of a particular test. However, this is highly idealised because if the data-generating process were already known, hypothesis tests would not be needed. Also, if it turns out that the null hypotheis were actually true, then the notion of power becomes ill-defined. Rather, we can still reason about how changing our experimental design will affect power, ie. increasing the sample size or choosing a larger $\alpha$ should increase power.

\section{Simple Linear Regression}

In statistical modelling, we specify that the dependent variable $y$ is a function of the independent variable $x$, given by some relationship
\begin{equation}
y = f\left(x\right)
\end{equation}
However, the `dependent' variable and `independent' variable are unfortunately named so, and need not have anything to do with the probabilistic definitions of the dependence and independence. The word `independent' is used to refer to the ability of the experimenter to choose the values of $x$ when designing an experiment, although in not all cases will be values of $x$ be freely available to be chosen by the experimenter. There are many synonyms that may be used in place of the dependent and independent variable.

\begin{table}[H]\centering
\begin{tabular}{|c|c|}
\hline 
$y$ & $x$ \\
\hline 
\hline 
Dependent variable & Independent variable \\
Explained variable & Explanatory variable \\
Label & Feature \\
Regressand & Regressor \\
Output variable & Input variable \\
Response variable & Covariate \\
Predicted variable & Control variable \\
Prediction & Predictor \\
\hline
\end{tabular}
\end{table}

\subsection{Confidence Intervals of the Conditional Mean}

\subsection{Prediction Intervals of the Observed Response}

\section{Analysis of Variance}

\section{Method of Moments}

\chapter{Intermediate Probability}

\section{Transformations of Random Variables}

\subsection{Linear Transformations of Random Variables}

Suppose random variable $X$ has probability density function $f_{X}\left(x\right)$ and cumulative distribution function $F_{X}\left(x\right)$. Define the linearly transformed random variable $Y = aX + b$. We derive expressions for the density function $f_{Y}\left(y\right)$ and cumulative distribution $F_{Y}\left(y\right)$. Firstly by definition of the cumulative distribution,
\begin{align}
F_{Y}\left(y\right) &= \operatorname{Pr}\left(Y \leq y\right) \\
&= \operatorname{Pr}\left(aX + b \leq y\right) \\
&= \operatorname{Pr}\left(X \leq \dfrac{y - b}{a}\right) \\
&= F_{X}\left(\dfrac{y - b}{a}\right)
\end{align}
Differentiating using the chain rule, the probability density function becomes
\begin{equation}
f_{Y}\left(y\right) = \dfrac{1}{a}f_{X}\left(\dfrac{y - b}{a}\right)
\end{equation}

\subsection{Parametric Distributions}

A probability distribution may belong to a `family' of distributions, characterised by a common functional form that is parametrised. A probability distribution parametrised in $\theta$ may be denoted $f\left(x; \theta\right)$.

\subsubsection{Scale Parameters}

A scale parameter `stretches' the distribution. If a particular distribution with scale parameter $\theta$ has density $f\left(x\right)$, then the distribution with scale parameter $a\theta$ has density $f\left(x/a\right)/a$.

\subsubsection{Rate Parameters}

The reciprocal of a scale parameter may be referred to as a rate parameter (ie. it `concentrates' the distribution). Hence if a particular distribution with rate parameter $\theta$ has density $f\left(x\right)$, then the distribution with rate parameter $b\theta$ has density $bf\left(bx\right)$.

\subsubsection{Location Parameters}

A location parameter `shifts' the distribution. This means that if a particular distribution with location parameter $\theta$ has density $f\left(x\right)$, then the distribution with location parameter $\theta + c$ has density $f\left(x - c\right)$. Families of distributions may typically be parametrised in terms of the mean, median or mode as a location parameter. 

\subsubsection{Shape Parameters}

Any parameter of a distribution which is neither a scale/rate nor location parameter may be referred to as a shape parameter.

\subsection{Sums of Random Variables}

Consider the sum of two continuous random variables $W = X + Y$. The PDF of $W$ can be defined as
\begin{equation}
f_{W}\left(w\right) = \int\int_{\left\{x,y: x + y = w\right\}}f_{XY}\left(x, y\right)dxdy
\end{equation}
We can then just integrate along the straight line $x + y = w$ and make a substitution of either $x = w - y$ or $y = w - x$ (which influences the integrating variable). This gives rise to two alternative formulas.
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(x, w-x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(w - y, y\right)dy
\end{gather}
If $X$ and $Y$ are independent, then
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(w-x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(w - y\right)f_{Y}\left(y\right)dy
\end{gather}
Note that that $f_{W}\left(\cdot\right)$ is a convolution of $f_{X}\left(\cdot\right)$ and $f_{Y}\left(\cdot\right)$. \\

For the case of the sum of two discrete random variables $W = X + Y$, the analogous formulae are
\begin{gather}
\operatorname{Pr}\left(W = w\right) = \sum_{x = -\infty}^{\infty}\operatorname{Pr}\left(X = x, Y = w - x\right) \\
\operatorname{Pr}\left(W = w\right) = \sum_{y = -\infty}^{\infty}\operatorname{Pr}\left(X = w - y, Y = y\right)
\end{gather}
and in the independent case
\begin{gather}
\operatorname{Pr}\left(W = w\right) = \sum_{x = -\infty}^{\infty}\operatorname{Pr}\left(X = x\right)\operatorname{Pr}\left(Y = w - x\right) \\
\operatorname{Pr}\left(W = w\right) = \sum_{y = -\infty}^{\infty}\operatorname{Pr}\left(X = w - y\right)\operatorname{Pr}\left(Y = y\right)
\end{gather}

\subsubsection{Difference of Random Variables}

Consider the difference of two continuous random variables $W = X - Y$. Analogously (by making substitutions $x = w + y$ or $y = -w + x$) we can show
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(x, -w + x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(w + y, y\right)dy
\end{gather}
and in the independent case
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(-w + x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(w + y\right)f_{Y}\left(y\right)dy
\end{gather}
\begin{theorem}
If $X$ and $Y$ are independent and identically distributed continuous random variables, than the distribution of their difference $W = X - Y$ is symmetric.
\end{theorem}
\begin{proof}
Note that $W$ will necessarily have a mean of zero (assuming $X$ and $Y$ have finite mean). If the mean of $W$ is not defined, its probability distribution will still have a `symmetry point' of zero. It suffices to show $f_{W}\left(w\right) = f_{W}\left(-w\right)$. Using the formulas above in the independent case,
\begin{equation}
f_{W}\left(-w\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(w + x\right)dx
\end{equation}
Since $X$ and $Y$ are identical, $f_{X}\left(x\right) = f_{Y}\left(x\right)$ so
\begin{equation}
f_{W}\left(-w\right) = \int_{-\infty}^{\infty}f_{Y}\left(x\right)f_{X}\left(w + x\right)dx
\end{equation}
`Renaming' the integrating variable to $y$ yields
\begin{equation}
f_{W}\left(-w\right) = \int_{-\infty}^{\infty}f_{Y}\left(y\right)f_{X}\left(w + y\right)dy = f_{W}\left(w\right)
\end{equation}
\end{proof}

\subsubsection{Stable Distributions}

A family of distributions is said to be stable if a linear combination of two random variables from that family also has a distribution in that family. Examples of stable distributions are the Gaussian and Cauchy distributions.

\subsection{Strictly Monotonic Transformations}

Let $X$ be a random variable with probability density $f_{X}\left(x\right)$, and let $A$ be the set $A = \left\{x \in\mathbb{R}\middle| f_{X}\left(x\right) > 0\right\}$. Suppose that $Y$ is a transformation of $X$ given by $Y = g\left(X\right)$ such that $g$ is strictly monotonic (either increasing or decreasing) over $A$. We derive the probability density of $Y$, denoted $f_{Y}\left(y\right)$, in terms of $f_{X}\left(x\right)$ and $g$. \\

First assume $g$ is monotonically increasing, so $g^{-1}\left(\cdot\right)$ will also be monotonically increasing. The cumulative distribution function of $Y$ is given by
\begin{align}
F_{Y}\left(y\right) &= \operatorname{Pr}\left(Y \leq y\right) \\
&= \operatorname{Pr}\left(g\left(X\right) \leq y\right) \\
&= \operatorname{Pr}\left(X \leq g^{-1}\left(y\right)\right) \\
&= F_{X}\left(g^{-1}\left(y\right)\right)
\end{align}
Differentiating $F_{Y}\left(y\right)$ with respect to $y$ gives the density of $Y$:
\begin{align}
f_{Y}\left(y\right) &= \dfrac{d}{dy}F_{Y}\left(y\right)\\
&= \dfrac{d}{dy}F_{X}\left(g^{-1}\left(y\right)\right)
\end{align}
Using the chain rule,
\begin{align}
f_{Y}\left(y\right) &= \dfrac{d}{dx}F_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right) \\
&= f_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right)
\end{align}
Now assume $g$ is monotonically decreasing, so $g^{-1}\left(\cdot\right)$ is also monotonically decreasing and has the ability to flip an inequality sign. Similar to before, we can write
\begin{align}
F_{Y}\left(y\right) &= \operatorname{Pr}\left(Y \leq y\right) \\
&= \operatorname{Pr}\left(g\left(X\right) \leq y\right) \\
&= \operatorname{Pr}\left(X \geq g^{-1}\left(y\right)\right) \\
&= 1 - F_{X}\left(g^{-1}\left(y\right)\right)
\end{align}
Now differentiating $F_{Y}$ gives 
\begin{align}
f_{Y}\left(y\right) &= \dfrac{d}{dy}F_{Y}\left(y\right) \\
&= -\dfrac{d}{dy}F_{X}\left(g^{-1}\left(y\right)\right) \\
&= -\dfrac{d}{dx}F_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right) \\
&= -f_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right)
\end{align}
Since by monotonically decreasing $g^{-1}\left(\cdot\right)$, we have $\dfrac{d}{dy}g^{-1}\left(y\right) < 0$ hence this density is positive. Combining the two cases, we get
\begin{equation}
f_{Y}\left(y\right) =  f_{X}\left(g^{-1}\left(y\right)\right)\cdot\left|\dfrac{d}{dy}g^{-1}\left(y\right)\right|
\end{equation}

\subsection{Inverse Transform Sampling}

\subsection{Gauss' Approximation Theorem \cite{Blom1989}}

For a differentiable function $g\left(X\right)$ of a single random variable $X$, we can make the approximations
\begin{gather}
\mathbb{E}\left[g\left(X\right)\right] \approx g\left(\mathbb{E}\left[X\right]\right) \\
\operatorname{Var}\left(g\left(X\right)\right) \approx \operatorname{Var}\left(X\right)g'\left(\mathbb{E}\left[X\right]\right)^{2}
\end{gather}
This is achieved using a first-order Taylor series expansion of $g\left(X\right)$ about $m := \operatorname{E}\left[X\right]$ as follows
\begin{equation}
g\left(X\right) \approx g\left(m\right) + \left(X - m\right)g'\left(m\right)
\end{equation}
Taking the expectations of both sides gives
\begin{align}
\mathbb{E}\left[g\left(X\right)\right] &\approx \mathbb{E}\left[g\left(m\right)\right] + \mathbb{E}\left[\left(X - m\right)g'\left(m\right)\right] \\
&\approx \mathbb{E}\left[g\left(\mathbb{E}\left[X\right]\right)\right] + \mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)\right]g'\left(\mathbb{E}\left[X\right]\right) \\
&\approx g\left(\mathbb{E}\left[X\right]\right) + \left(\mathbb{E}\left[X\right] - \mathbb{E}\left[X\right]\right)g'\left(\mathbb{E}\left[X\right]\right) \\
&\approx g\left(\mathbb{E}\left[X\right]\right)
\end{align}
Taking the variance of both sides of the expansion gives
\begin{align}
\operatorname{Var}\left(g\left(X\right)\right) &\approx \operatorname{Var}\left(g\left(m\right) + \left(X - m\right)g'\left(m\right)\right) \\
&\approx \operatorname{Var}\left(\left(X - m\right)g'\left(m\right)\right) \\
&\approx \operatorname{Var}\left(X - m\right)g'\left(m\right)^{2} \\
&\approx \operatorname{Var}\left(X\right)g'\left(m\right)^{2} \\
&\approx \operatorname{Var}\left(X\right)g'\left(\mathbb{E}\left[X\right]\right)^{2} 
\end{align}

\subsection{Independence of Transformed Random Variables}

If $X$ and $Y$ are independent random variables, then the transformed random variables $f\left(X\right)$ and $g\left(Y\right)$ are also independent. This is shown by first using the definition of independence that for sets $A$ and $B$, 
\begin{equation}
\operatorname{Pr}\left(X \in A \cap Y \in B\right) = \operatorname{Pr}\left(X \in A\right)\operatorname{Pr}\left(Y \in B\right)
\end{equation}
Then note the equivalence of events
\begin{gather}
\left\{f\left(X\right) \in A\right\} \equiv \left\{X \in f^{-1}\left(A\right)\right\} \\
\left\{g\left(Y\right) \in B\right\} \equiv \left\{Y \in g^{-1}\left(B\right)\right\}
\end{gather}
So
\begin{align}
\operatorname{Pr}\left(f\left(X\right) \in A \cap g\left(Y\right) \in B\right) &= \operatorname{Pr}\left(X \in f^{-1}\left(A\right) \cap Y \in g^{-1}\left(B\right)\right) \\
&=  \operatorname{Pr}\left(X \in f^{-1}\left(A\right)\right)\operatorname{Pr}\left(Y \in g^{-1}\left(B\right)\right) \\
&= \operatorname{Pr}\left(f\left(X\right) \in A\right)\operatorname{Pr}\left(g\left(Y\right) \in B\right)
\end{align}

Note that if $X$ and $Y$ were dependent, then $f\left(X\right)$ and $g\left(Y\right)$ need not necessarily be dependent. A trivial counter example is a case where $f\left(X\right)$ is a constant.

\subsection{Products of Random Variables}

The product distribution of random variables $X$ and $Y$ is the distribution of $Z = XY$. If $X$ and $Y$ are independent and have probability density functions $f_{X}\left(x\right)$ and $f_{Y}\left(y\right)$ respectively, then
\begin{equation}
f_{Z}\left(z\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(z/x\right)\dfrac{1}{\left|x\right|}dx
\end{equation}
\begin{proof}
We can express the probability density of $Z$ as the integration of the joint density of $X$ and $Y$ over the region where $XY = Z$.
\begin{equation}
f_{Z}\left(z\right) = \int\int_{\left\{x, y: xy = z\right\}}f_{X}\left(x\right)f_{Y}\left(y\right)dydx
\end{equation}
Make the substitution $y = z/x$ and rewrite the integral using the Dirac delta function:
\begin{align}
f_{Z}\left(z\right) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(z/x\right)\delta\left(xy - z\right)dydx \\
&= \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(z/x\right)\int_{-\infty}^{\infty}\delta\left(xy - z\right)dydx
\end{align}
Note the scaling property of the Dirac delta function, that $\delta\left(xy\right) = \dfrac{\delta\left(y\right)}{\left|x\right|}$ (this is to keep the property that $\int_{-\infty}^{\infty}\left|x\right|\delta\left(xy\right)dy = \int_{-\infty}^{\infty}\delta\left(y\right)dy = 1$). Hence
\begin{equation}
\int_{-\infty}^{\infty}\delta\left(xy - z\right)dy = \dfrac{1}{\left|x\right|}
\end{equation}
and
\begin{equation}
f_{Z}\left(z\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(z/x\right)\dfrac{1}{\left|x\right|}dx
\end{equation}
\end{proof}

Furthermore, for independent $X$ and $Y$, the expectation of $Z = XY$ is
\begin{equation}
\mathbb{E}\left[XY\right] = \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
\end{equation}
\begin{proof}
Using the definition of covariance,
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
\end{equation}
Then apply $\operatorname{Cov}\left(X, Y\right) = 0$ due to independence.
\end{proof}

If $X$ and $Y$ are not independent but have the joint density function $f_{XY}\left(x, y\right)$, then
\begin{equation}
f_{Z}\left(z\right) = \int_{-\infty}^{\infty}f_{XY}\left(x, z/x\right)\dfrac{1}{\left|x\right|}dx
\end{equation}

\subsection{Ratios of Random Variables}
The ratio (or quotient) distribution of random variables $X$ and $Y$ is the distribution of $W = \dfrac{X}{Y}$. If $X$ and $Y$ are independent and have probability density functions $f_{X}\left(x\right)$ and $f_{Y}\left(y\right)$ respectively, then
\begin{equation}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(wy\right)f_{Y}\left(y\right)\left|y\right|dy
\end{equation}
The proof is similar to the derivation of the product distribution.
\begin{proof}
We can express the probability density of $W$ as the integration of the joint density of $X$ and $Y$ over the region where $X/Y = W$.
\begin{equation}
f_{W}\left(w\right) = \int\int_{\left\{x, y: x/y = w\right\}}f_{X}\left(x\right)f_{Y}\left(y\right)dxdy
\end{equation}
Make the substitution $x = wy$ and rewrite the integral using the Dirac delta function:
\begin{align}
f_{W}\left(w\right) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X}\left(wy\right)f_{Y}\left(y\right)\delta\left(x/y - w\right)dydx \\
&= \int_{-\infty}^{\infty}f_{X}\left(wy\right)f_{Y}\left(y\right)\int_{-\infty}^{\infty}\delta\left(x/y - w\right)dxdy
\end{align}
By the scaling property of the Dirac delta function,
\begin{equation}
\int_{-\infty}^{\infty}\delta\left(x/y - z\right)dx = \left|y\right|
\end{equation}
Hence
\begin{equation}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(wy\right)f_{Y}\left(y\right)\left|y\right|dy
\end{equation}
\end{proof}
If $X$ and $Y$ are not independent but have the joint density function $f_{XY}\left(x, y\right)$, then
\begin{equation}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(wy, y\right)\dfrac{1}{\left|y\right|}dy
\end{equation}

The mean of the ratio distribution (even in the independent case) is not as straightforward compared to the product distribution, however can be approximated using Taylor expansions. To illustrate, we first develop an extension of Gauss' approximation theorem to a nonlinear function in two variables, $f\left(x, y\right)$. Suppose random variables $X$ and $Y$ have means $\mu_{X}$ and $\mu_{Y}$ respectively with $\mu_{Y} \neq 0$. Then a second order Taylor series approximation of $f\left(X, Y\right)$ about the mean $\left(\mu_{X}, \mu_{Y}\right)$ gives
\begin{multline}
f\left(X, Y\right) \approx f\left(\mu_{X}, \mu_{Y}\right) + \left.\nabla f\left(x, y\right)^{\top}\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}\begin{bmatrix} X - \mu_{X} \\ Y - \mu_{Y}\end{bmatrix} + \\
\dfrac{1}{2}\begin{bmatrix} X - \mu_{X} & Y - \mu_{Y}\end{bmatrix}\left.\nabla^{2} f\left(x, y\right)\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}\begin{bmatrix} X - \mu_{X} \\ Y - \mu_{Y}\end{bmatrix}
\end{multline}
If we take the expectation of this, the first order term vanishes because
\begin{equation}
\mathbb{E}\left[\begin{bmatrix} X - \mu_{X} \\ Y - \mu_{Y}\end{bmatrix}\right] = \begin{bmatrix} \mu_{X} - \mu_{X} \\ \mu_{Y} - \mu_{Y}\end{bmatrix}
\end{equation}
We are then left with
\begin{multline}
\mathbb{E}\left[f\left(X, Y\right)\right] \approx f\left(\mu_{X}, \mu_{Y}\right) + \dfrac{1}{2}\mathbb{E}\left[\left(X - \mu_{X}\right)^{2}\right]\left.\left(\dfrac{\partial^{2}f\left(x, y\right)}{\partial x^{2}}\right)\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)} \\
+\mathbb{E}\left[\left(X - \mu_{X}\right)\left(Y - \mu_{Y}\right)\right]\left.\left(\dfrac{\partial^{2}f\left(x, y\right)}{\partial x \partial y}\right)\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)} + \mathbb{E}\left[\left(Y - \mu_{Y}\right)^{2}\right]\left.\left(\dfrac{\partial^{2}f\left(x, y\right)}{\partial y^{2}}\right)\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}
\end{multline}
For the case $f\left(x, y\right) = \dfrac{x}{y}$, the second partial derivatives evaluate to
\begin{gather}
\dfrac{\partial^{2}f\left(x, y\right)}{\partial x^{2}} = 0 \\
\dfrac{\partial^{2}f\left(x, y\right)}{\partial x \partial y} = -\dfrac{1}{y^{2}} \\
\dfrac{\partial^{2}f\left(x, y\right)}{\partial y^{2}} = \dfrac{2x}{y^{3}}
\end{gather}
Hence
\begin{align}
\mathbb{E}\left[\dfrac{X}{Y}\right] &\approx \dfrac{\mu_{X}}{\mu_{Y}} - \mathbb{E}\left[\left(X - \mu_{X}\right)\left(Y - \mu_{Y}\right)\right]\dfrac{1}{\mu_{Y}^{2}} + \mathbb{E}\left[\left(Y - \mu_{Y}\right)^{2}\right]\dfrac{\mu_{X}}{\mu_{Y}^{3}} \\
&\approx \dfrac{\mathbb{E}\left[X\right]}{\mathbb{E}\left[Y\right]} - \dfrac{\operatorname{Cov}\left(X, Y\right)}{\mathbb{E}\left[Y\right]^{2}} + \operatorname{Var}\left(Y\right)\dfrac{\mathbb{E}\left[X\right]}{\mathbb{E}\left[Y\right]^{3}}
\end{align}
To approximate the variance, taking a first order Taylor approximation (for simplicity) gives
\begin{align}
\operatorname{Var}\left(f\left(X, Y\right)\right) &\approx \operatorname{Var}\left(f\left(\mu_{X}, \mu_{Y}\right) + \left.\nabla f\left(x, y\right)^{\top}\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}\begin{bmatrix} X - \mu_{X} \\ Y - \mu_{Y}\end{bmatrix}\right) \\
&\approx \operatorname{Var}\left(X\left.\dfrac{\partial f\left(x, y\right)}{\partial x}\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)} + Y\left.\dfrac{\partial f\left(x, y\right)}{\partial y}\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}\right) \\
&\begin{multlined}\approx \left.\left(\dfrac{\partial f\left(x, y\right)}{\partial x}\right)^{2}\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}\operatorname{Var}\left(X\right) \\
+ 2\left.\left(\dfrac{\partial f\left(x, y\right)}{\partial x}\cdot\dfrac{\partial f\left(x, y\right)}{\partial y}\right)\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}\operatorname{Cov}\left(X, Y\right) + \left.\left(\dfrac{\partial f\left(x, y\right)}{\partial y}\right)^{2}\right|_{\left(x, y\right) = \left(\mu_{X}, \mu_{Y}\right)}\operatorname{Var}\left(Y\right) \end{multlined}
\end{align}
Again by treating $f\left(x, y\right) = \dfrac{x}{y}$, 
\begin{align}
\operatorname{Var}\left(\dfrac{X}{Y}\right) &\approx \dfrac{1}{\mu_{Y}^{2}}\operatorname{Var}\left(X\right) - 2\dfrac{\mu_{X}}{\mu_{Y}^{3}}\operatorname{Cov}\left(X, Y\right) + \dfrac{\mu_{X}^{2}}{\mu_{Y}^{4}}\operatorname{Var}\left(Y\right) \\
&\approx \dfrac{\operatorname{Var}\left(X\right)}{\mathbb{E}\left[Y\right]^{2}} - 2\dfrac{\mathbb{E}\left[X\right]}{\mathbb{E}\left[Y\right]^{3}}\operatorname{Cov}\left(X, Y\right) + \dfrac{\mathbb{E}\left[X\right]^{2}}{\mathbb{E}\left[Y\right]^{4}}\operatorname{Var}\left(Y\right)
\end{align}

\subsection{Decompositions of Random Variables}

The distribution of random variable $Z$ is said to be decomposable if $Z$ can be expressed as the sum of non-constant independent random variables $X$ and $Y$, ie. $Z = X + Y$. If it is not possible, the distribution of $Z$ is said to be indecomposable.

\subsubsection{Divisibility}

The distribution of random variable $Z$ is said to be divisible if $Z$ can be expressed as the sum of independent and identically distributed random variables $X_{1}$ and $X_{2}$. If $Z$ can be expressed as a sum of arbitrarily many i.i.d. random variables, then the distribution of $Z$ is said to be infinitely divisible.

\subsubsection{Cram\'{e}r's Decomposition Theorem}

Cram\'{e}r's decomposition theorem states that if $X$ and $Y$ are independent real-valued random variables whose sum $X + Y$ is a normal random variable, then $X$ and $Y$ must also be normal. By induction, if the sum of finitely many independent random variables is normal, then the summands must be normal. 


\section{Families of Continuous Univariate Probability Distributions}

\subsection{Dirac Delta Distribution}

\subsection{Continuous Uniform Distribution}

\subsection{Exponential Distribution}

\subsubsection{Hyperexponential Distribution}

\subsection{Gaussian Distribution}

\subsubsection{Q Function}

The Q function is the complementary cumulative distribution function of the standard Gaussian distribution. It is denoted
\begin{align}
Q\left(x\right) &= 1 - \Phi\left(x\right) \\
&= \int_{x}^{\infty}\phi\left(u\right)du \\
&= \int_{x}^{\infty}\dfrac{e^{-u^{2}/2}}{\sqrt{2\pi}}du
\end{align}

\subsubsection{Mill's Ratio}

Mill's ratio of a distribution is defined as the ratio of the complementary cumulative distribution function to the probability density function. For the standard Gaussian distribution, Mill's ratio can be written in terms of the Q function as $\dfrac{Q\left(x\right)}{\phi\left(x\right)}$. This ratio satisfies some well-known inequalities. Firstly, we can show
\begin{align}
Q\left(x\right) &= \int_{x}^{\infty}\phi\left(u\right)du \\
& < \int_{x}^{\infty}\dfrac{u}{x}\phi\left(u\right)du
\end{align}
for $x > 0$, since the ratio $u/x > 1$ for $u > x$ and $\phi\left(u\right) > 0$ everywhere. Then with a change of variables $v = \dfrac{u^{2}}{2}$ (noting that $\dfrac{dv}{du} = u$):
\begin{align}
\int_{x}^{\infty}\dfrac{u}{x}\phi\left(u\right)du &= \int_{x}^{\infty}\dfrac{u}{x}\cdot \dfrac{e^{-u^{2}/2}}{\sqrt{2\pi}}du \\
&= \int_{x^{2}/2}^{\infty}\dfrac{e^{-v}}{x\sqrt{2\pi}}dv \\
&= -\left[\dfrac{1}{x\sqrt{2\pi}}e^{-v}\right]_{v=x^{2}/2}^{v = \infty} \\
&= \dfrac{e^{-x^{2}/2}}{x\sqrt{2\pi}} \\
&= \dfrac{\phi\left(x\right)}{x}
\end{align}
Hence we have the upper bound on Mill's ratio
\begin{equation}
\dfrac{Q\left(x\right)}{\phi\left(x\right)} < \dfrac{1}{x}
\end{equation}
for $x > 0$. A lower bound can be derived in a similar fashion. Start with
\begin{align}
\left(1 + \dfrac{1}{x^{2}}\right)Q\left(x\right) &= \int_{x}^{\infty}\left(1 + \dfrac{1}{x^{2}}\right)\phi\left(u\right)du \\
&> \int_{x}^{\infty}\left(1 + \dfrac{1}{u^{2}}\right)\phi\left(u\right)du
\end{align}
since $\dfrac{1}{x^{2}} > \dfrac{1}{u^{2}}$ for $u > x$ and $x$ while $\phi\left(u\right) > 0$. Now note that the derivative of $\phi\left(u\right)$ is
\begin{equation}
\dfrac{d\phi\left(u\right)}{du} = -u\phi\left(u\right)
\end{equation}
Then using the quotient rule, the derivative of $-\phi\left(u\right)/u$ is
\begin{align}
\dfrac{d}{du}\left(-\dfrac{\phi\left(u\right)}{u}\right) &= -\dfrac{-u^{2}\phi\left(u\right) - \phi\left(u\right)}{u^{2}} \\
&= \left(1 + \dfrac{1}{u^{2}}\right)\phi\left(u\right)
\end{align}
Hence
\begin{align}
\int_{x}^{\infty}\left(1 + \dfrac{1}{u^{2}}\right)\phi\left(u\right)du &= \left[-\dfrac{\phi\left(u\right)}{u}\right]_{u = x}^{u = \infty} \\
&= \dfrac{\phi\left(x\right)}{x}
\end{align}
This gives the lower bound
\begin{gather}
\left(1 + \dfrac{1}{x^{2}}\right)Q\left(x\right) > \dfrac{\phi\left(x\right)}{x} \\
\dfrac{x}{1 + x^{2}} < \dfrac{Q\left(x\right)}{\phi\left(x\right)}
\end{gather}
for $x > 0$.

\subsubsection{Truncated Gaussian Distribution}

\subsubsection{Folded Gaussian Distribution}

\subsubsection{Half-Gaussian Distribution}

\subsubsection{Mixture Gaussian Distribution}

\subsubsection{Lognormal Distribution}

\subsection{Laplace Distribution}

\subsection{Cauchy Distribution}

\subsection{Gamma Distribution}

\subsubsection{Gamma Function}

The (complete) Gamma function is an interpolation of the factorials defined by
\begin{equation}
\Gamma\left(z\right) = \int_{0}^{\infty}y^{z - 1}e^{-y}dy
\end{equation}
which is continuous over the positive reals, and satisfies the recurrence relation
\begin{gather}
\Gamma\left(1\right) = 1 \\
\Gamma\left(z + 1\right) = z\Gamma\left(z\right)
\end{gather}
Thus, the relation between the Gamma function and the factorials (for non-negative integer values of $z$) is
\begin{equation}
z! = \Gamma\left(z + 1\right)
\end{equation}

\subsubsection{Incomplete Gamma Functions}

The incomplete Gamma functions are defined as integrals with the same integrand as the Gamma function, but with incomplete limits. The lower incomplete Gamma function (sometimes known as the incomplete Gamma function of the first kind) is defined as
\begin{equation}
\gamma\left(z, x\right) = \int_{0}^{x}y^{z - 1}e^{-y}dy
\end{equation}
while the upper incomplete Gamma function (sometimes known as the incomplete Gamma function of the second kind) is defined as
\begin{equation}
\Gamma\left(z, x\right) = \int_{x}^{\infty}y^{z - 1}e^{-y}dy
\end{equation}
where we can see that
\begin{equation}
\gamma\left(z, x\right) + \Gamma\left(z, x\right) = \Gamma\left(z\right)
\end{equation}

\subsubsection{Bayesian's Gamma Distribution}

From the definitions of the incomplete and complete Gamma functions we can see that $\dfrac{\gamma\left(z, x\right)}{\Gamma\left(z\right)}$ (sometimes known as the regularised Gamma function) is a valid cumulative distribution function in $x$ defined over the positive reals. More generally, by introducing a parameter $\beta >0$ and another parameter $\alpha = z > 0$, we can define a cumulative distribution function
\begin{equation}
F\left(x\right) = \dfrac{\gamma\left(\alpha, \beta x\right)}{\Gamma\left(\alpha\right)}
\end{equation}
since $\gamma\left(\alpha, \beta x\right) \leq \Gamma\left(\alpha\right)$ for all $\beta > 0$ and $x > 0$. This is known as the (Bayesian's) Gamma distribution with shape parameter $\alpha$ and scale parameter $\beta$. We can differentiate the cumulative distribution function using the Fundamental Theorem of Calculus to obtain the density function $f\left(x\right)$:
\begin{align}
f\left(x\right) &= \dfrac{dF\left(x\right)}{dx} \\
&= \dfrac{1}{\Gamma\left(\alpha\right)}\dfrac{d\gamma\left(\alpha, \beta x\right)}{dx} \\
&= \dfrac{1}{\Gamma\left(\alpha\right)}\dfrac{d}{dx}\int_{0}^{\beta x}y^{\alpha - 1}e^{-y}dy \\
&= \dfrac{1}{\Gamma\left(\alpha\right)}\cdot\dfrac{1}{\beta}\left(\beta x\right)^{\alpha - 1}e^{-\beta x} \\
&= \dfrac{\beta^{\alpha}x^{\alpha - 1}e^{-\beta x}}{\Gamma\left(\alpha\right)}
\end{align}

\subsubsection{Econometrician's Gamma Distribution}

The econometrician's Gamma distribution is an alternative parametrisation with $k = \alpha$ as the shape parameter and $\theta = \dfrac{1}{\beta}$ as the scale parameter. The PDF may be written as
\begin{equation}
f\left(x\right) = \dfrac{x^{k - 1}e^{-x/\theta}}{\theta^{k}\Gamma\left(k\right)}
\end{equation}

\subsection{Beta Distribution}

\subsubsection{Beta Function}

Also known as the Euler integral of the first kind, the Beta function is defined as
\begin{equation}
\operatorname{B}\left(z, y\right) = \int_{0}^{1}t^{z - 1}\left(1 - t\right)^{y - 1}dt
\end{equation}
for $\operatorname{Re}\left(y\right) > 0$ (the Beta function is defined on the complex numbers). The Beta function has a special relation with the Gamma function as follows:
\begin{equation}
\operatorname{B}\left(z, y\right) = \dfrac{\Gamma\left(z\right)\Gamma\left(y\right)}{\Gamma\left(z + y\right)}
\end{equation}
This can be shown by first writing out the definitions of the product of two Gamma functions:
\begin{align}
\Gamma\left(z\right)\Gamma\left(y\right) &= \left(\int_{0}^{\infty}u^{z - 1}e^{-u}du\right)\left(\int_{0}^{\infty}v^{y - 1}e^{-v}dv\right) \\
&= \int_{0}^{\infty}\int_{0}^{\infty}u^{z - 1}v^{y - 1}e^{-u - v}dudv
\end{align}
Introduce the change in variables $u = st$, $v = s\left(1 - t\right)$, which when rearranged becomes $t = \dfrac{u}{v + u}$ and $s = v + u$. Hence for all $u \in \left(0, \infty\right), v \in \left(0, \infty\right)$, we have $t \in\left(0, 1\right), s \in\left(0, \infty\right)$. Hence rewriting the integral gives
\begin{equation}
\Gamma\left(z\right)\Gamma\left(y\right)  = \int_{0}^{\infty}\int_{0}^{1}\left(st\right)^{z - 1}\left[s\left(1 - t\right)\right]^{y - 1}e^{-s}\det\left(\mathbf{J}\right)dtds
\end{equation}
where the Jacobian determinant
\begin{equation}
\det\left(\mathbf{J}\right) = \det\left(\begin{bmatrix}\dfrac{\partial}{\partial t}st & \dfrac{\partial}{\partial s}st\\
\dfrac{\partial}{\partial t}s\left(1-t\right) & \dfrac{\partial}{\partial s}s\left(1-t\right)
\end{bmatrix}\right) = s\left(1 - t\right) + st = s
\end{equation}
So
\begin{align}
\Gamma\left(z\right)\Gamma\left(y\right) &= \int_{0}^{\infty}\int_{0}^{1}\left(st\right)^{z - 1}\left[s\left(1 - t\right)\right]^{y - 1}e^{-s}sdtds \\
&= \int_{0}^{\infty}\int_{0}^{1}\left(s^{z + y - 1}e^{-s}\right)t^{z - 1}\left(1 - t\right)^{y - 1}dtds \\
&= \left(\int_{0}^{\infty}\left(s^{z + y - 1}e^{-s}\right)ds\right)\left(\int_{0}^{1}t^{z - 1}\left(1 - t\right)^{y - 1}dt\right) \\
&= \Gamma\left(z + y\right)\operatorname{B}\left(z, y\right)
\end{align}
as required. From this property, we can also deduce the Beta function is symmetric, ie. $\operatorname{B}\left(z, y\right) = \operatorname{B}\left(y, z\right)$. The incomplete beta function is the Beta function with incomplete  limits, ie.
\begin{equation}
\operatorname{B}\left(x; z, y\right) = \int_{0}^{x}t^{z - 1}\left(1 - t\right)^{y - 1}dt
\end{equation}
The regularised incomplete Beta function is defined as $\dfrac{\operatorname{B}\left(x; z, y\right)}{\operatorname{B}\left(z, y\right)}$.

\subsubsection{Beta Distribution of the First Kind}

From the regularised incomplete Beta function we can see that it is a valid cumulative distribution function in $x$, defined on $\left[0, 1\right]$. Hence the Beta distribution (also known as the Beta distribution of the first kind) has cumulative distribution function
\begin{equation}
F\left(x\right) = \dfrac{\operatorname{B}\left(x; \alpha, \beta\right)}{\operatorname{B}\left(\alpha, \beta\right)}
\end{equation}
where $\alpha > 0$ and $\beta > 0$ are shape parameters. Differentiating the cumulative distribution using the Fundamental Theorem of Calculus yields the probability density function $f\left(x\right)$:
\begin{align}
f\left(x\right) &= \dfrac{dF\left(x\right)}{dx} \\
&= \dfrac{1}{\operatorname{B}\left(\alpha, \beta\right)}\dfrac{d\operatorname{B}\left(x; \alpha, \beta\right)}{dx} \\
&= \dfrac{1}{\operatorname{B}\left(\alpha, \beta\right)}\dfrac{d}{dx}\int_{0}^{x}t^{\alpha - 1}\left(1-t\right)^{\beta - 1}dt \\
&= \dfrac{x^{\alpha - 1}\left(1-x\right)^{\beta - 1}}{\operatorname{B}\left(\alpha, \beta\right)}
\end{align}
with an alternative form in terms of Gamma functions as
\begin{equation}
f\left(x\right) = \dfrac{\Gamma\left(\alpha + \beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}x^{\alpha - 1}\left(1-x\right)^{\beta - 1}
\end{equation}
If $\alpha$ and $\beta$ are positive integers, then the Beta distribution in terms of factorials is
\begin{equation}
f\left(x\right) = \dfrac{\left(\alpha + \beta - 1\right)!}{\left(\alpha - 1\right)!\left(\beta - 1\right)!}x^{\alpha - 1}\left(1-x\right)^{\beta - 1}
\end{equation}
By letting $n = \alpha + \beta - 2$ and $r = \alpha - 1$, an alternative parametrisation is
\begin{equation}
f\left(x\right) = \dfrac{\left(n + 1\right)!}{r!\left(n - r\right)!}x^{r}\left(1-x\right)^{n - r}
\end{equation}

\subsection{Chi-Squared Distribution}

The chi-squared distribution with $k$ degrees of freedom is the distribution of the sum of squares of $k$ standard normal random variables. Thus if $Z_{1}, \dots, Z_{k}$ are standard normal random variables, then
\begin{equation}
X = Z_{1}^{2} + \dots + Z_{k}^{2}
\end{equation}
is chi-squared distributed with $k$ degrees of freedom. This is commonly denoted $X \sim \chi_{k}^{2}$. Let $f\left(x\right)$ be the probability density function of $X$ which has support $\left[0, \infty\right)$. We can see that the following two integrals should be the same (integrating to $1$):
\begin{equation}
\int_{0}^{\infty}f\left(x\right)dx = \int_{x = 0}^{x = \infty}\int_{z_{1}^{2} + \dots + z_{n}^{2} = x}\mathcal{N}\left(z_{1}\right)\dots\mathcal{N}\left(z_{n}\right)dz_{1}\dots dz_{n}
\end{equation}
where $\mathcal{N}\left(\cdot\right)$ is the standard normal PDF. Hence we equate
\begin{equation}
f\left(x\right)dx = \int_{z_{1}^{2} + \dots + z_{n}^{2} = x}\mathcal{N}\left(z_{1}\right)\dots\mathcal{N}\left(z_{n}\right)dz_{1}\dots dz_{n}
\end{equation}
Using the expressions for the PDFs:
\begin{equation}
f\left(x\right)dx = \int_{z_{1}^{2} + \dots + z_{n}^{2} = x}\dfrac{\exp\left[-\left(z_{1}^{2} + \dots + z_{n}^{2}\right)/2\right]}{\left(2\pi\right)^{k/2}}dz_{1}\dots dz_{n}
\end{equation}
Since $x = z_{1}^{2} + \dots + z_{n}^{2}$ is constant with respect to the integral,
\begin{equation}
f\left(x\right)dx = \dfrac{e^{-x/2}}{\left(2\pi\right)^{k/2}}\int_{z_{1}^{2} + \dots + z_{n}^{2} = x}dz_{1}\dots dz_{n}
\end{equation}
Letting $x = r^{2}$, the integral $\int_{z_{1}^{2} + \dots + z_{n}^{2} = r^{2}}dz_{1}\dots dz_{n}$ is the volume of an infinitesimally thin shell of radius $r$ with thickness $dr$. Since $r = \sqrt{x}$, then
\begin{gather}
\dfrac{dr}{dx} = \dfrac{1}{2\sqrt{x}} \\
dr = \dfrac{1}{2\sqrt{x}}dx
\end{gather}
The surface area of a $k$-dimensional hypersphere with radius $r$ is $\dfrac{2r^{k-1}\pi^{k/2}}{\Gamma\left(k/2\right)}$ hence
\begin{equation}
f\left(x\right)dx = \dfrac{e^{-x/2}}{\left(2\pi\right)^{k/2}}\dfrac{2r^{k-1}\pi^{k/2}}{\Gamma\left(k/2\right)}\dfrac{1}{2\sqrt{x}}dx
\end{equation}
Substituting $r = \sqrt{x}$ and cancelling terms, this gives the expression for the PDF of the chi-squared distribution
\begin{equation}
f\left(x\right) = \dfrac{e^{-x/2}x^{k/2 - 1}}{2^{k/2}\Gamma\left(k/2\right)}
\end{equation}

\subsubsection{Chi-Distribution}

The chi-distribution is the distribution of the square root of the sum of squares of $k$ standard normal random variables. The density function $f_{R}\left(r\right)$ can be derived the same way as with the chi-squared distribution. As above, except without substituting $r$ for $x$, we get
\begin{equation}
f_{R}\left(r\right)dr = \dfrac{e^{-r^{2}/2}}{\left(2\pi\right)^{k/2}}\dfrac{2r^{k-1}\pi^{k/2}}{\Gamma\left(k/2\right)}dr
\end{equation}
Hence cancellation yields
\begin{equation}
f_{R}\left(r\right) = \dfrac{e^{-r^{2}/2}r^{k - 1}}{2^{k/2 - 1}\Gamma\left(k/2\right)}
\end{equation}
on support $r \in \left[0, \infty\right)$.

\subsubsection{Rayleigh Distribution}

The Rayleigh distribution is the chi-squared distribution with 2 degrees of freedom (ie. Euclidean norm of two uncorrelated Gaussian random variables). Its PDF is given by
\begin{align}
f_{R}\left(r\right) &= \left.\dfrac{e^{-r^{2}/2}r^{k - 1}}{2^{k/2 - 1}\Gamma\left(k/2\right)}\right|_{k = 2}  \\
&= re^{r^{2}/2}
\end{align}
A scale parameter $\sigma > 0$  can be introduced to represent the standard deviation of the two Gaussian random variables.
\begin{equation}
f_{R}\left(r\right) = \dfrac{r}{\sigma^{2}}e^{r^{2}/2\sigma^{2}}
\end{equation}
Integrating this, we find that the CDF has expression
\begin{equation}
F_{R}\left(r\right) = 1 - e^{r^{2}/2\sigma^{2}}
\end{equation}
Note that the the PDF of the Rayleigh distribution with $\sigma = 1$ is the vertical reflection (and also the horizontal reflection) of the derivative of the standard Gaussian PDF. That is,
\begin{equation}
\left.f_{R}\left(r\right)\right|_{\sigma = 1} = -\left.\dfrac{d\mathcal{N}\left(z; 0, 1^{2}\right)}{dz}\right|_{z = r} = \left.\dfrac{d\mathcal{N}\left(z; 0, 1^{2}\right)}{dz}\right|_{z = -r}
\end{equation}

\subsection{Student's $t$ Distribution}

A Student $t$'s distribution (or simply $t$ distribution) with $k = n - 1$ degrees of freedom can be defined as the distribution of the random variable
\begin{equation}
T = \dfrac{\overline{X} - \mu}{S/\sqrt{n}}
\end{equation}
where $\overline{X}$ is the sample mean from i.i.d. $X_{1}, \dots, X_{n} \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$ and
\begin{equation}
S = \sqrt{\dfrac{\sum_{i = 1}^{n}\left(X_{i} - \overline{X}\right)^{2}}{n - 1}}
\end{equation}
is the sample standard deviation. To derive the density of the $t$ distribution, we first show how the $t$ distribution relates to the chi-squared and the normal distribution. The sum of squares of the standardised random variables is given by
\begin{equation}
\sum_{i = 1}^{n}Z_{i}^{2} = \sum_{i = 1}^{n}\left(\dfrac{X_{i} - \mu}{\sigma}\right)^{2}
\end{equation}
We can make the following rearrangement:
\begin{align}
\sum_{i = 1}^{n}\left(X_{i} - \mu\right)^{2} &= \sum_{i = 1}^{n}\left(X_{i} -  \overline{X} + \overline{X} - \mu\right)^{2} \\
&= \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}+2\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)\left(\overline{X}-\mu\right)+\sum_{i=1}^{n}\left(\overline{X}-\mu\right)^{2} \\
&= \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}+2\left(\overline{X}-\mu\right)\green\cancelto{0}{\black\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)}\black+\sum_{i=1}^{n}\left(\overline{X}-\mu\right)^{2} \\
&= \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}+n\left(\overline{X}-\mu\right)^{2}
\end{align}
Hence
\begin{align}
\sum_{i = 1}^{n}\left(\dfrac{X_{i} - \mu}{\sigma}\right)^{2} &= \sum_{i=1}^{n}\left(\dfrac{X_{i}-\overline{X}}{\sigma}\right)^{2}+n\left(\dfrac{\overline{X}-\mu}{\sigma}\right)^{2} \\
&= \sum_{i=1}^{n}\left(\dfrac{X_{i}-\overline{X}}{\sigma}\right)^{2}+\left(\sqrt{n}\dfrac{\overline{X}-\mu}{\sigma}\right)^{2}
\end{align}
We reason that as the chi-squared distribution is the sum i.i.d. standard normal random variables, if $Q_{1}\sim\chi_{n_{1}}^{2}$ and $Q_{2}\sim\chi_{n_{2}}^{2}$, then $Q_{3} = Q_{1} + Q_{2} \sim \chi_{n_{1} + n_{2}}^{2}$. Therefore if given some $Q_{3}$ and $Q_{2}$, we deduce that there exists a random variable $Q_{1} \sim \chi_{n_{1}}^{2}$ independent of $Q_{2}$. Notice that the left hand side of the expression above means $\sum_{i = 1}^{n}Z_{i}^{2} \sim \chi_{n}^{2}$. Further note that $\sqrt{n}\dfrac{\overline{X}-\mu}{\sigma} \sim \mathcal{N}\left(0, 1\right)$. Then the term $\sum_{i=1}^{n}\left(\dfrac{X_{i}-\overline{X}}{\sigma}\right)^{2}$ should be chi-squared distributed with $k = n - 1$ degrees of freedom. Define the random variable
\begin{equation}
V = \sum_{i=1}^{n}\left(\dfrac{X_{i}-\overline{X}}{\sigma}\right)^{2}
\end{equation}
and notice that from the definition of $S$:
\begin{equation}
V = k\dfrac{S^{2}}{\sigma^{2}}
\end{equation}
Hence rearranging $T$ yields
\begin{align}
T &= \dfrac{\overline{X} - \mu}{S/\sqrt{n}} \\
&= \dfrac{\sqrt{n}\left(\overline{X} - \mu\right)}{\sigma\sqrt{V/k}} \\
&= \dfrac{Z}{\sqrt{V/k}}
\end{align}
We derive the $t$ distribution by deriving the distribution of $T = \dfrac{Z}{\sqrt{V/k}}$ where $Z\sim\mathcal{N}\left(0, 1\right)$ and $V\sim\chi_{k}^{2}$ independent of $Z$. In following the same approach taken to derive the ratio distribution, the density of $T$ can be expressed as
\begin{align}
f_{T}\left(t\right) &= \int\int_{\left\{ z,v:z/\sqrt{v/k}=t\right\} }f_{Z}\left(z\right)f_{V}\left(v\right)dzdv \\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{Z}\left(z\right)f_{V}\left(v\right)\delta\left(t-\dfrac{z}{\sqrt{v/k}}\right)dzdv
\end{align}
where $\delta\left(\cdot\right)$ is the Dirac delta function and the densities $f_{Z}\left(z\right)$ and $f_{V}\left(v\right)$ are respectively
\begin{gather}
f_{Z}\left(z\right)=\dfrac{e^{-z^{2}/2}}{\sqrt{2\pi}} \\
f_{V}\left(v\right)=\dfrac{e^{-v/2}v^{k/2-1}}{2^{k/2}\Gamma\left(k/2\right)}
\end{gather}
The support of $f_{V}\left(v\right)$ is $\left[0, \infty\right)$, so
\begin{equation}
f_{T}\left(t\right)=\int_{0}^{\infty}\int_{-\infty}^{\infty}f_{Z}\left(z\right)f_{V}\left(v\right)\delta\left(t-\dfrac{z}{\sqrt{v/k}}\right)dzdv
\end{equation}
Make the substitution $y = \dfrac{z}{\sqrt{v/k}}$, ie. $z = y\sqrt{v/k}$ and $dz = \sqrt{v/k}dy$.
\begin{align}
f_{T}\left(t\right) &= \int_{0}^{\infty}\int_{-\infty}^{\infty}f_{Z}\left(y\sqrt{\dfrac{v}{k}}\right)f_{V}\left(v\right)\delta\left(t-y\right)\sqrt{\dfrac{v}{k}}dydv \\
&= \int_{0}^{\infty}\sqrt{\dfrac{v}{k}}f_{V}\left(v\right)\int_{-\infty}^{\infty}f_{Z}\left(y\sqrt{\dfrac{v}{k}}\right)\delta\left(t-y\right)dydv
\end{align}
Evaluating the inner integral gives
\begin{equation}
\int_{-\infty}^{\infty}f_{Z}\left(y\sqrt{\dfrac{v}{k}}\right)\delta\left(t-y\right)dy=f_{Z}\left(t\sqrt{\dfrac{v}{k}}\right)
\end{equation}
Hence
\begin{equation}
f_{T}\left(t\right)=\int_{0}^{\infty}\sqrt{\dfrac{v}{k}}f_{V}\left(v\right)f_{Z}\left(t\sqrt{\dfrac{v}{k}}\right)dv
\end{equation}
Note that $f_{Z}\left(t\sqrt{\dfrac{v}{k}}\right)=\dfrac{\exp\left(-\frac{t^{2}v}{2k}\right)}{\sqrt{2\pi}}$. Substituting the expressions for the densities,
\begin{align}
f_{T}\left(t\right) &= \dfrac{1}{\sqrt{k}}\dfrac{1}{\sqrt{2\pi}}\dfrac{1}{2^{k/2}\Gamma\left(k/2\right)}\int_{0}^{\infty}\sqrt{v}\cdot v^{k/2-1}\cdot e^{-v/2}\cdot\exp\left(-\dfrac{t^{2}v}{2k}\right)dv \\
&= \dfrac{1}{\sqrt{2\pi k}2^{k/2}\Gamma\left(k/2\right)}\int_{0}^{\infty}v^{\frac{k-1}{2}}\cdot\exp\left[-\left(1+\dfrac{t^{2}}{k}\right)\dfrac{v}{2}\right]dv
\end{align}
Make the substitution $u = \left(1+\dfrac{t^{2}}{k}\right)\dfrac{v}{2}$ so $dv = 2\left(1+\dfrac{t^{2}}{k}\right)^{-1}du$:
\begin{align}
f_{T}\left(t\right) &= \dfrac{1}{\sqrt{\pi k}\Gamma\left(k/2\right)}\dfrac{1}{2^{\frac{k+1}{2}}}\int_{0}^{\infty}\left[2u\left(1+\dfrac{t^{2}}{k}\right)^{-1}\right]^{\frac{k-1}{2}}\cdot e^{-u}\cdot2\left(1+\dfrac{t^{2}}{k}\right)^{-1}du \\
&= \dfrac{1}{\sqrt{\pi k}\Gamma\left(k/2\right)}\dfrac{2^{\frac{k-1}{2}}\cdot2}{2^{\frac{k+1}{2}}}\left(1+\dfrac{t^{2}}{k}\right)^{\frac{-k+1}{2}-1}\int_{0}^{\infty}u^{\frac{k+1}{2}-1}\cdot e^{-u}du
\end{align}
Notice the integral is the Gamma function of $\dfrac{k+1}{2}$ so
\begin{equation}
f_{T}\left(t\right)=\dfrac{\left(1+t^{2}/k\right)^{-\frac{\left(k+1\right)}{2}}}{\sqrt{\pi k}\Gamma\left(k/2\right)}\Gamma\left(\dfrac{k+1}{2}\right)
\end{equation}
Applying the property of the Beta function, $\operatorname{B}\left(\dfrac{1}{2}, \dfrac{k}{2}\right) = \dfrac{\Gamma\left(1/2\right)\Gamma\left(k/2\right)}{\Gamma\left(\frac{k+1}{2}\right)}$. The term $\Gamma\left(1/2\right)$ evaluates to $\sqrt{\pi}$, so this means the density of the $t$-distribution can be alternatively expressed as
\begin{equation}
f_{T}\left(t\right)= \dfrac{1}{\sqrt{k}\operatorname{B}\left(1/2, k/2\right)}\left(1+\dfrac{t^{2}}{k}\right)^{-\frac{\left(k+1\right)}{2}}
\end{equation}

\subsection{$F$-Distribution}

Suppose $U$ and $V$ are independent Chi-squared random variables, with degrees of freedom $n$ and $m$ respectively. The $F$-distribution can be defined as the distribution of the random variable
\begin{equation}
X = \dfrac{U/m}{V/n}
\end{equation}
Alternatively, we may characterise the $F$-distribution as follows. Recall as shown in the derivation of the $t$ distribution that for a $m+1$ sample of normal random variables $Y_{1}, \dots, Y_{m + 1}$ with variance $\sigma^{2}$, the standardised sample variance follows a Chi-squared distribution with $m$ degrees of freedom:
\begin{equation}
\dfrac{1}{\sigma^{2}} \sum_{i = 1}^{m + 1}\left(Y_{i} - \overline{Y}\right)^{2} \sim \chi_{m}^{2}
\end{equation}
So letting $\dfrac{1}{\sigma^{2}} \sum_{i = 1}^{m + 1}\left(Y_{i} - \overline{Y}\right)^{2} = U$, then
\begin{align}
\dfrac{U}{m} &= \dfrac{1}{\sigma^{2}}\cdot\dfrac{1}{m}\sum_{i = 1}^{m + 1}\left(Y_{i} - \overline{Y}\right)^{2} \\
&= \dfrac{S_{Y}^{2}}{\sigma^{2}}
\end{align}
where $S_{Y}^{2}$ is the sample variance. Likewise, if we have an $n + 1$ sample of normal random variables $W_{1}, \dots, W_{n + 1}$ with variance $\varsigma^{2}$ and sample variance $S_{W}^{2}$, then
\begin{equation}
\dfrac{V}{n} = \dfrac{S_{W}^{2}}{\varsigma^{2}}
\end{equation}
Therefore we may characterise the $F$-distribution as the distribution of
\begin{equation}
X = \dfrac{S_{Y}^{2}/\sigma^{2}}{S_{W}^{2}/\varsigma^{2}}
\end{equation}
which is the standardised ratio of sample variances from two independent normal samples of different sizes. To obtain the probability density function, we use the expression for the ratio distribution of independent Chi-squared random variables. For simplicity, we first find the distribution of $X' = \dfrac{U}{V}$ (ie. suppose that $m = 1, n = 1$). Using the ratio distribution, the density of $X'$ in terms of the densities $f_{U}\left(u\right)$ and $f_{V}\left(v\right)$ for $U$ and $V$ respectively is
\begin{align}
f_{X'}\left(x\right) &= \int_{-\infty}^{\infty}f_{U}\left(uv\right)f_{V}\left(v\right)\left|v\right|dv \\
&= \int_{0}^{\infty}f_{U}\left(uv\right)f_{V}\left(v\right)vdv
\end{align}
since Chi-squared random variables are always non-negative. Applying the expression for the Chi-squared distribution, we get
\begin{align}
f_{X'}\left(x\right) &= \int_{0}^{\infty}\dfrac{e^{-xv/2}\left(xv\right)^{m/2 - 1}}{2^{m/2}\Gamma\left(m/2\right)}\cdot\dfrac{-e^{v/2}v^{n/2 - 1}}{2^{n/2}\Gamma\left(n/2\right)}vdv \\
&= \dfrac{x^{m/2 - 1}}{2^{\left(m + n\right)/2}\Gamma\left(m/2\right)\Gamma\left(n/2\right)}\int_{0}^{\infty}v^{\left(m + n\right)/2-2}ve^{-v\left(x + 1\right)/2}dv \\
&= \dfrac{x^{m/2 - 1}}{2^{\left(m + n\right)/2}\Gamma\left(m/2\right)\Gamma\left(n/2\right)}\int_{0}^{\infty}v^{\left(m + n\right)/2-1}e^{-v\left(x + 1\right)/2}dv
\end{align}
Make the substitution $t = v\left(x + 1\right)/2$ so that $v = t\left(\dfrac{x + 1}{2}\right)^{-1}$ and $dv = \left(\dfrac{x + 1}{2}\right)^{-1}dt$:
\begin{align}
f_{X'}\left(x\right) &= \dfrac{x^{m/2 - 1}}{2^{\left(m + n\right)/2}\Gamma\left(m/2\right)\Gamma\left(n/2\right)}\int_{0}^{\infty}t^{\left(m + n\right)/2-1}\left(\dfrac{x + 1}{2}\right)^{-\left(m + n\right)/2 + 1}e^{-t}\left(\dfrac{x + 1}{2}\right)^{-1}dt \\
&= \dfrac{x^{m/2 - 1}}{2^{\left(m + n\right)/2}\Gamma\left(m/2\right)\Gamma\left(n/2\right)}\left(\dfrac{x + 1}{2}\right)^{-\left(m + n\right)/2}\int_{0}^{\infty}t^{\left(m + n\right)/2-1}e^{-t}dt
\end{align}
Recognise that the integral becomes a Gamma integral, so using the definition of the Gamma function,
\begin{align}
f_{X'}\left(x\right) &= \dfrac{x^{m/2 - 1}\Gamma\left(\left(m + n\right)/2\right)}{2^{\left(m + n\right)/2}\Gamma\left(m/2\right)\Gamma\left(n/2\right)}\left(\dfrac{x + 1}{2}\right)^{-\left(m + n\right)/2} \\
&= \dfrac{x^{m/2 - 1}\Gamma\left(\left(m + n\right)/2\right)}{\Gamma\left(m/2\right)\Gamma\left(n/2\right)\left(x + 1\right)^{\left(m + n\right)/2}} 
\end{align}
Since $X = X'\dfrac{m}{n}$ then by the scaling properties of the density function
\begin{align}
f_{X}\left(x\right) &= \dfrac{m}{n}f_{X'}\left(\dfrac{m}{n}x\right) \\
&= \dfrac{m}{n}\cdot\dfrac{\left(xm/n\right)^{m/2 - 1}\Gamma\left(\left(m + n\right)/2\right)}{\Gamma\left(m/2\right)\Gamma\left(n/2\right)\left(xm/n + 1\right)^{\left(m + n\right)/2}} \\
&= \dfrac{x^{m/2 - 1}\left(m/n\right)^{m/2}\Gamma\left(\left(m + n\right)/2\right)}{\Gamma\left(m/2\right)\Gamma\left(n/2\right)\left(xm/n + 1\right)^{\left(m + n\right)/2}}
\end{align}
Using the definition of the Beta function, we can write an alternative expression for this density parametrised in two degrees of freedom $m$ and $n$:
\begin{equation}
f_{X}\left(x\right) = \dfrac{x^{m/2 - 1}\left(m/n\right)^{m/2}}{\operatorname{B}\left(m/2, n/2\right)\left(xm/n + 1\right)^{\left(m + n\right)/2}}
\end{equation}
An expression can be derived for the cumulative distribution function, $F_{X}\left(x\right)$. Firstly,
\begin{align}
F_{X}\left(x\right) &= \int_{-\infty}^{x}f_{X}\left(r\right)dr \\
&= \int_{0}^{x}\dfrac{r^{m/2 - 1}\left(m/n\right)^{m/2}}{\operatorname{B}\left(m/2, n/2\right)\left(rm/n + 1\right)^{\left(m + n\right)/2}}dr
\end{align}
Make the change of variables $s = \dfrac{mr}{mr + n}$ so that rearranging gives $r = \dfrac{n}{m}\cdot\dfrac{s}{1 - s}$ and $\dfrac{dr}{ds} = \dfrac{n}{m}\cdot\dfrac{1}{\left(1 - s\right)^{2}}$ by the quotient rule. Then
\begin{align}
F_{X}\left(x\right) &= \dfrac{1}{\operatorname{B}\left(m/2, n/2\right)}\int_{0}^{\frac{mx}{mx + n}}\dfrac{s^{m/2 - 1}\left(1 - s\right)^{-m/2 + 1}\left(n/m\right)^{m/2 - 1}\left(m/n\right)^{m/2}}{\left(\frac{s + 1 - s}{1 - s}\right)^{\left(m + n\right)/2}}\cdot\dfrac{n}{m}\cdot\dfrac{ds}{\left(1 - s\right)^{2}} \\
&= \dfrac{1}{\operatorname{B}\left(m/2, n/2\right)}\int_{0}^{\frac{mx}{mx + n}}\dfrac{s^{m/2 - 1}\left(1 - s\right)^{-m/2 + 1}}{\left(1 - s\right)^{-\left(m + n\right)/2 + 2}}ds \\
&= \dfrac{1}{\operatorname{B}\left(m/2, n/2\right)}\int_{0}^{\frac{mx}{mx + n}}s^{m/2 - 1}\left(1 - s\right)^{n/2 - 1}ds \\
&= \dfrac{\operatorname{B}\left(\frac{mx}{mx + n}; m/2, n/2\right)}{\operatorname{B}\left(m/2, n/2\right)}
\end{align}
which is a regularised incomplete Beta function.

\subsection{Pareto Distribution}

\subsubsection{Lomax Distribution}

\subsection{Weibull Distribution}

\subsection{Gumbel Distribution}

The Gumbel distribution has support $\mathbb{R}$, scale parameter $\beta > 0$ and location parameter $\mu$, with cumulative distribution function
\begin{equation}
F\left(x\right) = \exp\left(-e^{-\frac{x - \mu}{\beta}}\right)
\end{equation}
If can be verified that $\lim_{x\to -\infty}F\left(x\right) = 0$ and $\lim_{x\to \infty}F\left(x\right) = 1$. The probability density function is found by the chain rule:
\begin{align}
f\left(x\right) &= \dfrac{dF\left(x\right)}{dx} \\
&= \dfrac{1}{\beta}e^{-\frac{x - \mu}{\beta}}\exp\left(-e^{-\frac{x - \mu}{\beta}}\right) \\
&= \dfrac{1}{\beta}\exp\left[-\left(\frac{x - \mu}{\beta} + e^{-\frac{x - \mu}{\beta}}\right)\right]
\end{align}
The mode of the distribution can be found by taking another derivative.
\begin{align}
f'\left(x\right) &= \dfrac{1}{\beta}\left[-\left(\dfrac{1}{\beta} - \dfrac{1}{\beta}e^{-\frac{x - \mu}{\beta}}\right)\exp\left[-\left(\frac{x - \mu}{\beta} + e^{-\frac{x - \mu}{\beta}}\right)\right]\right] \\
&= -\left(\dfrac{1}{\beta} - \dfrac{1}{\beta}e^{-\frac{x - \mu}{\beta}}\right)f\left(x\right)
\end{align}
The mode $\breve{x}$ is found at $f\left(\breve{x}\right) = 0$ which gives
\begin{equation}
\dfrac{1}{\beta} = \dfrac{1}{\beta}e^{-\frac{\breve{x} - \mu}{\beta}}
\end{equation}
Hence the mode $\breve{x} = \mu$.

\subsection{Logistic Distribution}

\subsubsection{Fisk Distribution}

\section{Families of Discrete Univariate Probability Distributions}

\subsection{Kronecker Delta Distribution}

\subsection{Discrete Uniform Distribution}

\subsection{Bernoulli Distribution}

\subsection{Radermacher Distribution}

\subsection{Binomial Distribution}

The cumulative binomial distribution can be written as a sum:
\begin{align}
F\left(x\right) &= \operatorname{Pr}\left(X \leq x\right) \\
&= \sum_{i = 0}^{x}\operatorname{Pr}\left(X = i\right) \\
&= \sum_{i = 0}^{x}f\left(i\right) \\
&= \sum_{i = 0}^{x}\mathstrut^{n}\mathsf{C}_{i}p^{i}\left(1 - p\right)^{n - i}
\end{align}
An alternate form of the expression for the cumulative binomial distribution is the regularised incomplete Beta function:
\begin{align}
F\left(k\right) &= \dfrac{\operatorname{B}\left(1 - p; n - k, k + 1\right)}{\operatorname{B}\left(n - k, k + 1\right)} \\
&= \dfrac{1}{\operatorname{B}\left(n - k, k + 1\right)}\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt \\
&= \dfrac{\Gamma\left(n + 1\right)}{\Gamma\left(n - k\right)\Gamma\left(k + 1\right)}\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt \\
&= \dfrac{n!}{\left(n - k - 1\right)!k!}\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt \\
&= \left(n - k\right)\begin{pmatrix}n \\ k\end{pmatrix}\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt
\end{align}
This can be shown by repeated application of integration by parts on the Beta integral. Let $u = \left(1 - t\right)^{k}$ and $v' = t^{n - k - 1}$, then $u' = -k\left(1 - t\right)^{k - 1}$ and $v = \dfrac{1}{n - k}t^{n - k}$. So
\begin{align}
\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt &= \left[uv\right]_{0}^{1 - p} - \int_{0}^{1 - p}u'vdt \\
&= \dfrac{1}{n - k}\left[\left(1 - t\right)^{k}t^{n - k}\right]_{t = 0}^{t = 1 - p} + \int_{0}^{1 - p}\dfrac{k}{n - k}t^{n - k}\left(1 - t\right)^{k - 1}dt \\
&= \dfrac{1}{n - k}p^{k}\left(1 - p\right)^{n - k} + \dfrac{k}{n - k}\int_{0}^{1 - p}\left(1 - t\right)^{k - 1}t^{n - k}dt
\end{align}
Repeating this again on the resultant Beta integral, we will have
\begin{multline}
\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt = \dfrac{1}{n - k}p^{k}\left(1 - p\right)^{n - k} + \dfrac{k}{\left(n - k\right)\left(n - k + 1\right)}p^{k - 1}\left(1 - p\right)^{n - k + 1} \\
 + \dfrac{k\left(k - 1\right)}{\left(n - k\right)\left(n - k + 1\right)}\int_{0}^{1 - p}\left(1 - t\right)^{k - 2}t^{n - k + 1}dt
\end{multline}
and we will eventually end up with the series
\begin{multline}
\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt = \dfrac{1}{n - k}p^{k}\left(1 - p\right)^{n - k} + \dfrac{k}{\left(n - k\right)\left(n - k + 1\right)}p^{k - 1}\left(1 - p\right)^{n - k + 1} \\
 + \dfrac{k\left(k - 1\right)}{\left(n - k\right)\left(n - k + 1\right)\left(n - k + 2\right)}p^{k - 2}\left(1 - p\right)^{n - k + 2} + \dots + \dfrac{k\times\dots\times 2}{\left(n - k\right)\times\dots\times\left(n - 1\right)}p^{1}\left(1 - p\right)^{n - 1} \\
  + \dfrac{k!}{\left(n - k\right)\times\dots\times\left(n - 1\right)}\int_{0}^{1 - p}\left(1 - t\right)^{0}t^{n - 1}dt
\end{multline}
with $\int_{0}^{1 - p}\left(1 - t\right)^{0}t^{n - 1}dt = \dfrac{1}{n}\left(1 - p\right)^{n}$. Note then that this series can be represented compactly as
\begin{equation}
\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt = \sum_{j = 0}^{k}\dfrac{k!}{\left(k - j\right)!}\dfrac{\left(n - k\right)!}{\left(n - k\right)\left(n - k + j\right)!}p^{k - j}\left(1 - p\right)^{n - k + j}
\end{equation}
Via cancellations, we see that
\begin{align}
\left(n - k\right)\begin{pmatrix}n \\ k\end{pmatrix}\dfrac{k!}{\left(k - j\right)!}\dfrac{\left(n - k\right)!}{\left(n - k\right)\left(n - k + j\right)!} &= \dfrac{n!}{\left(k - j\right)!\left(n - k + j\right)!} \\
&= \begin{pmatrix}n \\ k - j\end{pmatrix}
\end{align}
Hence
\begin{equation}
\left(n - k\right)\begin{pmatrix}n \\ k\end{pmatrix}\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt = \sum_{j = 0}^{k}\begin{pmatrix}n \\ k - j\end{pmatrix}p^{k - j}\left(1 - p\right)^{n - k + j}
\end{equation}
Then replace $k - j$ with $i$ in the sum to obtain the expression for the cumulative binomial distribution:
\begin{align}
F\left(k\right) &= \left(n - k\right)\begin{pmatrix}n \\ k\end{pmatrix}\int_{0}^{1 - p}t^{n - k - 1}\left(1 - t\right)^{k}dt \\
&= \sum_{i = 0}^{k}\begin{pmatrix}n \\ i\end{pmatrix}p^{i}\left(1 - p\right)^{n - i}
\end{align}

\subsection{Poisson Distribution}

\subsection{Skellam Distribution}

\subsection{Negative Binomial Distribution}

The negative binomial distribution is a discrete distribution on support $x \in \left\{0, 1, 2, 3, \dots\right\}$ for the number of successes of i.i.d. Bernoulli trials (with success probability $p$) before $r$ failures occur. For example, if we are interested in 1 success before $r$ failures, the probability of this is given by
\begin{equation}
\operatorname{Pr}\left(X = 1\right) = \mathstrut^{r}\mathsf{C}_{1}\left(1 - p\right)^{r}p^{1}
\end{equation}
We know that the sequence will be of length $n = r + 1$, and that the final trial in the sequence is fixed as a failure. Hence the term $\mathstrut^{r}\mathsf{C}_{1} = \mathstrut^{n - 1}\mathsf{C}_{1}$ gives the number of combinations where we can place $1$ success among $n - 1$ trials. Generally, the probability mass function is given by
\begin{equation}
\operatorname{Pr}\left(X = x\right) = \mathstrut^{r + x - 1}\mathsf{C}_{x}\left(1 - p\right)^{r}p^{x}
\end{equation}
The distribution has a mean of
\begin{equation}
\mu = \dfrac{pr}{1 - p}
\end{equation}
and a variance of
\begin{equation}
\sigma^{2} = \dfrac{pr}{\left(1 - p\right)^{2}}
\end{equation}
In practice, the negative binomial distribution may be used as an alternative to the Poisson distribution when the data appears too `overdispersed' for the Poisson distribution. Since the negative binomial has two parameters ($r$ and $p$), we can fit the distribution better to the data. The negative binomial can also be seen as a mixture of Poissons with Gamma-distributed mixing weights. Suppose we know $\mu$ and $\sigma^{2}$, then solving the equations above for $p$ and $r$ gives the equations
\begin{gather}
p = 1 - \dfrac{\mu}{\sigma^{2}} \\
r = \dfrac{\mu^{2}}{\sigma^{2} - \mu}
\end{gather}
which can be used as the moment estimators.

\subsection{Geometric Distribution}

\subsection{Hypergeometric Distribution}

\subsection{Beta-Binomial Distribution}

\subsection{Benford Distribution}

\section{Random Vectors}

\subsection{Multivariate Medians}

The notion of median and sample median can be generalised to multivariate domains in several ways.

\subsubsection{Marginal Median}

Suppose that the random vector $\mathbf{X} \in \mathbb{R}^{d}$ has marginal cumulative distribution functions $F_{1}\left(x_{1}\right), \dots, F_{d}\left(x_{D}\right)$. Then the marginal median $\mathbf{m} = \begin{bmatrix} m_{1} & \dots & m_{d}\end{bmatrix}^{\top}$ can be defined as the value(s) for which
\begin{equation}
F_{1}\left(m_{1}\right) = \dots = F_{d}\left(m_{d}\right) = \dfrac{1}{2}
\end{equation}
Likewise, if we have a random sample of vectors $\mathbf{X}_{1}, \dots, \mathbf{X}_{n}$, then the sample marginal median can be defined as the the vector which has as each of its components the univariate median of the sample in the corresponding dimension.

\subsubsection{Medoid}

For a random sample of vectors $\mathbf{X}_{1}, \dots, \mathbf{X}_{n}$, the medoid $\mathbf{c}^{*}$ on a distance function $d\left(\mathbf{a}, \mathbf{b}\right)$ (eg. the Euclidean norm between two vectors) is defined as the vector from the sample that is closest on average to all others:
\begin{equation}
\mathbf{c}^{*} = \argmin_{\mathbf{c} \in \left\{\mathbf{X}_{1}, \dots, \mathbf{X}_{n}\right\}}\sum_{i = 1}^{d}d\left(\mathbf{X}_{i} - \mathbf{c}\right)
\end{equation}

\subsubsection{Spatial Median}

For a random sample of vectors $\mathbf{X}_{1}, \dots, \mathbf{X}_{n}$, the spatial median $\mathbf{a}^{*}$ on a norm $\left\Vert\cdot\right\Vert$ (eg. the Euclidean norm) is defined as the vector which minimises the average distance to each point:
\begin{equation}
\mathbf{a}^{*} = \argmin_{\mathbf{a}} \dfrac{1}{n}\sum_{i = 1}^{n}\left\Vert \mathbf{X}_{i} - \mathbf{a}\right\Vert
\end{equation}

\subsection{Covariance Matrices}
The covariance matrix $\mathbf{C}$ of a random vector $\mathbf{X}$ can be defined by
\begin{equation}
\mathbf{C} = \mathbb{E}\left[\left(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right]\right)\left(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right]\right)^{\top}\right]
\end{equation}
Covariance matrices of real random vectors are always positive semi-definite. We can show this by considering an arbitrary real vector $\mathbf{u}$. We have
\begin{align}
\mathbf{u}^{\top}\mathbf{C}\mathbf{u} &= \mathbf{u}^{\top}\mathbb{E}\left[\left(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right]\right)\left(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right]\right)^{\top}\right]\mathbf{u} \\
&= \mathbb{E}\left[\mathbf{u}^{\top}\left(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right]\right)\left(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right]\right)^{\top}\mathbf{u}\right]
\end{align}
Let $Y := \mathbf{u}^{\top}\left(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right]\right)$ be a zero-mean scalar random variable with variance $\sigma^{2} \geq 0$. Hence
\begin{align}
\mathbf{u}^{\top}\mathbf{C}\mathbf{u} &= \mathbb{E}\left[YY^{\top}\right] \\
&= \mathbb{E}\left[\left(Y - \mathbb{E}\left[Y\right]\right)^{2}\right] \\
&= \mathbb{E}\left[Y^{2}\right] \\
&= \sigma^{2} \\
&\geq 0
\end{align}
Therefore $\mathbf{C}$ satisfies the property of a positive semi-definite matrix.

\subsubsection{Precision Matrix}

The precision matrix of a random vector $\mathbf{X}$ is defined as the inverse of the covariance matrix, ie. $\mathbf{C}^{-1}$.

\section{Multivariate Probability Distributions}

\subsection{Multivariate Gaussian Distribution}

\subsection{Multinomial Distribution}

\subsection{Multivariate Hypergeometric Distribution}

\subsection{Dirichlet Distribution}

\subsection{Bivariate Cumulative Discrete Distribution Functions}

Suppose we have the discrete bivariate cumulative distribution function $F\left(x, y\right) := \operatorname{Pr}\left(X\leq x, Y \leq y\right)$. We are interested in deriving the probability mass function $f\left(x, y\right) = \operatorname{Pr}\left(X = x, Y = y\right)$ from this. Assume that $X$ and $Y$ are integer valued. We can derive this using the inclusion-exclusion principle. First define the events
\begin{gather}
A = \left\{X \leq x, Y < y\right\} \\
B = \left\{X < x, Y \leq y\right\}
\end{gather}
Then logically
\begin{gather}
A \cup B =  \left\{X \leq x, Y \leq y\right\}\setminus\left\{X = x, Y = y\right\} \\
A \cap B = \left\{X < x, Y < y\right\}
\end{gather}
Thus the joint probability mass function can be expressed as
\begin{align}
\operatorname{Pr}\left(X = x, Y = y\right) &= \operatorname{Pr}\left(X \leq x, Y \leq y\right) - \operatorname{Pr}\left(A \cup B\right) \\
&= \operatorname{Pr}\left(X \leq x, Y \leq y\right) - \operatorname{Pr}\left(A\right) - \operatorname{Pr}\left(B\right) + \operatorname{Pr}\left(A \cap B\right) \\
&= \operatorname{Pr}\left(X \leq x, Y \leq y\right) - \operatorname{Pr}\left(X \leq x, Y < y\right) - \operatorname{Pr}\left(X < x, Y \leq y\right) + \operatorname{Pr}\left(X < x, Y < y\right) \\
&\begin{multlined} =\operatorname{Pr}\left(X \leq x, Y \leq y\right) - \operatorname{Pr}\left(X \leq x, Y \leq y - 1\right) - \operatorname{Pr}\left(X \leq x - 1, Y \leq y\right) \\
+ \operatorname{Pr}\left(X \leq x - 1, Y \leq y - 1\right) \end{multlined} \\
&= F\left(x, y\right) - F\left(x, y - 1\right) - F\left(x - 1, y\right) + F\left(x - 1, y - 1\right)
\end{align}
\section{Inequalities in Probability}

\subsection{Boole's Inequality}

Also known as the union bound, Boole's inequality generalises the addition law of probability. It states that for events $A_{1}, A_{2}, A_{3}, \dots$:
\begin{equation}
\operatorname{Pr}\left(\bigcup_{i = 1}^{n}A_{i}\right) \leq \sum_{i = 1}^{n}\operatorname{Pr}\left(A_{i}\right)
\end{equation}
\begin{proof}
Starting with the base case, we clearly have for $n = 1$:
\begin{equation}
\operatorname{Pr}\left(A_{1}\right) \leq \operatorname{Pr}\left(A_{1}\right)
\end{equation}
For the induction step, suppose for some $n \geq 1$:
\begin{equation}
\operatorname{Pr}\left(\bigcup_{i = 1}^{n}A_{i}\right) \leq \sum_{i = 1}^{n}\operatorname{Pr}\left(A_{i}\right)
\end{equation}
By letting $A := \bigcup_{i = 1}^{n}A_{i}$ and $B := A_{n + 1}$ and using the addition law of probability,
\begin{equation}
\operatorname{Pr}\left(\bigcup_{i = 1}^{n + 1}A_{i}\right) = \operatorname{Pr}\left(\bigcup_{i = 1}^{n}A_{i}\right) + \operatorname{Pr}\left(A_{n + 1}\right) -  \operatorname{Pr}\left(\bigcup_{i = 1}^{n}A_{i} \cap A_{n + 1}\right)
\end{equation}
Since $\operatorname{Pr}\left(\bigcup_{i = 1}^{n}A_{i} \cap A_{n + 1}\right) \geq 0$, then
\begin{equation}
\operatorname{Pr}\left(\bigcup_{i = 1}^{n + 1}A_{i}\right) \leq \operatorname{Pr}\left(\bigcup_{i = 1}^{n}A_{i}\right) + \operatorname{Pr}\left(A_{n + 1}\right)
\end{equation}
and because it is given $\operatorname{Pr}\left(\bigcup_{i = 1}^{n}A_{i}\right) \leq \sum_{i = 1}^{n}\operatorname{Pr}\left(A_{i}\right)$, we have
\begin{align}
\operatorname{Pr}\left(\bigcup_{i = 1}^{n + 1}A_{i}\right) &\leq \sum_{i = 1}^{n}\operatorname{Pr}\left(A_{i}\right) + \operatorname{Pr}\left(A_{n + 1}\right) \\
&\leq \sum_{i = 1}^{n + 1}\operatorname{Pr}\left(A_{i}\right)
\end{align}
\end{proof}

\subsection{Comparison of Random Variables}

\begin{theorem}
Let $X$ and $Y$ be two continuous random variables with means $\mu_{X}$ and $\mu_{Y}$ respectively such that $\mu_{Y} \geq \mu_{X}$. Let $\delta = \dfrac{\mu_{Y} - \mu_{X}}{2}$. Then
\begin{equation}
\operatorname{Pr}\left(X > Y\right) \leq \operatorname{Pr}\left(X > \mu_{X} + \delta\right) + \operatorname{Pr}\left(Y < \mu_{Y} - \delta\right)
\end{equation}
\end{theorem}
\begin{proof}
The value $\mu_{X} + \delta$ can be thought of as the `midway' point between $\mu_{X}$ and $\mu_{Y}$, so we can deduce the event $\left\{X < \mu_{X} + \delta \cap Y > \mu_{Y} - \delta\right\} \subseteq \left\{X <  Y\right\}$ which implies
\begin{align}
\operatorname{Pr}\left(X < \mu_{X} + \delta \cap Y > \mu_{Y} - \delta\right) &\leq \operatorname{Pr}\left(X < Y\right) \\
&\leq 1 - \operatorname{Pr}\left(X > Y\right)
\end{align}
From DeMorgan's laws,
\begin{equation}
\operatorname{Pr}\left(X > \mu_{X} + \delta \cup Y < \mu_{Y} - \delta\right) = 1 - \operatorname{Pr}\left(X < \mu_{X} + \delta \cap Y > \mu_{Y} - \delta\right)
\end{equation}
which can be combined with the inequality above to give
\begin{equation}
\operatorname{Pr}\left(X > Y\right) \leq \operatorname{Pr}\left(X > \mu_{X} + \delta \cup Y < \mu_{Y} - \delta\right)
\end{equation}
Lastly, applying Boole's inequality
\begin{equation}
\operatorname{Pr}\left(X > \mu_{X} + \delta \cup Y < \mu_{Y} - \delta\right) \leq \operatorname{Pr}\left(X > \mu_{X} + \delta\right) + \operatorname{Pr}\left(Y < \mu_{Y} - \delta\right)
\end{equation}
completes the proof.
\end{proof}

\subsection{Jensen's Inequality}

A function $f\left(x\right)$ is convex if and only Jensen's inequality holds for any $x$ and $y$ in the domain of $f$:
\begin{equation}
f\left(\theta x + \left(1 - \theta\right)y\right) \leq \theta f\left(x\right) + \left(1 - \theta\right)f\left(y\right)
\end{equation}
where $\theta \in \left[0, 1\right]$ and $\theta x + \left(1 - \theta\right)y$ is termed as a `convex combination' of $x$ and $y$. Intuitively speaking, no part of a secant joining any two points on the graph of $f\left(x\right)$ can lie below the graph. A convex combination can be extended to more than two points, and so can Jensen's inequality:
\begin{equation}
f\left(\theta_{1}x_{1} + \dots + \theta_{k}x_{k}\right) \leq \theta_{1}f\left(x_{1}\right) + \dots + \theta_{k}f\left(x_{k}\right)
\end{equation}
where $\theta_{1}, \dots, \theta_{k} \geq 0$ and $\theta_{1} + \dots + \theta_{k} = 1$. We may extend this to infinite sums, ie. integrals over probability distributions:
\begin{equation}
f\left(\int p\left(x\right) x dx\right) \leq \int f\left(x\right)p\left(x\right) dx
\end{equation}
where $p\left(x\right) \geq 0$ and $\int p\left(x\right) dx = 1$. Recognise that the above inequality pertains to the expected value of a random variable $X$ with probability density $p\left(x\right)$, and a convex function $f\left(x\right)$. We can write
\begin{equation}
f\left(\mathbb{E}\left[X\right]\right) \leq \mathbb{E}\left[f\left(X\right)\right]
\end{equation}
Jensen's inequality generalises to real convex functions of a random vector:
\begin{equation}
f\left(\mathbb{E}\left[\mathbf{X}\right]\right) \leq \mathbb{E}\left[f\left(\mathbf{X}\right)\right]
\end{equation}

\subsection{Markov's Inequality}

Let $X$ be a non-negative random variable (it cannot take on negative values), and let $a > 0$ be some constant. Then
\begin{equation}
\operatorname{Pr}\left(X\geq a\right) \leq \dfrac{\mathbb{E}\left[X\right]}{a}
\end{equation}
or alternatively, if we let $\tilde{a}$ be some constant such that $a = \tilde{a}\mathbb{E}\left[X\right]$, then making this substitution gives
\begin{equation}
\operatorname{Pr}\left(X\geq \tilde{a}\mathbb{E}\left[X\right]\right) \leq \dfrac{1}{\tilde{a}}
\end{equation}
\begin{proof}
For a continuous non-negative random variable, the expectation is defined as
\begin{align}
\mathbb{E}\left[X\right] &= \int_{0}^{\infty}xf\left(x\right)dx \\
&= \int_{0}^{a}xf\left(x\right)dx + \int_{a}^{\infty}xf\left(x\right)dx \\
&\geq \int_{a}^{\infty}xf\left(x\right)dx
\end{align}
We then have
\begin{align}
\int_{a}^{\infty}af\left(x\right)dx &\leq \int_{a}^{\infty}xf\left(x\right)dx \\
a\int_{a}^{\infty}f\left(x\right)dx &\leq \mathbb{E}\left[X\right] \\
a\operatorname{Pr}\left(X \geq a\right) &\leq \mathbb{E}\left[X\right] \\
\operatorname{Pr}\left(X \geq a\right) &\leq \dfrac{\mathbb{E}\left[X\right]}{a}
\end{align}
If $X$ is a non-negative discrete random variable on support $\left\{0, 1, 2, \dots\right\}$, then
\begin{align}
\mathbb{E}\left[X\right] &= \sum_{x = 0}^{\infty}x\operatorname{Pr}\left(X = x\right) \\
&= \sum_{x = 0}^{a - 1}x\operatorname{Pr}\left(X = x\right) + \sum_{x = a}^{\infty}x\operatorname{Pr}\left(X = x\right) \\
&\geq \sum_{x = a}^{\infty}x\operatorname{Pr}\left(X = x\right)
\end{align}
Then similar to before,
\begin{align}
\sum_{x = a}^{\infty}a\operatorname{Pr}\left(X = x\right) &\leq \sum_{x = a}^{\infty}x\operatorname{Pr}\left(X = x\right) \\
a\sum_{x = a}^{\infty}\operatorname{Pr}\left(X = x\right) &\leq \mathbb{E}\left[X\right] \\
a\operatorname{Pr}\left(X \geq a\right) &\leq \mathbb{E}\left[X\right] \\
\operatorname{Pr}\left(X \geq a\right) &\leq \dfrac{\mathbb{E}\left[X\right]}{a}
\end{align}
\end{proof}
An alternative proof is given below:
\begin{proof}
Let $\mathbb{I}_{X \geq a}$ be an indicator random variable where $\mathbb{I}_{X \geq a} = 1$ if the event $X \geq a$ occurs, and $\mathbb{I}_{X \geq a} = 0$ if the event $X < a$ occurs. Then we can write
\begin{equation}
a\mathbb{I}_{X \geq a} \leq X
\end{equation}
This can be seen as follows:
\begin{itemize}
\item Suppose $X \geq a$, then $\mathbb{I}_{X \geq a} = 1$ and the inequality above holds.
\item Suppose $X < a$, then $\mathbb{I}_{X \geq a} = 0$ and the inequality above is satisfied because $X$ is non-negative.
\end{itemize}
Take the expectation operator of both sides. The expectation operator is monotonically increasing (ie. $\mathbb{E}\left[c\right] = c$ for some constant $c$) so this preserves the inequality.
\begin{equation}
\mathbb{E}\left[a\mathbb{I}_{X \geq a}\right] \leq \mathbb{E}\left[X\right]
\end{equation}
Taking the constant $a$ out of the expectation:
\begin{gather}
a\mathbb{E}\left[\mathbb{I}_{X \geq a}\right] \leq \mathbb{E}\left[X\right] \\
\mathbb{E}\left[\mathbb{I}_{X \geq a}\right] \leq \dfrac{\mathbb{E}\left[X\right]}{a}
\end{gather}
We can evaluate $\mathbb{E}\left[\mathbb{I}_{X \geq a}\right]$
\begin{equation}
\mathbb{E}\left[\mathbb{I}_{X \geq a}\right] = 1\times\operatorname{Pr}\left(X\geq a\right) + 0\times\operatorname{Pr}\left(X < a\right) = \operatorname{Pr}\left(X\geq a\right)
\end{equation}
Hence
\begin{equation}
\operatorname{Pr}\left(X\geq a\right) \leq \dfrac{\mathbb{E}\left[X\right]}{a}
\end{equation}
\end{proof}

We can use this result to bound probabilities of $X$ being greater than some value in relation to the mean. For example, suppose the population of $X$ is income (which we assume to be non-negative). Then if we choose $a$ to be 10 times the average income, we can make the statement that the proportion of people earning at least 10 times the average income is no more than 10\%.

\subsubsection{Reverse Markov's Inequality}

A lower bound on $operatorname{Pr}\left(X \geq a\right)$ in terms of $\mathbb{E}\left[X\right]$ can be obtained under special circumstances. Suppose $X$ is upper bounded by $b$ such that $\operatorname{Pr}\left(X \leq b\right) = 1$. Then the random variable $X' = b - X$ is non-negative so Markov's inequality can be applied to this. We have for some $a > 0$
\begin{equation}
\operatorname{Pr}\left(X' \geq a\right) \leq \dfrac{\mathbb{E}\left[X'\right]}{a}
\end{equation}
Since $\mathbb{E}\left[X'\right] = b - \mathbb{E}\left[X\right]$:
\begin{gather}
\operatorname{Pr}\left(X' \geq a\right) \leq \dfrac{b - \mathbb{E}\left[X\right]}{a} \\
\operatorname{Pr}\left(b - X \geq a\right) \leq \dfrac{b - \mathbb{E}\left[X\right]}{a} \\
\operatorname{Pr}\left(b - a \geq X\right) \leq \dfrac{b - \mathbb{E}\left[X\right]}{a}
\end{gather}
By complements,
\begin{gather}
1 - \operatorname{Pr}\left(b - a \leq X\right) \leq \dfrac{b - \mathbb{E}\left[X\right]}{a} \\
\operatorname{Pr}\left(X \geq b - a\right) \geq 1 - \dfrac{b - \mathbb{E}\left[X\right]}{a} \\
\operatorname{Pr}\left(X \geq b - a\right) \geq \dfrac{a - b + \mathbb{E}\left[X\right]}{a}
\end{gather}
Let $c = b - a$, then
\begin{equation}
\operatorname{Pr}\left(X \geq c\right) \geq \dfrac{\mathbb{E}\left[X\right] - x}{b - c}
\end{equation}
Note that this inequality is only useful when $c < \mathbb{E}\left[X\right]$.

\subsection{Chebychev's Inequality}

Let $X$ be a random variable with finite mean $\mu$ and non-zero standard deviation $\sigma$. Then for any $k > 0$, Chebychev's inequality (also known as the Bienaym\'{e}-Chebychev inequality) states that
\begin{equation}
\operatorname{Pr}\left(\left|X - \mu\right| \geq k\sigma\right) \leq \dfrac{1}{k^{2}}
\end{equation}
In words, this states that the probability of $X$ being at least $k$ standard deviations away from the mean can be no more than $\dfrac{1}{k^{2}}$.
\begin{proof}
Let a random variable $Y$ be defined as $Y = \left(X - \mu\right)^{2}$ and let $a = \left(k\sigma\right)^{2}$. Then applying Markov's inequality
\begin{gather}
\operatorname{Pr}\left(Y \geq a\right) \leq \dfrac{\mathbb{E}\left[Y\right]}{a} \\
\operatorname{Pr}\left(\left(X - \mu\right)^{2} \geq k^{2}\sigma^{2}\right) \leq \dfrac{\mathbb{E}\left[\left(X - \mu\right)^{2}\right]}{k^{2}\sigma^{2}}
\end{gather}
The event inside the probability is equivalent to $\left|X - \mu\right| \geq k\sigma$ and the numerator on the right hand side is the definition of variance $\sigma^{2}$. So
\begin{equation}
\operatorname{Pr}\left(\left|X - \mu\right| \geq k\sigma\right) \leq \dfrac{1}{k^{2}}
\end{equation}
\end{proof}
Note that this inequality is only useful for when $k > 1$, because when $0 < k \leq 1$, the term $\dfrac{1}{k^{2}} \geq 1$ so Chebychev's inequality is trivially satisfied. An alternative form of the inequality is to let $k = \varepsilon/\sigma$. Then we have
\begin{equation}
\operatorname{Pr}\left(\left|X - \mu\right| \geq \varepsilon\right) \leq \dfrac{\sigma^{2}}{\varepsilon^{2}}
\end{equation}
Chebychev's inequality can be used to bound probabilities within a certain number of standard deviations from the mean, for very general distributions. Obtaining more knowledge of the distribution (such as information the distribution is normal) will often lead to tighter (ie. smaller) bounds.

\subsubsection{Multivariate Chebychev's Inequality}

If $\mathbf{X}$ is an $n$-dimensional random vector with mean $\mathbb{E}\left[\mathbf{X}\right] = \boldsymbol{\mu}$ and covariance matrix $\operatorname{Cov}\left(\mathbf{X}\right) = \Sigma$, then multivariate Chebychev's inequality states that for any $k > 0$:
\begin{equation}
\operatorname{Pr}\left(\sqrt{\left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\Sigma^{-1}\left(\mathbf{X} - \boldsymbol{\mu}\right)} > k\right) \leq \dfrac{n}{k^{2}}
\end{equation}
\begin{proof}
Define the random variable $Y = \left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\Sigma^{-1}\left(\mathbf{X} - \boldsymbol{\mu}\right)$. By positive definiteness of $\Sigma$ and hence $\Sigma^{-1}$, $Z$ is positive so
\begin{align}
\operatorname{Pr}\left(\sqrt{\left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\Sigma^{-1}\left(\mathbf{X} - \boldsymbol{\mu}\right)} > k\right) &= \operatorname{Pr}\left(\sqrt{Y} > k\right) \\
&= \operatorname{Pr}\left(Y > k^{2}\right) \\
&\leq \dfrac{\mathbb{E}\left[Y\right]}{k^{2}}
\end{align}
where the Markov inequality is applied in the last step. We then have
\begin{equation}
\mathbb{E}\left[Y\right] = \mathbb{E}\left[\left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\Sigma^{-1}\left(\mathbf{X} - \boldsymbol{\mu}\right)\right]
\end{equation}
Since $Y$ is univariate,
\begin{equation}
\mathbb{E}\left[Y\right] = \mathbb{E}\left[\operatorname{trace}\left(\left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\Sigma^{-1}\left(\mathbf{X} - \boldsymbol{\mu}\right)\right)\right]
\end{equation}
Because the trace is invariant to cyclic permutations,
\begin{equation}
\mathbb{E}\left[Y\right] = \mathbb{E}\left[\operatorname{trace}\left(\Sigma^{-1}\left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\left(\mathbf{X} - \boldsymbol{\mu}\right)\right)\right]
\end{equation}
Then use the fact the trace is a linear operator:
\begin{equation}
\mathbb{E}\left[Y\right] = \operatorname{trace}\left(\Sigma^{-1}\mathbb{E}\left[\left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\left(\mathbf{X} - \boldsymbol{\mu}\right)\right]\right)
\end{equation}
And lastly by using the definition of the covariance matrix and the trace of the identity:
\begin{align}
\mathbb{E}\left[Y\right] &= \operatorname{trace}\left(\Sigma^{-1}\Sigma\right) \\
&= \operatorname{trace}\left(I\right) \\
&= n
\end{align}
Therefore
\begin{equation}
\operatorname{Pr}\left(\sqrt{\left(\mathbf{X} - \boldsymbol{\mu}\right)^{\top}\Sigma^{-1}\left(\mathbf{X} - \boldsymbol{\mu}\right)} > k\right) \leq \dfrac{n}{k^{2}}
\end{equation}
\end{proof}

\subsection{Cantelli's Inequality}
For a random variable $X$ with expectation $\mu$ and variance $\sigma^{2}$, Cantelli's inequality states that for $\delta > 0$:
\begin{equation}
\operatorname{Pr}\left(\left|X - \mu\right| \geq \delta\right) \leq  \dfrac{2\sigma^{2}}{\sigma^{2} + \delta^{2}}
\end{equation}
\begin{proof}
Consider the probability $\operatorname{Pr}\left(X - \mu > \lambda\right)$ for the case where $\lambda > 0$. Define $Y := X - \mu$ so that $\mathbb{E}\left[Y\right] = 0$ and $\operatorname{Var}\left(Y\right) = \sigma^{2}$. Then for any $u \geq 0$:
\begin{align}
\operatorname{Pr}\left(X - \mu \geq \lambda\right) &= \operatorname{Pr}\left(Y \geq \lambda\right) \\
&= \operatorname{Pr}\left(Y + u \geq \lambda + u\right) \\
&\leq \operatorname{Pr}\left(\left(Y + u\right)^{2} \geq \left(\lambda + u\right)^{2}\right)
\end{align}
where the last inequality comes from recognising that $\lambda + u$ is positive, but $Y + u$ can generally be negative, so the event $\left\{Y + u \geq \lambda + u\right\} \subseteq \left\{\left(Y + u\right)^{2} \geq \left(\lambda + u\right)^{2}\right\}$. Then applying Markov's inequality on the non-negative random variable $\left(Y - u\right)^{2}$,
\begin{align}
\operatorname{Pr}\left(X - \mu \geq \lambda\right) &\leq \dfrac{\mathbb{E}\left[\left(Y + u\right)^{2}\right]}{\left(\lambda + u\right)^{2}} \\
&\leq \dfrac{\mathbb{E}\left[Y^{2} + Yu + u^{2}\right]}{\left(\lambda + u\right)^{2}} \\
&\leq \dfrac{\mathbb{E}\left[Y^{2}\right] + \mathbb{E}\left[Yu\right] + \mathbb{E}\left[u^{2}\right]}{\left(\lambda + u\right)^{2}} \\
&\leq \dfrac{\operatorname{Var}\left(Y\right) + u^{2}}{\left(\lambda + u\right)^{2}} \\
&\leq \dfrac{\sigma^{2} + u^{2}}{\left(\lambda + u\right)^{2}}
\end{align}
To make this bound as tight as possible, we find the value $u^{*} \geq 0$ that minimises the left hand side. Differentiating,
\begin{align}
\dfrac{d}{du}\left(\dfrac{\sigma^{2} + u^{2}}{\left(\lambda + u\right)^{2}}\right) &= \dfrac{d}{du}\left(\sigma^{2}\left(\lambda + u\right)^{2} + u^{2}\left(\lambda + u\right)^{-2}\right) \\
&= -2\sigma^{2}\left(\lambda + u\right)^{-3} + 2u\left(\lambda + u\right)^{-2} - 2u^{2}\left(\lambda + u\right)^{3}
\end{align}
Setting the derivative to zero and solving:
\begin{gather}
\dfrac{2u^{*}}{\left(\lambda + u^{*}\right)^{2}} - \dfrac{2u^{*2}}{\left(\lambda + u^{*}\right)^{2}} - \dfrac{2\sigma^{2}}{\left(\lambda + u^{*}\right)^{3}} = 0 \\
u^{*}\left(\lambda + u^{*}\right) - u^{*2} - \sigma^{2} = 0 \\
u^{*}\lambda - \sigma^{2} = 0 \\
u^{*} = \dfrac{\sigma^{2}}{\lambda}
\end{gather}
Hence substituting $u^{*}$:
\begin{align}
\operatorname{Pr}\left(X - \mu \geq \lambda\right) &\leq \dfrac{\sigma^{2}+\sigma^{4}/\lambda^{2}}{\left(\lambda+\sigma^{2}/\lambda\right)^{2}} \\
&\leq \dfrac{\sigma^{2}+\sigma^{4}/\lambda^{2}}{\lambda^{2}+2\sigma^{2}+\sigma^{4}/\lambda^{2}} \\
&\leq \dfrac{\lambda^{2}\sigma^{2}+\sigma^{4}}{\lambda^{4}+2\sigma^{2}\lambda^{2}+\sigma^{4}} \\
&\leq \dfrac{\sigma^{2}\left(\lambda^{2}+\sigma^{2}\right)}{\left(\lambda^{2}+\sigma^{2}\right)^{2}} \\
&\leq  \dfrac{\sigma^{2}}{\lambda^{2}+\sigma^{2}}
\end{align}
Now suppose $\lambda < 0$, then define $\alpha := -\lambda > 0$ and $Z = -Y$. Then as above,
\begin{align}
\operatorname{Pr}\left(X - \mu < \lambda\right) &= \operatorname{Pr}\left(Y < \lambda\right) \\
&= \operatorname{Pr}\left(Z > \alpha\right) \\
&\leq \dfrac{\sigma^{2}}{\alpha^{2}+\sigma^{2}} \\
&\leq \dfrac{\sigma^{2}}{\lambda^{2}+\sigma^{2}}
\end{align}
To combine both cases, define $\delta := \left|\lambda\right| > 0$, then
\begin{align}
\operatorname{Pr}\left(\left|X - \mu\right| \geq \delta\right) &= \operatorname{Pr}\left(X - \mu \geq \delta \cup X - \mu \leq -\delta\right) \\
&= \operatorname{Pr}\left(X - \mu \geq \delta\right) + \operatorname{Pr}\left(X - \mu < -\delta\right) \\
&\leq \dfrac{\sigma^{2}}{\delta^{2}+\sigma^{2}} + \dfrac{\sigma^{2}}{\delta^{2}+\sigma^{2}} \\
&\leq \dfrac{2\sigma^{2}}{\delta^{2}+\sigma^{2}}
\end{align}
\end{proof}

\subsection{Cauchy-Schwarz Inequality}

By viewing random variables as objects which assign a number to every outcome in a random experiment, they can be treated as points on a vector space. An inner product between random variables $X$ and $Y$ can be defined as
\begin{equation}
\left\langle X, Y\right\rangle := \mathbb{E}\left[XY\right]
\end{equation}
We can show that this satisfies the definition of an inner product space because of the linearity of expectation, and also because $\mathbb{E}\left[X^{2}\right] = 0$ implies $\operatorname{Pr}\left(X = 0\right) = 1$ (ie. a value of $0$ is assigned to every outcome in the random experiment) and vice-versa. The latter is shown by considering
\begin{align}
\mathbb{E}\left[X^{2}\right] &= \operatorname{Var}\left(X\right) + \mathbb{E}\left[X\right]^{2} \\
&= 0
\end{align}
which can only be satisfied if $\operatorname{Var}\left(X\right) = 0$ since we must have $0 \leq \mathbb{E}\left[X\right]^{2} = -\operatorname{Var}\left(X\right) \leq 0$. The converse is more obvious to show, just by using the definition of mean and variance. Applying the Cauchy-Schwarz inequality to this inner product gives
\begin{equation}
\mathbb{E}\left[XY\right]^{2} \leq \mathbb{E}\left[X^{2}\right]\mathbb{E}\left[Y^{2}\right]
\end{equation}
This can also be used to show the inequality for the correlation coefficient:
\begin{align}
\operatorname{Cov}\left(X, Y\right)^{2} &= \mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)\left(Y - \mathbb{E}\left[Y\right]\right)\right] \\
&\leq \mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)^{2}\right]\mathbb{E}\left[\left(Y - \mathbb{E}\left[Y\right]\right)^{2}\right] \\
&\leq \operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)
\end{align}
Hence
\begin{gather}
-\sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)} \leq \operatorname{Cov}\left(X, Y\right) \leq \sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)} \\
-1 \leq \operatorname{Corr}\left(X, Y\right) \leq 1
\end{gather}

\subsection{H\"{o}lder's Inequality}

For random variables $X$, $Y$ and some $p > 1$, $q > 1$ such that $\dfrac{1}{p} + \dfrac{1}{q} = 1$, H\"{o}lder's inequality for probability states that
\begin{equation}
\mathbb{E}\left[\left|XY\right|\right]\leq\mathbb{E}\left[\left|X\right|^{p}\right]^{1/p}\mathbb{E}\left[\left|Y\right|^{q}\right]^{1/q}
\end{equation}
\begin{proof}
If the left hand side is zero (ie. $\operatorname{Pr}\left(X = 0\right) = 1$ or $\operatorname{Pr}\left(Y = 0\right) = 1$), then by applying similar reasoning as for the Cauchy-Schwarz inequality, the right hand side must also be zero, satisfying H\"{o}lder's inequality. We then focus on the case where the left hand side is positive. For some positive $a$, $b$, there exist $s$, $t$ such that $a = e^{s/p}$ and $b = e^{t/q}$. Since $\exp\left(\cdot\right)$ is a convex function, then by $p^{-1} + q^{-1} = 1$:
\begin{equation}
e^{p^{-1}s + q^{-1}t} \leq p^{-1}e^{s} + q^{-1}e^{t}
\end{equation}
Then applying the definitions of $a$ and $b$, this gives what is known as Young's inequality for products:
\begin{equation}
ab \leq \dfrac{a^{p}}{p} + \dfrac{b^{q}}{q}
\end{equation}
which is satisfied when $a = 0$ or $b = 0$ as well as for positive $a$, $b$. Then let $a = \dfrac{\left|X\right|}{\mathbb{E}\left[\left|X\right|^{p}\right]^{1/p}}$ and $b = \dfrac{\left|Y\right|}{\mathbb{E}\left[\left|Y\right|^{q}\right]^{1/q}}$ so that
\begin{equation}
\dfrac{\left|XY\right|}{\mathbb{E}\left[\left|X\right|^{p}\right]^{1/p}\mathbb{E}\left[\left|Y\right|^{q}\right]^{1/q}}\leq\dfrac{1}{p}\dfrac{\left|X\right|^{p}}{\mathbb{E}\left[\left|X\right|^{p}\right]}+\dfrac{1}{q}\dfrac{\left|Y\right|^{q}}{\mathbb{E}\left[\left|Y\right|^{q}\right]}
\end{equation}
Taking the expectation of both sides,
\begin{align}
\dfrac{\mathbb{E}\left[\left|XY\right|\right]}{\mathbb{E}\left[\left|X\right|^{p}\right]^{1/p}\mathbb{E}\left[\left|Y\right|^{q}\right]^{1/q}} &\leq \dfrac{1}{p}\dfrac{\mathbb{E}\left[\left|X\right|^{p}\right]}{\mathbb{E}\left[\left|X\right|^{p}\right]}+\dfrac{1}{q}\dfrac{\mathbb{E}\left[\left|Y\right|^{q}\right]}{\mathbb{E}\left[\left|Y\right|^{q}\right]} \\
&\leq \dfrac{1}{p} + \dfrac{1}{q} \\
&\leq 1
\end{align}
Therefore
\begin{equation}
\mathbb{E}\left[\left|XY\right|\right]\leq\mathbb{E}\left[\left|X\right|^{p}\right]^{1/p}\mathbb{E}\left[\left|Y\right|^{q}\right]^{1/q}
\end{equation}
\end{proof}

\subsection{Minkowski's Inequality}

For random variables $X$, $Y$ and some $p \geq 1$, Minkowski's inequality for probability states that
\begin{equation}
\mathbb{E}\left[\left|X + Y\right|^{p}\right]^{1/p} \leq \mathbb{E}\left[\left|X\right|^{p}\right]^{1/p} + \mathbb{E}\left[\left|Y\right|^{p}\right]^{1/p}
\end{equation}
\begin{proof}
It suffices to show that for non-negative $X$ and $Y$ that
\begin{equation}
\mathbb{E}\left[\left(X^{1/p} + Y^{1/p}\right)^{p}\right] \leq \left(\mathbb{E}\left[X\right]^{1/p} + \mathbb{E}\left[Y\right]\right)^{p}
\end{equation}
because then we can then simply replace $X$ with $\left|X\right|^{p}$ and $Y$ with $\left|Y\right|^{p}$, then take both sides to the power of $1/p$ to obtain Minkowski's inequality. Introduce the function $f\left(x, y\right) = \left(x^{1/p} + y^{1/p}\right)^{p}$, which we will show is concave. Taking the partial derivatives (up to second order):
\begin{align}
\dfrac{\partial f\left(x,y\right)}{\partial x} &= p\cdot\dfrac{1}{p}x^{1/p-1}\left(x^{1/p}+y^{1/p}\right)^{p-1} \\
&= x^{1/p-1}\left(x^{1/p}+y^{1/p}\right)^{p-1}
\end{align}
and
\begin{align}
\dfrac{\partial^{2}f\left(x,y\right)}{\partial x^{2}} &= x^{1/p-1}\left(p-1\right)\cdot\dfrac{1}{p}x^{1/p-1}\left(x^{1/p}+y^{1/p}\right)^{p-2}+\left(\dfrac{1}{p}-1\right)x^{1/p-2}\left(x^{1/p}+y^{1/p}\right)^{p-1} \\
&= \dfrac{p-1}{p}x^{2/p-2}\left(x^{1/p}+y^{1/p}\right)^{p-2}-\dfrac{p-1}{p}x^{1/p-2}\left(x^{1/p}+y^{1/p}\right)\left(x^{1/p}+y^{1/p}\right)^{p-2} \\
&= \dfrac{p-1}{p}\left(x^{1/p}+y^{1/p}\right)^{p-2}\left[x^{2/p-2}-x^{1/p-2}\left(x^{1/p}+y^{1/p}\right)\right] \\
&= \dfrac{1-p}{p}\left(x^{1/p}+y^{1/p}\right)^{p-2}x^{1/p-2}y^{1/p}
\end{align}
which is negative since $p \geq 1$. By symmetry, an analogous expression can be found for $\dfrac{\partial^{2}f\left(x,y\right)}{\partial y^{2}}$. Also,
\begin{equation}
\dfrac{\partial^{2}f\left(x,y\right)}{\partial x\partial y}=\dfrac{p-1}{p}x^{1/p-1}y^{1/p-1}\left(x^{1/p}+y^{1/p}\right)^{p-2}
\end{equation}
We can then show negative semi-definiteness of the Hessian of $f\left(x, y\right)$, since
\begin{gather}
\left(\dfrac{\partial^{2}f\left(x,y\right)}{\partial x\partial y}\right)^{2}=\left(\dfrac{p-1}{p}\right)^{2}\left(x^{1/p}+y^{1/p}\right)^{2p-4}x^{2/p-2}y^{2/p-2} \\
\dfrac{\partial^{2}f\left(x,y\right)}{\partial x^{2}}\dfrac{\partial^{2}f\left(x,y\right)}{\partial y^{2}}=\left(\dfrac{p-1}{p}\right)^{2}\left(x^{1/p}+y^{1/p}\right)^{2p-4}x^{2/p-2}y^{2/p-2}
\end{gather}
which gives $\left(\dfrac{\partial^{2}f\left(x,y\right)}{\partial x\partial y}\right)^{2} = \dfrac{\partial^{2}f\left(x,y\right)}{\partial x^{2}}\dfrac{\partial^{2}f\left(x,y\right)}{\partial y^{2}}$. Hence $f\left(x, y\right)$ is concave (or equivalently, $-f\left(x, y\right)$ is convex. Using Jensen's inequality,
\begin{equation}
f\left(\mathbb{E}\left[X\right], \mathbb{E}\left[Y\right]\right) \geq \mathbb{E}\left[f\left(X, Y\right)\right]
\end{equation}
therefore
\begin{equation}
\mathbb{E}\left[\left(X^{1/p} + Y^{1/p}\right)^{p}\right] \leq \left(\mathbb{E}\left[X\right]^{1/p} + \mathbb{E}\left[Y\right]\right)^{p}
\end{equation}
as required.
\end{proof}

\subsection{Lyapunov's Inequality}

Lyapunov's inequality for probability can be derived from H\"{o}lder's inequality. First let $0 < \alpha < \beta$ and choose $p = \beta/\alpha > 1$ and $q = \beta/\left(\beta - \alpha\right) > 1$, noting that this satisfies $p^{-1} + q^{-1} = 1$. Then apply H\"{o}lder's inequality to the random variable $\left|X\right|^{\alpha}$ and the degenerate random variable $1$ to yield
\begin{equation}
\mathbb{E}\left[\left|X\right|^{\alpha}\right]\leq\mathbb{E}\left[\left|X\right|^{\beta}\right]^{\alpha/\beta}
\end{equation}
for $0 < \alpha \leq \beta$, noting that this inequality is trivially satisfied (with equality) when $\alpha = \beta$.

\section{Notions of Convergence}

\subsection{Convergence in Distribution}

Let $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ be a sequence of real-valued random variables. The sequence of random variables is said to converge in distribution to a random variable $X$ if
\begin{equation}
\lim_{n\to \infty}F_{n}\left(x\right) = F\left(x\right)
\end{equation}
where $F_{n}\left(x\right)$ is the cumulative distribution function of $X_{n}$ and $F\left(x\right)$ is the cumulative distribution function of $X$. \\

Let $\mathbf{X}_{1}, \mathbf{X}_{2}, \dots$ be real valued random vectors and let $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ be a sequence of random vectors. The sequence is said to converge in distribution to a random vector $\mathbf{X}$ if
\begin{equation}
\lim_{n\to\infty}\operatorname{Pr}\left(\mathbf{X}_{n}\in A\right) = \operatorname{Pr}\left(\mathbf{X}\in A\right)
\end{equation}
for every $A\subset \mathbb{R}^{n}$ which is a continuity set of $\mathbf{X}$. Note that it is stronger condition to say it converges in distribution if the cumulative distribution functions converge. If 
\begin{equation}
\lim_{n\to \infty}F_{n}\left(x\right) = F\left(x\right)
\end{equation}
then this implies convergence in distribution. Convergence in distribution may be denoted with the $\overset{\mathrm{d}}\to$ symbol.

\subsection{Convergence in Mean}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ converges in mean (or in expectation) towards random variable $X$ if
\begin{equation}
\lim_{n\to\infty}\mathbb{E}\left[\left|X_{n} - X\right|\right] = 0
\end{equation}
A sequence of random vectors $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ converges in mean (or in expectation) towards random vector $\mathbf{X}$ if
\begin{equation}
\lim_{n\to\infty}\mathbb{E}\left[\left\Vert\mathbf{X}_{n} - \mathbf{X}\right\Vert\right] = 0
\end{equation}

\subsection{Convergence in $p$-Mean}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ converges in $p$-mean (or in $L_{p}$) towards random variable $X$ if for $p \geq 1$
\begin{equation}
\lim_{n\to\infty}\mathbb{E}\left[\left|X_{n} - X\right|^{p}\right] = 0
\end{equation}
A sequence of random vectors $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ converges in $p$-mean (or in $L_{p}$) towards random vector $\mathbf{X}$ if for $p \geq 1$
\begin{equation}
\lim_{n\to\infty}\mathbb{E}\left[\left\Vert\mathbf{X}_{n} - \mathbf{X}\right\Vert^{p}\right] = 0
\end{equation}

\subsection{Convergence in Probability}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ converges in probability towards random variable $X$ if
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) = 0
\end{equation} 
for all $\varepsilon > 0$. If $X$ is a constant, then the sequence is said to converge in probability to a constant.\\

A sequence of random vectors $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ converges in probability towards random vector $\mathbf{X}$ if
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\left\Vert \mathbf{X}_{n} - \mathbf{X}\right\Vert > \varepsilon\right) = 0
\end{equation} 
for all $\varepsilon > 0$. Convergence in probability may be denoted with the $\overset{\mathrm{p}}\to$ symbol.

\subsection{Pointwise Convergence in Probability}

Pointwise convergence in probability extends the notion of convergence in probability to sequences which are parametrised by some parameter $\theta$ from a parameter space $\Theta$. For example, the sequence $\left\{X_{n}\right\}$ can be thought of as estimators and $\theta$ is a parameter of the data generating process. A formal definition is given as follows. \\
 
Denote by $X$ a random variable obtained from an experiment parametrised by $\theta\in\Theta$, and denote by $X_{\theta}$ the random variable obtained by fixing $\theta$ during the experiment. Denote by $\left\{X_{1, \theta}, X_{2, \theta}, \dots, X_{n, \theta}\right\}$ a sequence of random variables obtained by fixing $\theta$ during the experiment. Then the sequence is said to be pointwise convergent in probability to $X$ if for each $\theta\in\Theta$
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) = 0
\end{equation}
for all $\varepsilon > 0$. That is, convergence in probability holds for any value of $\theta\in\Theta$. An alternative equivalent definition is that given any $\varepsilon > 0$, $\delta > 0$ and $\theta\in\Theta$, one can find an integer $n_{0}$ (possibly dependent on $\varepsilon, \delta, \theta$) such that
\begin{equation}
\operatorname{Pr}\left(\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) < \delta
\end{equation}
if $n > n_{0}$. Pointwise convergence in probability can be analogously defined for sequences of random vectors (using $\left\Vert\cdot\right\Vert$ rather than $\left|\cdot\right|$).

\subsection{Uniform Convergence in Probability}

Uniform convergence in probability is a stronger condition of pointwise convergence in probability. Denote by $X$ a random variable obtained from an experiment parametrised by $\theta\in\Theta$, and denote by $X_{\theta}$ the random variable obtained by fixing $\theta$ during the experiment. Denote by $\left\{X_{1, \theta}, X_{2, \theta}, \dots, X_{n, \theta}\right\}$ a sequence of random variables obtained by fixing $\theta$ during the experiment. Then the sequence is said to be uniformly convergent in probability to $X$ if
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\sup_{\theta\in\Theta}\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) = 0
\end{equation}
That is, $\sup_{\theta\in\Theta}\left|X_{n, \theta} - X_{\theta}\right|$ converges in probability to zero. By examining an alternative equivalent definition, we have that given any $\varepsilon > 0$, $\delta > 0$, one can find an integer $n_{0}$ (possibly dependent on $\varepsilon, \delta$) such that 
\begin{equation}
\operatorname{Pr}\left(\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) < \delta
\end{equation}
for all $\theta\in\Theta$ if $n > n_{0}$. The difference from pointwise convergence in probability is that one must now relax dependence on $\theta$. So we need to find a $n_{0}$ for a given $\varepsilon, \delta$ that works for all $\theta\in\Theta$. Uniform convergence in probability can be analogously defined for sequences of random vectors (using $\left\Vert\cdot\right\Vert$ rather than $\left|\cdot\right|$).

\subsection{Almost Sure Convergence}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ converges almost surely towards random variable $X$ if
\begin{equation}
\operatorname{Pr}\left(\lim_{n\to\infty}X_{n} = X\right) = 1
\end{equation}
A sequence of random vectors $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ converges almost surely towards random vector $\mathbf{X}$ if
\begin{equation}
\operatorname{Pr}\left(\lim_{n\to\infty}\mathbf{X}_{n} = \mathbf{X}\right) = 1
\end{equation}
where the equality is evaluated element-wise. Almost sure convergence may be denoted with the $\overset{\mathrm{a.s.}}\to$ symbol.\\

Almost sure convergence implies convergence in probability, however convergence in probability does not imply almost sure convergence. For example, consider the sequence where $X_{n}$ takes the value $1$ with probability $\dfrac{1}{n}$, and takes the value of $0$ otherwise. Although this sequence converges in probability to $0$, since the series $\sum_{n = 1}^{\infty}\operatorname{Pr}\left(X_{n} = 1\right) = \sum_{n = 1}^{\infty}\dfrac{1}{n}$ diverges, and each $X_{n}$ is independent, we have by the converse Borel-Cantelli lemma that there is probability 1 that the event $X_{n} = 1$ occurs infinitely many times. Therefore the sequence does not almost surely converge to $0$.

\subsection{Complete Convergence}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots \right\}$ is said to be completely convergent to $X$ if
\begin{equation}
\sum_{n = 1}^{\infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) < \infty
\end{equation}
for all $\varepsilon > 0$. If $X_{n}$ are independent, then complete convergence is equivalent to almost sure convergence.
\begin{proof}
To show that complete convergence implies almost sure convergence, let event $E_{n}$ be $\left\{\left|X_{n} - X\right| > \varepsilon\right\}$ for some $\varepsilon > 0$. Then since $\sum_{n = 1}^{\infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) < \infty$ by complete convergence, the Borel-Cantelli lemma tells us that
\begin{equation}
\operatorname{Pr}\left(\limsup_{n\to\infty}\left\{\left|X_{n} - X\right| > \varepsilon\right\}\right) = 0
\end{equation}
Since $\varepsilon$ can be arbitrarily small, then
\begin{align}
\operatorname{Pr}\left(\limsup_{n\to\infty}\left\{\left|X_{n} - X\right| = 0\right\}\right) &= 1 \\
\operatorname{Pr}\left(\lim_{n\to\infty}X_{n} = X\right) &= 1
\end{align}
To show that almost sure convergence implies complete convergence, first suppose we have almost sure convergence but not complete converge. This means there exists an $\varepsilon > 0$ such that
\begin{equation}
\sum_{n = 1}^{\infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) = \infty
\end{equation}
Given that the sequence $\left\{X_{n}\right\}$ is independent, we can verify that $\left\{X_{n} - X\right\}$ is also independent as follows. Since $X_{n} \to X$, then $X$ is almost sure constant. Hence $\left\{X_{n} - X\right\}$ is independent. By the converse Borel-Cantelli lemma, this gives for that particular $\varepsilon > 0$:
\begin{equation}
\operatorname{Pr}\left(\limsup_{n\to\infty}\left\{\left|X_{n} - X\right| > \varepsilon\right\}\right) = 1
\end{equation}
This contradicts almost sure convergence, hence almost sure convergence implies complete convergence.
\end{proof}

\subsection{With High Probability}

An event $E$ occurs with high probability if the probability $\operatorname{Pr}\left(E\right)$ depends on some number $n$, and $\operatorname{Pr}\left(E\right) \to 1$ as $n \to \infty$. That is, we can make the probability as close as desired to $1$ by making $n$ big enough.

\subsection{Weak Law of Large Numbers}

Let $X_{1}, \dots, X_{n}$ be a sequence of independent and identically distributed random variables, with mean $\operatorname{E}\left[X_{i}\right] = \mu$ and standard deviation $\sigma$. Then the sample mean $\overline{X}$ is defined as
\begin{equation}
\overline{X} = \dfrac{X_{1} + \dots + X_{n}}{n}
\end{equation}
The Weak Law of Large Numbers states that $\overline{X} \overset{\mathrm{p}}{\rightarrow} \mu$ as $n \rightarrow \infty$. That is, the sample mean converges in probability to the population mean as the sample size increases to infinity. We can also write this as
\begin{equation}
\lim_{n\rightarrow\infty}\operatorname{Pr}\left(\left|\overline{X} - \mu\right|\geq \varepsilon \right) = 0
\end{equation}
for all $\varepsilon > 0$.
\begin{proof}
From Chebychev's inequality
\begin{equation}
\operatorname{Pr}\left(\left|\overline{X} - \mu\right| \geq \varepsilon\right) \leq \dfrac{\operatorname{Var}\left(\overline{X}\right)}{\varepsilon^{2}}
\end{equation}
Using $\operatorname{Var}\left(\overline{X}\right) = \sigma^{2}/n$
\begin{equation}
\operatorname{Pr}\left(\left|\overline{X} - \mu\right| \geq \varepsilon\right) \leq \dfrac{\sigma^{2}}{n\varepsilon^{2}}
\end{equation}
Taking the limit
\begin{equation}
\lim_{n\rightarrow\infty}\operatorname{Pr}\left(\left|\overline{X} - \mu\right|\geq \varepsilon \right) = 0
\end{equation}
\end{proof}

\subsection{Strong Law of Large Numbers}

The Strong Law of Large Numbers is a stronger version of the Weak Law of Large Numbers which says that the sample mean converges almost surely to the population mean. That is,
\begin{equation}
\operatorname{Pr}\left(\lim_{n\rightarrow\infty}\overline{X} = \mu\right) = 1
\end{equation}

\subsection{Continuous Mapping Theorem}

The continuous mapping theorem states that limits of convergent sequences of random elements are preserved through a continuous mapping. Formally, let $\left\{\mathbf{X}_{n}\right\}$ be a sequence of random vectors converging in probability to the random vector $\mathbf{X}$. Let $g: \mathbb{R}^{d} \to \mathbb{R}^{m}$ be an arbitrary mapping that is continuous over the support of $\mathbf{X}$ (where $d$ is the dimension of $\mathbf{X}$ and $m$ is the dimension being mapped to). Then
\begin{equation}
g\left(\mathbf{X}_{n}\right) \overset{\mathrm{p}}\to g\left(\mathbf{X}\right)
\end{equation}
Similarly, if $\left\{\mathbf{X}_{n}\right\}$ is a sequence of random vectors converging in distribution to $\mathbf{X}$, then
\begin{equation}
g\left(\mathbf{X}_{n}\right) \overset{\mathrm{d}}\to g\left(\mathbf{X}\right)
\end{equation}
and analogously if $\left\{\mathbf{X}_{n}\right\}$ is a sequence of random vectors converging almost surely to $\mathbf{X}$, then
\begin{equation}
g\left(\mathbf{X}_{n}\right) \overset{\mathrm{a.s.}}\to g\left(\mathbf{X}\right)
\end{equation}

\subsection{Slutsky's Theorem}

Slutsky's theorem describes properties of algebraic operations on convergent sequences of random variables. Let $\left\{X_{n}\right\}$, $\left\{Y_{n}\right\}$ be sequences of random variables. Suppose $\left\{X_{n}\right\}$ converges in distribution to random variable $X$, while $\left\{Y_{n}\right\}$ converges in probability to a constant $c$. Then
\begin{gather}
X_{n} + Y_{n} \overset{\mathrm{d}}{\to} X + c \\
X_{n}Y_{n} \overset{\mathrm{d}}{\to} cX \\
X_{n}/Y_{n} \overset{\mathrm{d}}{\to} X/c
\end{gather}
where the last property requires that $c\neq 0$. Analogous results hold if $\left\{X_{n}\right\}$ were to converge in probability to $X$.

\subsection{Dominated Convergence Theorem}

Suppose there is a sequence of random variables $\left\{X_{n}\right\}$ converging to $X$ almost surely as $n\to\infty$. If there exists a random variable $Y$ such that $\mathbb{E}\left[Y\right] < \infty$ and $\left|X_{n}\right| < Y$ for all $n$, then this implies expectation and limits can be exchanged, ie.
\begin{equation}
\lim_{n\to\infty}\mathbb{E}\left[X_{n}\right] = \mathbb{E}\left[\lim_{n\to\infty}X_{n}\right] = \mathbb{E}\left[X\right]
\end{equation}
Although it might seem intuitive that the expectation and limit be always exchangeable, a simple counterexample can be given. Let a sequence of random variables be defined by
\begin{equation}
X_{n} = \begin{cases} 2^{n}, & \mathrm{w.p.}\ \dfrac{1}{2^{n}} \\ 0, & \mathrm{w.p.}\ 1 - \dfrac{1}{2^{n}}\end{cases}
\end{equation}
The expectation can be calculated to be $\mathbb{E}\left[X_{n}\right] = 1$ for all $n$. However, notice that $X_{n} \overset{\mathrm{a.s.}}{\to} 0$. This can be shown using the Borel-Cantelli lemma, by noting the convergent series
\begin{equation}
\sum_{n = 1}^{\infty}\operatorname{Pr}\left(X_{n} = 1\right) = \sum_{n = 1}^{\infty}\dfrac{1}{2^{n}} = 1
\end{equation}
so the event $\left\{X_{n} = 1\right\}$ occurs finitely many times, meaning $\operatorname{Pr}\left(\lim_{n\to\infty}X_{n} = 0\right) = 1$. Therefore in this case,
\begin{equation}
\mathbb{E}\left[X_{n}\right] = 1 \neq \mathbb{E}\left[\lim_{n\to\infty}X_{n}\right] = \mathbb{E}\left[0\right] = 0
\end{equation}
because there does not exist a random variable $Y$ with finite expectation such that $\left|X_{n}\right| < Y$ for all $n$.

\section{Branching Processes}

\chapter{Intermediate Statistics}

\section{Statistical Decision Theory}

\subsection{Optimal Prediction}

Suppose we have to make a prediction about the realisation of a random variable $Y$, which has the probability density function $f_{Y}\left(y\right)$. One reasonable choice of the prediction $\hat{Y}$ is the value which minimises the expected squared deviation of the realisation of the prediction, ie.
\begin{align}
\hat{Y} &= \argmin_{c}\mathbb{E}\left[\left(Y - c\right)^{2}\right] \\
&= \argmin_{c}\int_{-\infty}^{\infty}f_{Y}\left(y\right)\left(y - c\right)^{2}dy
\end{align}
We can derive $\hat{Y}$ by taking the derivative (supposing sufficient regularity to do so):
\begin{align}
\dfrac{\partial}{\partial c}\int_{-\infty}^{\infty}f_{Y}\left(y\right)\left(y - c\right)^{2}dy &= \int_{-\infty}^{\infty}f_{Y}\left(y\right)\dfrac{\partial}{\partial c}\left(y - c\right)^{2}dy \\
&= -\int_{-\infty}^{\infty}f_{Y}\left(y\right)2\left(y - c\right)dy
\end{align}
Setting the derivative to zero:
\begin{gather}
\int_{-\infty}^{\infty}f_{Y}\left(y\right)\left(y - \hat{Y}\right)dy = 0 \\
\int_{-\infty}^{\infty}f_{Y}\left(y\right)ydy = \hat{Y}\int_{-\infty}^{\infty}f_{Y}\left(y\right)dy \\
\hat{Y} = \int_{-\infty}^{\infty}f_{Y}\left(y\right)ydy \\
\hat{Y} = \mathbb{E}\left[Y\right]
\end{gather}
Hence the expectation of $Y$ (if it exists) is the optimal prediction in this sense. The same can be analogously shown if $Y$ is a discrete random variable. In most cases arising in statistics, the true distribution of $Y$ will not be known, nor even it's population mean. If however we have a sample drawn from the distribution of $Y$, we can take the sample mean to give an unbiased estimate of $\mathbb{E}\left[Y\right]$. These optimal predictions can also be conditional on predictors $X$, in which case we take the conditional expectation. It can be analogously shown that the optimal prediction conditional on predictors $X$ defined as
\begin{align}
\hat{Y}|X &= \argmin_{c}\mathbb{E}\left[\left(Y - c\right)^{2}\middle|X\right] \\
&= \argmin_{c}\int_{-\infty}^{\infty}f_{Y|X}\left(y\right)\left(y - c\right)^{2}dy
\end{align}
is equal to
\begin{equation}
\hat{Y}|X = \mathbb{E}\left[Y\middle|X\right]
\end{equation}
Hence this justifies estimating the conditional mean from a sample for predictive or modelling purposes. We choose the model regression function to ideally give a good estimate of
\begin{equation}
f\left(x\right) = \mathbb{E}\left[Y\middle| X = x\right]
\end{equation}
If we instead change the prediction criterion to minimising the expected absolute deviation:
\begin{equation}
\hat{Y} = \argmin_{c}\mathbb{E}\left[\left|Y - c\right|\right]
\end{equation}
then we can derive an alternative prediction rule by first writing (supposing that $Y$ is a continuous random variable):
\begin{align}
\mathbb{E}\left[\left|Y - c\right|\right] &= \int_{-\infty}^{\infty}f_{Y}\left(y\right)\left|y - c\right|dy \\
&= \int_{-\infty}^{c}f_{Y}\left(y\right)\left(c - y\right)dy + \int_{c}^{\infty}f_{Y}\left(y\right)\left(y - c\right)dy
\end{align}
We seek to find the value for which the right derivative of the first term equals the negative of the left derivative of the second term (the whole term is not differentiable everywhere). We can further split this into
\begin{gather}
\int_{-\infty}^{c}f_{Y}\left(y\right)\left(c - y\right)dy = c\int_{-\infty}^{c}f_{Y}\left(y\right)dy - \int_{-\infty}^{c}f_{Y}\left(y\right)ydy \\
\int_{c}^{\infty}f_{Y}\left(y\right)\left(y - c\right)dy = \int_{c}^{\infty}f_{Y}\left(y\right)ydy - c\int_{c}^{\infty}f_{Y}\left(y\right)dy
\end{gather}
Note that by the Fundamental Theorem of Calculus, we have
\begin{align}
\dfrac{\partial }{\partial c}\int_{c}^{\infty}f_{Y}\left(y\right)dy &= \dfrac{\partial }{\partial c}\left(\lim_{y \to \infty}F_{Y}\left(y\right) - F_{Y}\left(c\right)\right) \\
&= \dfrac{\partial }{\partial c}\left(1 - F_{Y}\left(c\right)\right) \\
&= -f_{Y}\left(c\right)
\end{align}
where $F_{Y}\left(y\right)$ is the cumulative distribution function of $Y$. Similarly, $\dfrac{\partial }{\partial c}\int_{c}^{\infty}f_{Y}\left(y\right)ydy = -f_{Y}\left(c\right)c$ and by the product rule, $\dfrac{\partial }{\partial c}c\int_{c}^{\infty}f_{Y}\left(y\right)dy = \int_{c}^{\infty}f_{Y}\left(y\right)dy - cf_{Y}\left(c\right)$. Using these to take the derivative the terms above we obtain
\begin{align}
\dfrac{\partial }{\partial c}\int_{-\infty}^{c}f_{Y}\left(y\right)\left(c - y\right)dy &= cf_{Y}\left(c\right) + \int_{-\infty}^{c}f_{Y}\left(y\right)dy - f_{Y}\left(c\right)c \\
&= \int_{-\infty}^{c}f_{Y}\left(y\right)dy
\end{align}
and
\begin{align}
\dfrac{\partial }{\partial c}\int_{c}^{\infty}f_{Y}\left(y\right)\left(y - c\right)dy &= -f_{Y}\left(c\right)c - \int_{c}^{\infty}f_{Y}\left(y\right)dy + cf_{Y}\left(c\right) \\
&= - \int_{c}^{\infty}f_{Y}\left(y\right)dy
\end{align}
Therefore we find the value of $c$ for which
\begin{equation}
\int_{-\infty}^{c}f_{Y}\left(y\right)dy = \int_{c}^{\infty}f_{Y}\left(y\right)dy
\end{equation}
This value happens to be the median $m_{Y}$ of $Y$, since at the median,
\begin{gather}
\int_{-\infty}^{m_{Y}}f_{Y}\left(y\right)dy = \dfrac{1}{2} \\
\int_{m_{Y}}^{\infty}f_{Y}\left(y\right)dy = \dfrac{1}{2}
\end{gather}
So the optimal prediction is the population median
\begin{equation}
\hat{Y} = m_{Y}
\end{equation}
If we have a sample drawn from the population of $Y$, then we may estimate the population median using the sample median. Likewise, if given a predictor $x$, it can be shown via roughly the same steps that the ideal predictive function is the conditional median (ie. median of the conditional distribution):
\begin{align}
f\left(x\right) &= \argmin_{c}\mathbb{E}\left[\left|Y - c\right|\middle|X = x\right] \\
&= m_{Y|X = x}
\end{align}

\subsection{Maximum Likelihood Hypothesis Testing}

\subsection{Maximum a Posteriori Hypothesis Testing}

\subsection{Minimum Cost Hypothesis Testing}

\subsection{Neyman-Pearson Lemma}

\subsection{Receiver Operating Characteristic}

\subsection{Uncertainty Quantification}

Uncertainty quantification is the science of characterising uncertainty (ie. the distribution of errors, in a statistical sense) for a model.

\subsubsection{Propagation of Errors}

Consider a generally nonlinear model $f\left(x\right)$ in the explanatory variables $x$, where $x$ is a vector of dimension $n$. However, it is known that we obtain or measure $x$ with some error $\varepsilon$ with $\operatorname{Cov}\left(\varepsilon\right) = \Sigma$, so that we have the random vector $\mathbf{X} := x + \varepsilon$. We then ask how this error propagates through to the estimate $f\left(\mathbf{X}\right)$. Suppose $\varepsilon$ is a continuous random variable and $f\left(x\right)$ is differentiable. Then by a first order Taylor series approximation,
\begin{align}
f\left(x + \varepsilon\right) &\approx f\left(x\right) + \sum_{i = 1}^{n}\dfrac{\partial f}{\partial x_{i}}\varepsilon_{i} \\
&\approx f\left(x\right) + \nabla f\left(x\right)^{\top}\varepsilon
\end{align}
So then we can approximate the variance of $f\left(x + \varepsilon\right)$ by
\begin{align}
\operatorname{Var}\left(f\left(x + \varepsilon\right)\right) &\approx \operatorname{Var}\left(f\left(x\right) + \nabla f\left(x\right)^{\top}\varepsilon\right) \\
&\approx \operatorname{Cov}\left(\nabla f\left(x\right)^{\top}\varepsilon\right) \\
&\approx \nabla f\left(x\right)^{\top}\operatorname{Cov}\left(\varepsilon\right)\nabla f\left(x\right) \\
&\approx \nabla f\left(x\right)^{\top}\Sigma\nabla f\left(x\right)
\end{align}
More generally, suppose we have a vector-valued nonlinear model $\mathbf{f}\left(x\right)$, then let $\mathbf{J} = \dfrac{\partial \mathbf{f}}{\partial x}$ denote the Jacobian matrix. Then the first order Taylor series approximation is
\begin{equation}
\mathbf{f}\left(x + \varepsilon\right) \approx \mathbf{f}\left(x\right) + \mathbf{J}\varepsilon
\end{equation}
and by the same vein, an approximation of the error propagation is
\begin{equation}
\operatorname{Cov}\left(\mathbf{f}\left(x + \varepsilon\right)\right) \approx \mathbf{J}\Sigma\mathbf{J}^{\top}
\end{equation}
Even more generally, suppose we have a composition of nonlinear mappings $\mathbf{g}\circ \mathbf{f}\left(x\right)$. Then taking the Jacobian matrix $\dfrac{\partial \mathbf{g}}{\partial \mathbf{f}}$ we come up with the approximation
\begin{equation}
\operatorname{Cov}\left(\mathbf{g}\circ\mathbf{f}\left(x + \varepsilon\right)\right) \approx \dfrac{\partial \mathbf{g}}{\partial \mathbf{f}}\mathbf{J}\Sigma\mathbf{J}^{\top}\dfrac{\partial \mathbf{g}}{\partial \mathbf{f}}^{\top}
\end{equation}
We can in this way approximate the error propagation through an arbitrary amount of nested nonlinear models by chaining Jacobians, keeping in mind that the approximation may get worse and worse through multiple compositions.

\section{Least Squares}

\subsection{Ordinary Least Squares}

Suppose we have $n$ data pairs consisting of regressors $\mathbf{x}_{i}$ (vectors) and scalar responses $y_{i}$. The task is to fit a linear model of the form $\hat{y}_{i} = \hat{\beta}^{\top}\mathbf{x}_{i}$ where $\hat{\beta}$ is an estimated $d$-dimensional vector of parameters. We use the least squares cost function
\begin{align}
V\left(\beta\right) &= \dfrac{1}{2}\sum_{i = 1}^{n}\left(y_{i} - \hat{y}_{i}\right)^{2} \\
&= \dfrac{1}{2}\sum_{i = 1}^{n}\left(y_{i} - \beta^{\top}\mathbf{x}_{i}\right)^{2}
\end{align}
Using matrix notation, this problem can be written down more compactly. Denote by the matrices $\mathbf{X}$ and $\mathbf{Y}$:
\begin{gather}
\mathbf{X} = \begin{bmatrix}
\mathbf{x}_{1}^{\top} \\ \vdots \\ \mathbf{x}_{n}^{\top}
\end{bmatrix} \\
\mathbf{Y} = \begin{bmatrix} y_{1} & \dots & y_{n} \end{bmatrix}^{\top}
\end{gather}
Hence $\mathbf{X}$ is a `tall' matrix and $\mathbf{Y}$ is actually a column vector. Note that if we wish to include a constant in the regression function, ie. $\hat{y}_{i} = \beta_{0} + \beta_{1}x_{i,1} + \dots + \beta_{d - 1}x_{i, d - 1}$, then we may simply let the first element of $\mathbf{x}_{i}$ be equal to $1$. Using this matrix notation, we can rewrite the least squares problem as
\begin{align}
\hat{\beta} &= \argmin_{\beta}V\left(\beta\right) \\
 &= \argmin_{\beta}\dfrac{1}{2}\left(\mathbf{Y} - \mathbf{X}\beta\right)^{\top}\left(\mathbf{Y} - \mathbf{X}\beta\right)
\end{align}
Expanding out the quadratic cost function,
\begin{equation}
V\left(\beta\right) = \dfrac{1}{2}\mathbf{Y}^{\top}\mathbf{Y} - \mathbf{Y}^{\top}\mathbf{X}\beta + \dfrac{1}{2}\beta^{\top}\mathbf{X}^{\top}\mathbf{X}\beta
\end{equation}
Taking the derivative:
\begin{equation}
\nabla V\left(\beta\right) = -\mathbf{X}^{\top}\mathbf{Y} + \mathbf{X}^{\top}\mathbf{X}\beta
\end{equation}
Setting this to zero and solving obtains the ordinary least squares estimator.
\begin{gather}
-\mathbf{X}^{\top}\mathbf{Y} + \mathbf{X}^{\top}\mathbf{X}\hat{\beta} = 0 \\
\hat{\beta} = \left(\mathbf{X}^{\top}\mathbf{X}\right)^{-1}\mathbf{X}^{\top}\mathbf{Y}
\end{gather}
Note that the matrix $\mathbf{X}^{\dagger} = \left(\mathbf{X}^{\top}\mathbf{X}\right)^{-1}\mathbf{X}^{\top}$ is the left Moore-Penrose pseudoinverse of $\mathbf{X}$. So we can alternatively write the least squares estimator as
\begin{equation}
\hat{\beta} = \mathbf{X}^{\dagger}\mathbf{Y}
\end{equation}
which is the value of $\beta$ which `solves' $\mathbf{X}\beta = \mathbf{Y}$.

\subsection{Total Least Squares}

\subsection{Deming Regression}

\subsection{Weighted Least Squares}

In the case where we `trust' some observations more than others, we can modify the least squares criterion with weightings. Introduce weights $w_{i} > 0$ for each observation $i = 1, \dots, n$. The weighted linear least squares cost function becomes
\begin{align}
V\left(\beta\right) &= \dfrac{1}{2}\sum_{i = 1}^{n}w_{i}\left(y_{i} - \beta^{\top}\mathbf{x}_{i}\right)^{2}
\end{align}
Note that only the relative values of the weights matter. For example, we may set all $w_{i} = 1$ by default and adjust $w_{i} < 1$ for observations we do not trust or $w_{i} > 1$ for observations we really trust. Using matrix notation, denote by $\mathbf{W}$ the diagonal weighting matrix:
\begin{equation}
\mathbf{W} = \begin{bmatrix}
w_{1} & & \\ & \ddots & \\ & & w_{n}
\end{bmatrix}
\end{equation}
Then
\begin{equation}
V\left(\beta\right) = \dfrac{1}{2}\left(\mathbf{Y} - \mathbf{X}\beta\right)^{\top}\mathbf{W}\left(\mathbf{Y} - \mathbf{X}\beta\right)
\end{equation}
Solving for the estimator $\hat{\beta}$ takes similar steps as for ordinary least squares, and yields
\begin{equation}
\hat{\beta} = \left(\mathbf{X}^{\top}\mathbf{W}\mathbf{X}\right)^{-1}\mathbf{X}^{\top}\mathbf{W}\mathbf{Y}
\end{equation}
Note that if $w_{1} = \dots = w_{n}$, then the problem effectively reduces to ordinary least squares.

\subsection{Generalised Least Squares}

\subsection{Regularised Least Squares}

\subsection{Recursive Least Squares}

\subsection{Gauss-Newton Algorithm}

The Gauss-Newton algorithm is a method for solving nonlinear least squares problems. It is based on the Newton method of finding local optima, however it does not require computation of the Hessian (which can be unwieldy for particular nonlinear functions). Suppose we have $n$ data pairs $\left(x_{i}, y_{i}\right)$ and would like to fit a nonlinear model $f\left(x_{i}, \theta\right)$ with respect to a $d$-dimensional vector of parameters $\theta$. Then define the residuals
\begin{equation}
r_{i}\left(\theta\right) = y_{i} - f\left(x_{i}, \theta\right)
\end{equation}
and the residual vector
\begin{equation}
r\left(\theta\right) = \begin{bmatrix} r_{1}\left(\theta\right) & \dots & r_{n}\left(\theta\right) \end{bmatrix}^{\top}
\end{equation}
Then the least squares problem is to minimise (half) the sum of squared residuals $V\left(\theta\right)$:
\begin{align}
\theta^{*} &= \argmin_{\theta}V\left(\theta\right) \\
&= \argmin_{\theta}\dfrac{1}{2}\sum_{i = 1}^{n}r_{i}\left(\theta\right)^{2} \\
&= \argmin_{\theta}\dfrac{1}{2}\left\Vert r\left(\theta\right)\right\Vert^{2}
\end{align}
Define the Jacobian matrix as
\begin{align}
\mathbf{J}\left(\theta\right) &= \dfrac{\partial r\left(\theta\right)}{\partial \theta} \\
&= \begin{bmatrix}\partial r_{1}/\partial\theta_{1} & \dots & \partial r_{1}/\partial\theta_{d}\\
\vdots & \ddots & \vdots\\
\partial r_{n}/\partial\theta_{1} & \dots & \partial r_{n}/\partial\theta_{d}
\end{bmatrix} \\
&= \begin{bmatrix}
\nabla r_{1}\left(\theta\right)^{\top} \\ \vdots \\ \nabla r_{n}\left(\theta\right)
\end{bmatrix}
\end{align}
Taking the derivative of the least squares cost function, we have by the chain rule
\begin{align}
\nabla V\left(\theta\right) &= \dfrac{1}{2}\sum_{i = 1}^{n}2r_{i}\left(\theta\right)\nabla r_{i}\left(\theta\right) \\
&= \begin{bmatrix} \nabla r_{1}\left(\theta\right) & \dots & r_{n}\left(\theta\right)\end{bmatrix}^{\top} \begin{bmatrix} r_{1}\left(\theta\right) \\ \vdots \\ r_{n}\left(\theta\right) \end{bmatrix} \\
&= \mathbf{J}\left(\theta\right)^{\top}r\left(\theta\right)
\end{align}
We also obtain the Hessian of the least squares cost function by the product rule
\begin{align}
\nabla^{2}V\left(\theta\right) &= \sum_{i = 1}^{n}\nabla r_{i}\left(\theta\right)\nabla r_{i}\left(\theta\right)^{\top} + \sum_{i = 1}^{n}r_{i}\left(\theta\right)\nabla^{2}r_{i}\left(\theta\right) \\
&= \begin{bmatrix} \nabla r_{1}\left(\theta\right) & \dots & r_{n}\left(\theta\right)\end{bmatrix}^{\top} \begin{bmatrix} \nabla r_{1}\left(\theta\right)^{\top} \\ \vdots \\ \nabla r_{n}\left(\theta\right)^{\top} \end{bmatrix} + \sum_{i = 1}^{n}r_{i}\left(\theta\right)\nabla^{2}r_{i}\left(\theta\right) \\
&= \mathbf{J}\left(\theta\right)^{\top}\mathbf{J}\left(\theta\right) + \sum_{i = 1}^{n}r_{i}\left(\theta\right)\nabla^{2}r_{i}\left(\theta\right)
\end{align}
In the Gauss-Newton algorithm, the approximation is made on the Hessian
\begin{equation}
\nabla^{2}V\left(\theta\right) \approx \mathbf{J}\left(\theta\right)^{\top}\mathbf{J}\left(\theta\right)
\end{equation}
This approximation is reasonable if the residuals $r_{i}\left(\theta\right)$ are small, which are what we are trying to make small with the objective of least squares. Using this approximation to an update step identical to the Newton algorithm, the parameter update $\theta_{k + 1}$ after $k$ iterations is given by
\begin{equation}
\theta_{k + 1} = \theta_{k} - \left(\mathbf{J}\left(\theta_{k}\right)^{\top}\mathbf{J}\left(\theta_{k}\right)\right)^{-1}\mathbf{J}\left(\theta_{k}\right)^{\top}r\left(\theta_{k}\right)
\end{equation}

\subsection{Levenberg-Marquardt Algorithm}

Also known as damped least squares, the Levenberg-Marquardt algorithm finds a middle ground between gradient descent and the Gauss-Newton algorithm. The update step is given by
\begin{equation}
\theta_{k + 1} = \theta_{k} - \left(\mathbf{J}\left(\theta_{k}\right)^{\top}\mathbf{J}\left(\theta_{k}\right) + \lambda I\right)^{-1}\mathbf{J}\left(\theta_{k}\right)^{\top}r\left(\theta_{k}\right)
\end{equation}
where $\lambda$ is known as the damping parameter. Note that if $\lambda$ is small, the update will be closer to the Levenberg-Marquardt algorithm. If $\lambda$ is relatively large, then the update direction will be closer to the (steepest) gradient descent direction. The choice of parameter $\lambda$ can be made adaptive, so that it starts off larger (where the Hessian approximation made by the Levenberg-Marquardt algorithm may not be as valid), and reduces over the number of iterations. An alternative update is given by
\begin{equation}
\theta_{k + 1} = \theta_{k} - \left(\mathbf{J}\left(\theta_{k}\right)^{\top}\mathbf{J}\left(\theta_{k}\right) + \lambda \operatorname{diag}\left\{\mathbf{J}\left(\theta_{k}\right)^{\top}\mathbf{J}\left(\theta_{k}\right)\right\}\right)^{-1}\mathbf{J}\left(\theta_{k}\right)^{\top}r\left(\theta_{k}\right)
\end{equation}
where $\operatorname{diag}\left\{\mathbf{J}\left(\theta_{k}\right)^{\top}\mathbf{J}\left(\theta_{k}\right)\right\}$ is a diagonal matrix containing only the main diagonal of $\mathbf{J}\left(\theta_{k}\right)^{\top}\mathbf{J}\left(\theta_{k}\right)$. This version of the update makes it so that the damping term is of the same scale as $\mathbf{J}\left(\theta_{k}\right)^{\top}\mathbf{J}\left(\theta_{k}\right)$.

\section{Estimation Theory}

\subsection{Asymptotic Consistency}

\subsection{Sufficient Statistics}

\subsection{Minimum Mean Square Error Estimators}

\subsection{Minimum Variance Unbiased Estimators}

\subsection{Gauss-Markov Theorem}

\section{Maximum Likelihood Estimation}

\subsection{Maximum Likelihood Justification of Least Squares}

Suppose we have data consisting of $n$ data pairs with regressors $x_{i}$ and observations $y_{i}$ along with a model $f\left(x_{i}; \theta\right)$ which is parametrised by the parameter vector $\theta$ to be estimated. If we assume the data generating process
\begin{equation}
Y_{i} = f\left(x_{i}; \theta\right) + \varepsilon_{i}
\end{equation}
with Gaussian errors $\varepsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$, then $Y_{i} \sim \mathcal{N}\left(f\left(x_{i}; \theta\right), \sigma^{2}\right)$ and the likelihood under an i.i.d. assumption on each $Y_{i}$ is given by
\begin{align}
\mathcal{L}\left(\theta\middle|Y_{1}, \dots, Y_{n}\right) &= p\left(Y_{1}, \dots, Y_{n}\middle|\theta\right) \\
&= p\left(Y_{1}\middle|\theta\right)\dots p\left(Y_{n}\middle|\theta\right) \\
&= \prod_{i = 1}^{n}p\left(Y_{i}\middle|\theta\right) \\
&= \prod_{i = 1}^{n}\dfrac{1}{\sigma\sqrt{2\pi}}\exp\left(-\dfrac{\left(y_{i} - f\left(x_{i};\theta\right)\right)^{2}}{2\sigma^{2}}\right)
\end{align}
The log-likelihood is given by
\begin{equation}
\log\mathcal{L}\left(\theta\middle|Y_{1}, \dots, Y_{m}\right) = -\sum_{i = 1}^{n}\dfrac{1}{2\sigma^{2}}\left(y_{i} - f\left(x_{i};\theta\right)\right)^{2} - \sum_{i = 1}^{n}\log\left(\sigma\sqrt{2\pi}\right)
\end{equation}
and maximising the log-likelihood with respect to the parameters,
\begin{align}
\argmax_{\theta}\log\mathcal{L}\left(\theta\middle|Y_{1}, \dots, Y_{n}\right) &= \argmax_{\theta}\left\{-\sum_{i = 1}^{n}\dfrac{1}{2\sigma^{2}}\left(y_{i} - f\left(x_{i};\theta\right)\right)^{2} - \sum_{i = 1}^{n}\log\left(\sigma\sqrt{2\pi}\right)\right\} \\
&= \argmin_{\theta}\left\{\sum_{i = 1}^{n}\dfrac{1}{2\sigma^{2}}\left(y_{i} - f\left(x_{i};\theta\right)\right)^{2} + \sum_{i = 1}^{n}\log\left(\sigma\sqrt{2\pi}\right)\right\} \\
&= \argmin_{\theta}\sum_{i = 1}^{n}\dfrac{1}{2\sigma^{2}}\left(y_{i} - f\left(x_{i};\theta\right)\right)^{2} \\
&= \argmin_{\theta}\dfrac{1}{2}\sum_{i = 1}^{n}\left(y_{i} - f\left(x_{i};\theta\right)\right)^{2}
\end{align}
so we arrive at the least squares cost function. Hence the least squares fitting problem can be interpreted as a maximum likelihood problem under an i.i.d. assumption on the observations $Y_{i}$ and Gaussian errors $\varepsilon_{i}$. If the standard deviation on the errors is allowed to vary such that $\operatorname{sd}\left(\varepsilon_{i}\right) = \sigma_{i}$, then the estimation problem becomes
\begin{equation}
\theta^{*} = \argmin_{\theta}\dfrac{1}{2}\sum_{i = 1}^{n}\dfrac{\left(y_{i} - f\left(x_{i};\theta\right)\right)^{2}}{\sigma_{i}^{2}}
\end{equation}
and the formulation becomes equivalent to weighted least squares.

\subsection{Log Concavity of Likelihoods}

Sometimes, it is useful to know whether the maximum likelihood problem has a unique solution. Establishing log concavity of the likelihood function can be used to determine this. A function $f: \mathbb{R}^{n}\to\mathbb{R}$ is said to be log concave if $f\left(x\right) > 0$ and $\log f\left(x\right)$ is concave in $x$. This implies $-\log f\left(x\right)$ is convex in $x$. Note that in the context of maximum likelihood, we look for log concavity of the likelihood function in the parameters. A necessary and sufficient condition for log concavity can be obtained by the second derivative of $g\left(x\right) := \log f\left(x\right)$. In the simple case $x\in\mathbb{R}$ and assuming $f\left(x\right)$ is twice differentiable:
\begin{equation}
g'\left(x\right) = \dfrac{f'\left(x\right)}{f\left(x\right)}
\end{equation}
Using the product rule:
\begin{align}
g''\left(x\right) &= -\left[\dfrac{f'\left(x\right)}{f\left(x\right)}\right]^{2} + \dfrac{f''\left(x\right)}{f\left(x\right)} \\
&= \dfrac{f''\left(x\right)f\left(x\right) - f'\left(x\right)^{2}}{f\left(x\right)^{2}}
\end{align}
The condition for concavity of $g\left(x\right)$ is $g''\left(x\right) \leq 0$, so this gives the condition for log concavity of $f\left(x\right)$ as
\begin{equation}
f'\left(x\right)^{2} \geq f''\left(x\right)f\left(x\right)
\end{equation}
while $f'\left(x\right)^{2} > f''\left(x\right)f\left(x\right)$ for strict log concavity. Another useful result is that log concavity of the cumulative distribution function can be established from log concavity of the density function. This can be useful in cases where observations consist of `greater than' or `less than' some parameter.
\begin{theorem}
The cumulative distribution function of a log concave differentiable probability density function is also log concave.
\end{theorem}
\begin{proof}
If $f\left(x\right)$ is a log concave density then $h\left(x\right) := -\log f\left(x\right)$ is convex. The cumulative distribution is
\begin{align}
F\left(x\right) &= \int_{-\infty}^{x}f\left(t\right)dt \\
&= \int_{-\infty}^{x}e^{-h\left(t\right)}dt
\end{align}
The derivatives of $F\left(x\right)$ are
\begin{align}
F'\left(x\right) &= f\left(x\right) \\
&= e^{-h\left(x\right)}
\end{align}
and
\begin{align}
F''\left(x\right) &= -h'\left(x\right)e^{-h\left(x\right)} \\
&= -h'\left(x\right)f\left(x\right)
\end{align}
Consider the case $h'\left(x\right) \geq 0$. Then because $F\left(x\right) > 0$ and $f\left(x\right) > 0$:
\begin{equation}
F\left(x\right)F''\left(x\right) = -F\left(x\right)h'\left(x\right)f\left(x\right) \leq 0
\end{equation}
and
\begin{equation}
F'\left(x\right)^{2} \geq 0
\end{equation}
which implies $F'\left(x\right)^{2} \geq F\left(x\right)F''\left(x\right)$, satisfying the log concavity condition. Now consider the case $h'\left(x\right) < 0$. By the property that tangents of a convex function never lie above the graph, we get
\begin{equation}
h\left(t\right) \geq h\left(x\right) + h'\left(x\right)\left(t - x\right)
\end{equation}
Hence
\begin{equation}
\int_{-\infty}^{x}e^{-h\left(t\right)}dt \leq \int_{-\infty}^{x}e^{-h\left(x\right) - h'\left(x\right)\left(t - x\right)
}dt
\end{equation}
We show for the right hand side:
\begin{align}
\int_{-\infty}^{x}e^{-h\left(x\right) - h'\left(x\right)\left(t - x\right)
}dt &= e^{-h\left(x\right) + xh'\left(x\right)}\int_{-\infty}^{x}e^{-th'\left(x\right)}dt \\
&= e^{-h\left(x\right) + xh'\left(x\right)}\dfrac{e^{-th'\left(x\right)}}{h'\left(x\right)} \\
&= \dfrac{e^{-h\left(x\right)}}{h'\left(x\right)}
\end{align}
Multiplying both sides of the inequality by $-h'\left(x\right)e^{-h\left(x\right)}$:
\begin{equation}
-h'\left(x\right)e^{-h\left(x\right)}\int_{-\infty}^{x}e^{-h\left(t\right)}dt \leq e^{-2h\left(x\right)}
\end{equation}
which is the same as the log concavity condition:
\begin{equation}
F''\left(x\right)F\left(x\right) \leq F\left(x\right)^{2}
\end{equation}
\end{proof}

\subsection{Expectation Maximisation Algorithm}

The expectation maximisation (EM) algorithm is suited to more difficult maximum likelihood problems with latent (unobserved values). Assume that the parametric density of complete data $X$ has density $p\left(x\middle|\theta\right)$, while observed data $Y$ has parametric density $p\left(y\middle|\theta\right)$, with the goal of finding the maximum likelihood estimate:
\begin{equation}
\hat{\theta}_{\mathrm{MLE}} = \argmax_{\theta\in\Theta}\log p\left(y\middle|\theta\right)
\end{equation}
Starting from some initial guess $\hat{\theta}_{0}$ of $\theta$ and until some termination condition, the $k$\textsuperscript{th} iteration of the algorithm consists of the following steps \cite{Gupta2010}:
\begin{enumerate}
\item Formulate the conditional probability distribution $p\left(x\middle|y, \hat{\theta}_{k}\right)$ for $X$.
\item Calculate the conditional expected log-likelihood:
\begin{align}
Q\left(\theta;\hat{\theta}_{k}\right) &= \int\log p\left(x\middle|\theta\right)p\left(x\middle|y, \hat{\theta}_{k}\right)dx \\
&= \mathbb{E}_{X}\left[\log p\left(X\middle|\theta\right) \middle| Y, \hat{\theta}_{k}\right]
\end{align}
\item Find
\begin{equation}
\hat{\theta}_{k + 1} = \argmax_{\theta\in\Theta}Q\left(\theta;\hat{\theta}_{k}\right) 
\end{equation}
\end{enumerate}
Intuitively, the EM algorithm can be thought to be iteratively making guesses $X$, then this can be used to make a guess about $\theta$, which can be used to make a better guess about $X$, and so on.

\subsection{M-estimators}

M-estimators generalise maximum likelihood estimators. M-estimators are a class of estimators that are obtained by minimisation of a negative sum of logs (of which maximum likelihood estimation is a special case).

\subsection{Extremum Estimators}

Extremum estimators are a class of estimators which further generalise M-estimators. Extremum estimators are obtained by the minimisation (or maximisation) of some objective function which depends on the data.

\section{Fisher Information}

Let $f\left(x;\theta\right)$ be a probability density function for the random variable $X$ parametrised by $\theta$. Note that $f\left(X; \theta\right)$ is a random variable for the density of $X$, and that $f\left(x;\theta\right)$ is also a likelihood function for $\theta$. The partial derivative of the log likelihood with respect to $\theta$ is called the score $\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)$. It can be shown that the expected value of the score with respect to $X$ is zero:
\begin{align}
\mathbb{E}_{X}\left[\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right] & \begin{multlined}= \mathbb{E}_{X}\left[\dfrac{1}{f\left(X;\theta\right)}\dfrac{\partial}{\partial \theta}f\left(X;\theta\right)\right] \\
\text{\blue by the chain rule\black}
\end{multlined} \\
&= \int_{-\infty}^{\infty}\dfrac{1}{f\left(x;\theta\right)}\cdot\dfrac{\partial}{\partial \theta}f\left(x;\theta\right)\cdot f\left(x;\theta\right) dx \\
&= \int_{-\infty}^{\infty}\dfrac{\partial}{\partial \theta}f\left(x;\theta\right) dx \\
&= \dfrac{\partial}{\partial \theta}\left(\int_{-\infty}^{\infty}f\left(x;\theta\right) dx\right) \\
&= \dfrac{\partial}{\partial \theta}\left(1\right) \\
&= 0
\end{align}
The Fisher information $\mathcal{I}\left(\theta\right)$ is defined as the variance of the score
\begin{align}
\mathcal{I}\left(\theta\right) &= \operatorname{Var}\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right) \\
&= \mathbb{E}_{X}\left[\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2}\right] - \green\cancelto{0}{\black\mathbb{E}_{X}\left[\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right]^{2}} \\
&= \mathbb{E}_{X}\left[\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2}\right]
\end{align}
We can show that
\begin{align}
\dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right) & \begin{multlined}=\dfrac{\partial}{\partial \theta}\left(\dfrac{1}{f\left(X;\theta\right)}\cdot\dfrac{\partial}{\partial \theta}f\left(X;\theta\right)\right) \\
\text{\blue by the chain rule \black} \end{multlined} \\
& \begin{multlined}= -\dfrac{\dfrac{\partial}{\partial \theta}f\left(X;\theta\right)}{f\left(X;\theta\right)^{2}}\cdot\dfrac{\partial}{\partial \theta}f\left(X;\theta\right) + \dfrac{1}{f\left(X;\theta\right)}\cdot \dfrac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right) \\
\text{\blue by the product rule \black} \end{multlined} \\
&= -\left(\dfrac{\frac{\partial}{\partial \theta}f\left(X;\theta\right)}{f\left(X;\theta\right)}\right)^{2} + \dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} \\
& \begin{multlined}= \dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} - \left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2} \\
\text{\blue using }\blue\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right) = \dfrac{\frac{\partial}{\partial \theta}f\left(X;\theta\right)}{f\left(X;\theta\right)}\black
\end{multlined}
\end{align}
Hence
\begin{equation}
\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2} = \dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} - \dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)
\end{equation}
Additionally, we can show
\begin{align}
\mathbb{E}_{X}\left[\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)}\right] &= \int_{-\infty}^{\infty}\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(x;\theta\right)}{f\left(x;\theta\right)}\cdot f\left(x; \theta\right) dx \\
&= \int_{-\infty}^{\infty}\dfrac{\partial^{2}}{\partial \theta^{2}}f\left(x;\theta\right) dx \\
&= \dfrac{\partial^{2}}{\partial \theta^{2}}\left(\int_{-\infty}^{\infty}f\left(x;\theta\right) dx\right) \\
&= \dfrac{\partial^{2}}{\partial \theta^{2}}\left(1\right) \\
&= 0
\end{align}
Returning to the Fisher information
\begin{align}
\mathcal{I}\left(\theta\right) &= \mathbb{E}_{X}\left[\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2}\right] \\
&= \mathbb{E}_{X}\left[\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} - \dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)\right] \\
&= \green\cancelto{0}{\black \mathbb{E}_{X}\left[\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)}\right]}\black - \mathbb{E}_{X}\left[\dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)\right] \\
&= - \mathbb{E}_{X}\left[\dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)\right]
\end{align}
This shows that the Fisher information is equal to the negative of the curvature of the log likelihood, assuming the log likelihood is twice differentiable with respect to $\theta$. If $\theta$ is a vector, then
\begin{equation}
\mathcal{I}\left(\theta\right) = - \mathbb{E}_{X}\left[\nabla_{\theta}^{2}\log f\left(X;\theta\right)\right]
\end{equation}

\subsection{Fisher Information in Linear Regression}

In a linear regression model with $y_{i} = x_{i}^{\top}\beta + \sigma\varepsilon_{i}$ where $\varepsilon \sim \mathcal{N}\left(0, 1\right)$, the log likelihood of an observation $\left(x_{i}, y_{i}\right)$ is given by
\begin{align}
\log\mathcal{L}\left(\beta; x_{i}, y_{i}\right) &= \log\left[\dfrac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\dfrac{1}{2}\cdot\dfrac{\left(y_{i} - x_{i}^{\top}\beta\right)^{2}}{\sigma^{2}}\right)\right] \\
&= -\dfrac{1}{2}\log\left(2\pi\sigma^{2}\right) - \dfrac{1}{2}\cdot\dfrac{\left(y_{i} - x_{i}^{\top}\beta\right)^{2}}{\sigma^{2}}
\end{align}
The gradient (using the chain rule) is given by
\begin{align}
\nabla_{\beta}\log\mathcal{L}\left(\beta; x_{i}, y_{i}\right) &= -\dfrac{1}{2\sigma^{2}}\left[-\left(y_{i} - x_{i}^{\top}\beta\right)x_{i}\right] \\
&= \dfrac{\left(y_{i} - x_{i}^{\top}\beta\right)x_{i}}{2\sigma^{2}}
\end{align}
Which gives the Hessian
\begin{align}
\nabla_{\beta}^{2}\log\mathcal{L}\left(\beta; x_{i}, y_{i}\right) &= -\dfrac{2x_{i}x_{i}^{\top}}{2\sigma^{2}} \\
&= -\dfrac{x_{i}x_{i}^{\top}}{\sigma^{2}}
\end{align}
The Fisher information is the negative of the expected Hessian, so
\begin{equation}
\mathcal{I}\left(\beta\right) = \dfrac{1}{\sigma^{2}}\mathbb{E}\left[x_{i}x_{i}^{\top}\right]
\end{equation}
Suppose there are $N$ i.i.d. observations $\left(x_{i}, y_{i}\right)$, then the log likelihood is simply the sum of individual log likelihoods, and so the Fisher information is also just the sum:
\begin{equation}
\mathcal{I}\left(\beta\right) = \dfrac{1}{\sigma^{2}}\mathbb{E}\left[\sum_{i = 1}^{N}x_{i}x_{i}^{\top}\right]
\end{equation}
We can also define $X$ as a (tall) matrix given by $X := \begin{bmatrix} x_{1} & \dots & x_{N}\end{bmatrix}^{\top}$, then this lets us write the Fisher information as
\begin{align}
\mathcal{I}\left(\beta\right) &= \dfrac{1}{\sigma^{2}}\mathbb{E}\left[\begin{bmatrix} x_{1} & \dots & x_{N}\end{bmatrix}\begin{bmatrix} x_{1}^{\top} \\ \vdots \\ x_{N}^{\top}\end{bmatrix}\right] \\
&= \dfrac{\mathbb{E}\left[X^{\top}X\right]}{\sigma^{2}}
\end{align}
Note that if $X$ is a `design matrix', that is the factors in $X$ can be freely chosen, then the expectation is redundant and the Fisher information (also known as simply the `information matrix' \cite{Atkinson2007}) is
\begin{equation}
\mathcal{I}\left(\beta\right) = \dfrac{X^{\top}X}{\sigma^{2}}
\end{equation}

\subsection{Cramer-Rao Bound}

Let $\hat{\theta}$ be an unbiased estimator for the scalar parameter $\theta$.  Let $V$ be the score of the log likelihood $\log f\left(X;\theta\right)$. Then the covariance between $V$ and $\hat{\theta}$ satisfies
\begin{equation}
\operatorname{Cov}\left(V, \hat{\theta}\right) = \mathbb{E}\left[V\hat{\theta}\right]
\end{equation}
since we know the expected value of the score $\mathbb{E}\left[V\right] = 0$. Then
\begin{align}
\mathbb{E}\left[V\hat{\theta}\right] &= \int_{\mathcal{X}}f\left(x;\theta\right)V\hat{\theta}dx \\
&= \int_{\mathcal{X}}f\left(x;\theta\right)\dfrac{\frac{\partial f\left(x;\theta\right)}{\partial \theta}}{f\left(x;\theta\right)}\hat{\theta}dx \\
&= \int_{\mathcal{X}}\dfrac{\partial f\left(x;\theta\right)}{\partial \theta}\hat{\theta}dx \\
&= \dfrac{\partial }{\partial \theta}\int_{\mathcal{X}}f\left(x;\theta\right)\hat{\theta}dx \\
&= \dfrac{\partial }{\partial \theta}\mathbb{E}\left[\hat{\theta}\right] \\
&= \dfrac{\partial }{\partial \theta}\theta \\
&= 1
\end{align}
Hence $\operatorname{Cov}\left(V, \hat{\theta}\right) = \mathbb{E}\left[V\hat{\theta}\right] = 1$. Then from $\operatorname{Corr}\left(V, \hat{\theta}\right) \leq 1$:
\begin{gather}
\operatorname{Cov}\left(V, \hat{\theta}\right) \leq \sqrt{\operatorname{Var}\left(V\right)}\sqrt{\operatorname{Var}\left(\hat{\theta}\right)} \\
1 \leq \sqrt{\operatorname{Var}\left(V\right)}\sqrt{\operatorname{Var}\left(\hat{\theta}\right)} \\
\operatorname{Var}\left(V\right)\operatorname{Var}\left(\hat{\theta}\right) \geq 1
\end{gather}
Then from the fact that the Fisher information $\mathcal{I}\left(\theta\right)$ is the variance of the score, this gives the Cramer-Rao bound:
\begin{equation}
\operatorname{Var}\left(\hat{\theta}\right) \geq \dfrac{1}{\mathcal{I}\left(\theta\right)}
\end{equation}
In the case $\theta$ is a vector, then $\operatorname{Cov}\left(\hat{\theta}\right)$ is a covariance matrix and the Cramer-Rao bound becomes
\begin{equation}
\operatorname{Cov}\left(\hat{\theta}\right) \succeq \mathcal{I}\left(\theta\right)^{-1}
\end{equation}

\subsection{Efficient Estimators}

An estimator that achieves the Cramer-Rao bound with equality, ie.
\begin{equation}
\operatorname{Cov}\left(\hat{\theta}\right) = \mathcal{I}\left(\theta\right)^{-1}
\end{equation}
is said to be efficient. This implies that it is an unbiased estimator with the `least variance'. The maximum likelihood estimator can be shown to be asymptotically efficient (ie. as data size $n \to \infty$).

\section{James-Stein Estimation}

\subsection{Stein's Lemma}

\subsection{Stein's Example}

\subsection{James-Stein Estimator}

\section{Generalised Linear Models}

\subsection{Poisson Regression}

\subsection{Logistic Regression}

When the response variable $Y$ can take on a $0$ or $1$ (ie. success of fail), the probability that the response equals $1$ can be modelled using logistic regression. For a parameter vector $\beta$ and predictors $x$, this probability takes the form
\begin{equation}
\operatorname{Pr}\left(Y = 1\middle| X = x\right) = \dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}
\end{equation}
An alternative form is
\begin{equation}
\operatorname{Pr}\left(Y = 1\middle| X = x\right) = \dfrac{\exp\left(\beta^{\top}x\right)}{\exp\left(\beta^{\top}x\right) + 1}
\end{equation}
Notice that this probability will be bounded between $0$ and $1$. Denoting $p\left(x\right) := \operatorname{Pr}\left(Y = 1\middle| X = x\right)$, we can rearrange for a `linear form', given as
\begin{equation}
\ln\left(\dfrac{p\left(x\right)}{1 - p\left(x\right)}\right) = \beta^{\top}x
\end{equation}
To fit a logistic regression to some data, we first construct the likelihood (assuming independent samples) as
\begin{align}
L\left(\beta\middle|x\right) &= \operatorname{Pr}\left(Y\middle|X = x, \beta\right) \\
&= \prod_{i = 1}^{n} \operatorname{Pr}\left(y_{i}\middle|X = x_{i}, \beta\right) \\
&= \prod_{i = 1}^{n} \left(\dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right)^{y_{i}}\left(1 - \dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right)^{1 - y_{i}}
\end{align}
Note that since $y_{i}$ can only be $0$ or $1$, each multiplicand `selects' either the probability of occurrence of $\operatorname{Pr}\left(Y = 1\middle|X = x, \beta\right)$ if $y_{i} = 1$ or $\operatorname{Pr}\left(Y = 0\middle|X = x, \beta\right)$ if $y_{i} = 0$. The log-likelihood is
\begin{equation}
\log L\left(\beta\middle|x\right) = \sum_{i = 1}^{n} y_{i}\log\left(\dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right) + \sum_{i = 1}^{n}\left(1 - y_{i}\right)\log\left(1 - \dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right)
\end{equation}
Hence the maximum-likelihood estimate is the solution to
\begin{equation}
\min_{\beta} \left\{-\log L\left(\beta\middle|x\right)\right\}
\end{equation}

\subsection{Probit Regression}

\subsection{Multinomial Logistic Regression}

\section{Statistical Distance}

\subsection{Distance Measures}

\subsubsection{Mahalanobis Distance}

\subsubsection{Bhattacharyya Distance}

\subsection{Tests for Normality}

\chapter{Advanced Probability}

\section{Moments}

The $n$\textsuperscript{th} moment of a random variable $X$ is defined as $\mathbb{E}\left[X^{n}\right]$. Thus the 1st moment is simply the expected value while the 2nd moment of a zero-mean random variable is the same as its variance. The integral for the $n$\textsuperscript{th} moment of a continuous random variable $X$ with probability density function $f_{X}\left(x\right)$ is given by
\begin{equation}
\mathbb{E}\left[X^{n}\right] = \int_{-\infty}^{\infty}x^{n}f_{X}\left(x\right)dx
\end{equation}
If $X$ is instead a discrete random variable with probability mass function $\operatorname{Pr}\left(X = x\right)$, then
\begin{equation}
\mathbb{E}\left[X^{n}\right] = \sum_{x = -\infty}^{\infty}x^{n}\operatorname{Pr}\left(X = x\right)
\end{equation}

\subsection{Central Moments}

The $n$\textsuperscript{th} central moment of a random variable $X$ is defined as $\mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)^{n}\right]$. It is equivalent to the $n$\textsuperscript{th} moment of a random variable defined as $Y := X - \mu$ where $\mu := \mathbb{E}\left[X\right]$. Thus for a continuous random variable,
\begin{equation}
\mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)^{n}\right] = \int_{-\infty}^{\infty}\left(x - \mu\right)^{n}f_{X}\left(x\right)dx
\end{equation}
Note that the second central moment is identical to the definition of the variance.

\subsection{Standardised Moments}

The $n$\textsuperscript{th} standardised moment of a random variable $X$ is defined as the $n$\textsuperscript{th} central moment of $X$ divided by the $n$\textsuperscript{th} power of the standard deviation $\sigma$ of $X$. That is,
\begin{align}
\dfrac{\mathbb{E}\left[\left(X - \mu\right)^{n}\right]}{\sigma^{n}} &= \dfrac{\mathbb{E}\left[\left(X - \mu\right)^{n}\right]}{\left(\sqrt{\mathbb{E}\left[\left(X - \mu\right)^{2}\right]}\right)^{n}} \\
&= \dfrac{\mathbb{E}\left[\left(X - \mu\right)^{n}\right]}{\mathbb{E}\left[\left(X - \mu\right)^{2}\right]^{n/2}}
\end{align}
By definition, the second standardised moment is $1$. The third standardised moment is used to define the skewness of $X$, while the fourth standardised moment is used to define the kurtosis of $X$.

\subsection{Moment Generating Functions}

The moment generating function of a random variable $X$ is defined as
\begin{equation}
\phi_{X}\left(s\right) = \mathbb{E}\left[e^{sX}\right]
\end{equation}
That is, it is the expectation of the random variable $e^{sX}$ as a function of $s$. If $X$ has a probability density function $f_{X}\left(x\right)$, then
\begin{equation}
\phi_{X}\left(s\right) = \int_{-\infty}^{\infty}e^{sx}f_{X}\left(x\right)dx
\end{equation}
Notice that
\begin{equation}
\phi_{X}\left(-s\right) = \int_{-\infty}^{\infty}e^{-sx}f_{X}\left(x\right)dx
\end{equation}
which is the Laplace transform of $f_{X}\left(x\right)$. \\

The moment generating function can be used to compute the $n$\textsuperscript{th} moment of a random variable. The random variable $X$ with  MGF $\phi_{X}\left(s\right)$ has $n$\textsuperscript{th} moment
\begin{equation}
\mathbb{E}\left[X^{n}\right] = \left.\dfrac{d^{n}\phi_{X}\left(s\right)}{ds^{n}}\right|_{s = 0}
\end{equation}
That is, we take the $n$\textsuperscript{th} derivative of the MGF and evaluate it at $s = 0$.
\begin{proof}
Assuming $X$ is a continuous random variable, the $n$\textsuperscript{th} derivative of the MGF is
\begin{align}
\dfrac{d^{n}\phi_{X}\left(s\right)}{ds^{n}} &= \dfrac{d^{n}}{ds^{n}}\int_{-\infty}^{\infty}e^{sx}f_{X}\left(x\right)dx \\
&= \int_{-\infty}^{\infty}\dfrac{d^{n}}{ds^{n}}e^{sx}f_{X}\left(x\right)dx \\
&= \int_{-\infty}^{\infty}x^{n}e^{sx}f_{X}\left(x\right)dx
\end{align}
Evaluating at $s = 0$ gives
\begin{align}
\left.\dfrac{d^{n}\phi_{X}\left(s\right)}{ds^{n}}\right|_{s = 0} &= \int_{-\infty}^{\infty}x^{n}f_{X}\left(x\right)dx \\
&= \mathbb{E}\left[X^{n}\right]
\end{align}
This can be analogously proved for a discrete random variable in the same manner.
\end{proof}
Note that a random variable is not always guaranteed to have a moment generating function (ie. when the moments do not exist, such as for a Cauchy random variable).

\subsection{Sums of Random Variables with Moment Generating Functions}

Let $X = X_{1} + \dots  + X_{n}$ be a sum of independent random variables. Then
\begin{align}
\phi_{X}\left(s\right) &= \mathbb{E}\left[e^{s\left(X_{1} + \dots + X_{n}\right)}\right] \\
&= \mathbb{E}\left[e^{sX_{1}}\times\dots\times e^{sX_{n}}\right]
\end{align}
By independence,
\begin{align}
\phi_{X}\left(t\right) &= \mathbb{E}\left[e^{sX_{1}}\right]\times\dots\times\mathbb{E}\left[e^{sX_{n}}\right] \\
&= \phi_{X_{1}}\left(s\right)\times\dots\times\phi_{X_{n}}\left(s\right)
\end{align}
Hence the moment generating function of the sum of independent random variables is just the product of the moment generating functions.

\subsubsection{Random Sums with Moment Generating Functions}

Given an positive integer valued random variable $N$ and a sequence of i.i.d. random variables $X_{1}, X_{2}, \dots$, let the random sum $R$ be defined by
\begin{equation}
R = X_{1} +  X_{2} + \dots + X_{N}
\end{equation}
\begin{theorem}
If each term $X$ in a random sum has moment generating function $\phi_{X}\left(s\right)$ and the number of terms $N$ has moment generating function $\phi_{N}\left(s\right)$ which is independent of all the terms, then the random sum $R$ has moment generating function
\begin{equation}
\phi_{R}\left(s\right) = \phi_{N}\left(\ln \phi_{X}\left(s\right) \right)
\end{equation}
\end{theorem}
\begin{proof}
The MGF of $R$ is given by $\phi_{R}\left(s\right) = \mathbb{E}\left[e^{sR}\right]$. Using the law of iterated expectations, this is then
\begin{equation}
\phi_{R}\left(s\right) = \sum_{n = 0}^{\infty}\mathbb{E}\left[e^{s\left(X_{1} + \dots + X_{n}\right)}\middle|N = n\right]\operatorname{Pr}\left(N = n\right)
\end{equation}
By independence of $N$ from $X_{1}, \dots, X_{N}$, 
\begin{align}
\phi_{R}\left(s\right) &= \sum_{n = 0}^{\infty}\mathbb{E}\left[e^{s\left(X_{1} + \dots + X_{n}\right)}\right]\operatorname{Pr}\left(N = n\right) \\
&= \sum_{n = 0}^{\infty}\mathbb{E}\left[e^{sX_{1}}\right]\times\dots\times\mathbb{E}\left[e^{sX_{n}}\right]\operatorname{Pr}\left(N = n\right) \\
&= \sum_{n = 0}^{\infty}\left[\phi_{X}\left(x\right)\right]^{n}\operatorname{Pr}\left(N = n\right)
\end{align}
Note that we can write $\left[\phi_{X}\left(x\right)\right]^{n} = \left[\exp\left(\ln\phi_{X}\left(x\right)\right)\right]^{n} = \exp\left(\ln\phi_{X}\left(s\right)\cdot n\right)$. Hence
\begin{equation}
\phi_{R}\left(s\right) = \sum_{n = 0}^{\infty}\exp\left(\ln\phi_{X}\left(s\right)\cdot n\right)\operatorname{Pr}\left(N = n\right)
\end{equation}
This is just the MGF of $N$ evaluated at $\ln\phi_{X}\left(s\right)$. Therefore
\begin{equation}
\phi_{R}\left(s\right) = \phi_{N}\left(\ln \phi_{X}\left(s\right) \right)
\end{equation}
\end{proof}

\begin{theorem}
For the random sum of i.i.d. random variables $R = X_{1} + \dots + X_{N}$, we have
\begin{gather}
\mathbb{E}\left[R\right] = \mathbb{E}\left[X\right]\mathbb{E}\left[N\right]
\operatorname{Var}\left(R\right) = \mathbb{E}\left[N\right]\operatorname{Var}\left(X\right) + \operatorname{Var}\left(N\right)\mathbb{E}\left[X\right]^{2}
\end{gather}
\end{theorem}
\begin{proof}
The MGF of $R$ is $\phi_{R}\left(s\right) = \phi_{N}\left(\ln \phi_{X}\left(s\right) \right)$. By the moment generating property of the MGF, $\mathbb{E}\left[R\right]$ is given by the first derivative of $\phi_{R}\left(s\right)$ evaluated at $s = 0$. By applying the chain rule, this yields
\begin{align}
\phi_{R}'\left(s\right) &= \phi_{N}'\left(\ln\phi_{X}\left(s\right)\right)\dfrac{d}{ds}\ln\phi_{X}\left(s\right) \\
&= \phi_{N}'\left(\ln\phi_{X}\left(s\right)\right)\dfrac{\phi_{X}'\left(s\right)}{\phi_{X}\left(s\right)}
\end{align}
Since $\phi_{X}\left(0\right) = \mathbb{E}\left[e^{0}\right] = 1$ and $\phi_{N}'\left(0\right) = \mathbb{E}\left[N\right]$ and $\phi_{X}'\left(0\right) = \mathbb{E}\left[X\right]$, then
\begin{align}
\mathbb{E}\left[R\right] &= \phi_{N}'\left(0\right)\phi_{X}'\left(0\right) \\
&= \mathbb{E}\left[X\right]\mathbb{E}\left[N\right]
\end{align}
To find $\operatorname{Var}\left(R\right)$, we first need to find the second moment $\mathbb{E}\left[R^{2}\right]$. The second derivative of $\phi_{R}\left(s\right)$ using the product and quotient rules is
\begin{align}
\phi_{R}''\left(s\right) &= \phi_{N}''\left(\ln\phi_{X}\left(s\right)\right)\left(\dfrac{\phi_{X}'\left(s\right)}{\phi_{X}\left(s\right)}\right)^{2} + \phi_{N}'\left(\ln\phi_{X}\left(s\right)\right)\dfrac{d}{ds}\left(\dfrac{\phi_{X}'\left(s\right)}{\phi_{X}\left(s\right)}\right) \\
&= \phi_{N}''\left(\ln\phi_{X}\left(s\right)\right)\left(\dfrac{\phi_{X}'\left(s\right)}{\phi_{X}\left(s\right)}\right)^{2}+\phi_{N}'\left(\ln\phi_{X}\left(s\right)\right)\dfrac{\phi_{X}\left(s\right)\phi_{X}''\left(s\right)-\phi_{X}'\left(s\right)^{2}}{\phi_{X}\left(s\right)^{2}}
\end{align}
Evaluating this at $s = 0$ gives
\begin{align}
\mathbb{E}\left[R^{2}\right] &= \phi_{N}''\left(0\right)\left(\dfrac{\phi_{X}'\left(0\right)}{1}\right)^{2}+\phi_{N}'\left(0\right)\dfrac{\phi_{X}''\left(0\right)-\phi_{X}'\left(0\right)^{2}}{1} \\
&= \mathbb{E}\left[N^{2}\right]\mathbb{E}\left[X\right]^{2}+\mathbb{E}\left[N\right]\left(\mathbb{E}\left[X^{2}\right]-\mathbb{E}\left[X\right]^{2}\right) \\
&= \mathbb{E}\left[N^{2}\right]\mathbb{E}\left[X\right]^{2}+\mathbb{E}\left[N\right]\operatorname{Var}\left(X\right)
\end{align}
Then to get $\operatorname{Var}\left(R\right)$, subtract $\mathbb{E}\left[R\right]^{2} = \mathbb{E}\left[N\right]^{2}\mathbb{E}\left[X\right]^{2}$ from $\mathbb{E}\left[R^{2}\right]$.
\begin{align}
\operatorname{Var}\left(R\right) &= \mathbb{E}\left[R^{2}\right] - \mathbb{E}\left[R\right]^{2} \\
&= \mathbb{E}\left[N^{2}\right]\mathbb{E}\left[X\right]^{2}+\mathbb{E}\left[N\right]\operatorname{Var}\left(X\right) - \mathbb{E}\left[N\right]^{2}\mathbb{E}\left[X\right]^{2} \\
&= \mathbb{E}\left[N\right]\operatorname{Var}\left(X\right) + \left(\mathbb{E}\left[N^{2}\right] - \mathbb{E}\left[N\right]^{2}\right)\mathbb{E}\left[X\right]^{2} \\
&= \mathbb{E}\left[N\right]\operatorname{Var}\left(X\right) + \operatorname{Var}\left(N\right)\mathbb{E}\left[X\right]^{2}
\end{align}
\end{proof}

\subsection{Chernoff Bound}

Let $X$ be a random variable and let $\phi_{X}\left(s\right)$ be the moment generating function of $X$. Then for all $s\geq 0$, 
\begin{equation}
\operatorname{Pr}\left(X \geq a\right) \leq e^{-sa}\phi_{X}\left(s\right)
\end{equation}
which also means that 
\begin{equation}
\operatorname{Pr}\left(X \geq a\right) \leq \min_{s\geq 0}\left\{e^{-sa}\phi_{X}\left(s\right)\right\}
\end{equation}
\begin{proof}
From the Markov inequality, we have for some random variable $Y$ and any $b > 0$, 
\begin{equation}
\operatorname{Pr}\left(Y \geq b\right) \leq \dfrac{\mathbb{E}\left[Y\right]}{b}
\end{equation}
Let $b = e^{sa} > 0$ for any $a\in\mathbb{R}$, and let $Y = e^{sX}$. Hence
\begin{equation}
\operatorname{Pr}\left(e^{sX} \geq e^{sa}\right) \leq \dfrac{\mathbb{E}\left[e^{sX}\right]}{e^{sa}}
\end{equation}
Since $s\geq 0$, then $e^{sX} \geq e^{sa}$ is equivalent to $X \geq a$. Also, $\mathbb{E}\left[e^{sX}\right]$ is the definition of the moment generating function of $X$. Therefore
\begin{equation}
\operatorname{Pr}\left(X \geq a\right) \leq e^{-sa}\phi_{X}\left(s\right)
\end{equation}
\end{proof}

\subsection{Hoeffding's Lemma}

Hoeffding's lemma bounds the moment generating function $\mathbb{E}\left[e^{\lambda X}\right]$ of a bounded random variable $X$.
\begin{lemma}
Let $X$ be a random variable with expectation $\mathbb{E}\left[X\right] = 0$ and is bounded by $a \leq X \leq b$ almost surely. Then for all $\lambda \in \mathbb{R}$,
\begin{equation}
\mathbb{E}\left[e^{\lambda X}\right] \leq \exp\left[\dfrac{\lambda^{2}\left(b - a\right)^{2}}{8}\right]
\end{equation}
\end{lemma}
\begin{proof}
Since $e^{\lambda X}$ is convex in $X$, taking a convex combination of $e^{\lambda a}$ and $e^{\lambda b}$ gives for all $a \leq X \leq b$:
\begin{equation}
e^{\lambda X} \leq \dfrac{b - X}{b - a}e^{\lambda a} + \dfrac{X - a}{b - a}e^{\lambda b}
\end{equation}
Taking the expectation of both sides:
\begin{equation}
\mathbb{E}\left[e^{\lambda X}\right] \leq \dfrac{b - \mathbb{E}\left[X\right]}{b - a}e^{\lambda a} + \dfrac{\mathbb{E}\left[X\right] - a}{b - a}e^{\lambda b}
\end{equation}
Since $\mathbb{E}\left[X\right] = 0$, then
\begin{equation}
\mathbb{E}\left[e^{\lambda X}\right] \leq \dfrac{b}{b - a}e^{\lambda a} - \dfrac{a}{b - a}e^{\lambda b}
\end{equation}
Now by letting $h = \lambda\left(b - a\right)$, $p = \dfrac{-a}{b - a}$ and $L\left(h\right) = -hp + \ln\left(1 - p + pe^{h}\right)$, we can show that $\dfrac{b}{b - a}e^{\lambda a} - \dfrac{a}{b - a}e^{\lambda b} = e^{L\left(h\right)}$ as follows:
\begin{align}
e^{L\left(h\right)} &= e^{-hp + \ln\left(1 - p + pe^{h}\right)} \\
&= e^{-hp}e^{\ln\left(1 - p + pe^{h}\right)} \\
&= \left(1 - p + pe^{h}\right)e^{-hp} \\
&= \left(1 + \dfrac{a}{b - a} - \dfrac{a}{b - a}e^{\lambda\left(b - a\right)}\right)\exp\left[-\lambda\left(b - a\right)\dfrac{-a}{b - a}\right] \\
&= \left(\dfrac{b}{b - a} - \dfrac{a}{b - a}e^{\lambda\left(b - a\right)}\right)e^{\lambda a} \\
&= \dfrac{b}{b - a}e^{\lambda a} - \dfrac{a}{b - a}e^{\lambda b}
\end{align}
Differentiating $L\left(h\right)$, we find
\begin{equation}
L'\left(h\right) = -p + \dfrac{pe^{h}}{1 - p + pe^{h}}
\end{equation}
and by the product rule
\begin{align}
L''\left(h\right) &= \dfrac{pe^{h}}{1 - p + pe^{h}} + \dfrac{pe^{h}\left(pe^{h}\right)}{-\left(1 - p + pe^{h}\right)^{2}} \\
&= \dfrac{pe^{h}}{1 - p + pe^{h}}\left(1 - \dfrac{pe^{h}}{1 - p + pe^{h}} \right)
\end{align}
Note that $L\left(0\right) = 0$ and $L'\left(0\right) = 0$. Letting $t = \dfrac{pe^{h}}{1 - p + pe^{h}}$, we also see that
\begin{align}
L''\left(h\right) &= t\left(1 - t\right) \\
&\leq \dfrac{1}{4}
\end{align}
which can be deduced by considering the graph of the parabola $t\left(1 - t\right)$. By applying Taylor's theorem about zero, we have for some $\theta\in\left(0, 1\right)$:
\begin{align}
L\left(h\right) &= L\left(0\right) + hL'\left(0\right) + \dfrac{1}{2}h^{2}L''\left(\theta h\right) \\
&= \dfrac{1}{2}h^{2}L''\left(\theta h\right) \\
&\leq \dfrac{1}{8}\lambda^{2}\left(b - a\right)^{2}
\end{align}
Therefore
\begin{align}
\mathbb{E}\left[e^{\lambda X}\right] &\leq \dfrac{b}{b - a}e^{\lambda a} - \dfrac{a}{b - a}e^{\lambda b} \\
&\leq e^{L\left(h\right)} \\
&\leq \exp\left[\dfrac{\lambda^{2}\left(b - a\right)^{2}}{8}\right]
\end{align}
\end{proof}

\subsection{Hoeffding's Inequality}

Hoeffding's lemma can be used to derive Hoeffding's inequality for a sum of bounded independent random variables (they need not necessarily be identical).
\begin{theorem}
Let $X_{1}, \dots, X_{n}$ be independent random variables bounded by $a_{i} \leq X_{i} \leq b_{i}$ almost surely for all $i = 1, \dots, n$. Let $S = \sum_{i = 1}^{n}X_{i}$. Then for all $t > 0$, 
\begin{equation}
\operatorname{Pr}\left(\left|S - \mathbb{E}\left[S\right]\right| \geq t\right) \leq 2\exp\left[-\dfrac{2t^{2}}{\sum_{i = 1}^{n}\left(b_{i} - a_{i}\right)^{2}} \right]
\end{equation}
\end{theorem}
\begin{proof}
For any random variable $Z$ and $s > 0$,
\begin{equation}
\operatorname{Pr}\left(Z \geq t\right) = \operatorname{Pr}\left(e^{sZ} \geq e^{st}\right)
\end{equation}
since $e^{sz}$ is monotonically increasing in $z$. Then apply Markov's inequality to the non-negative random variable $e^{sZ}$:
\begin{align}
\operatorname{Pr}\left(e^{sZ} \geq e^{st}\right) &\leq \dfrac{\mathbb{E}\left[e^{sZ}\right]}{e^{st}} \\
&\leq e^{-st}\mathbb{E}\left[e^{sZ}\right]
\end{align}
Choose $Z = S - \mathbb{E}\left[S\right]$, so
\begin{align}
\operatorname{Pr}\left(\sum_{i = 1}^{n}\left(X_{i} - \mathbb{E}\left[X_{i}\right]\right) \geq t\right) &\leq e^{-st}\mathbb{E}\left[\exp\left[s\sum_{i = 1}^{n}\left(X_{i} - \mathbb{E}\left[X_{i}\right]\right)\right]\right]	 \\
&\leq e^{-st}\mathbb{E}\left[\prod_{i = 1}^{n}\exp\left[s\left(X_{i} - \mathbb{E}\left[X_{i}\right]\right)\right]\right] \\
&\leq e^{-st}\prod_{i = 1}^{n}\mathbb{E}\left[\exp\left[s\left(X_{i} - \mathbb{E}\left[X_{i}\right]\right)\right]\right]
\end{align}
where the latter is due to independence of all the $X_{i}$. Note that $\mathbb{E}\left[\exp\left[s\left(X_{i} - \mathbb{E}\left[X_{i}\right]\right)\right]\right]$ is the moment generating function of $X_{i} - \mathbb{E}\left[X_{i}\right]$. Hence applying Hoeffding's lemma,
\begin{align}
\operatorname{Pr}\left(\sum_{i = 1}^{n}\left(X_{i} - \mathbb{E}\left[X_{i}\right]\right) \geq t\right) &\leq e^{-st}\prod_{i = 1}^{n}\exp\left[\dfrac{s^{2}\left(b_{i} - a_{i}\right)^{2}}{8}\right] \\
&\leq \exp\left[-st + s^{2}\sum_{i = 1}^{n}\dfrac{\left(b_{i} - a_{i}\right)^{2}}{8}\right]
\end{align}
We can choose the value of $s > 0$ which gives the best bound. This amounts to minimising the term $-st + s^{2}\sum_{i = 1}^{n}\dfrac{\left(b_{i} - a_{i}\right)^{2}}{8}$, which is a quadratic in $s$ with coefficents $\sum_{i = 1}^{n}\dfrac{\left(b_{i} - a_{i}\right)^{2}}{8}$ and $-t$. Hence the choice of
\begin{equation}
s = \dfrac{4t}{\sum_{i = 1}^{n}\left(b_{i} - a_{i}\right)^{2}}
\end{equation}
minimises the upper bound. Substituting this value of $s$ gives
\begin{align}
\operatorname{Pr}\left(S - \mathbb{E}\left[S\right] \geq t\right)  &= \operatorname{Pr}\left(\sum_{i = 1}^{n}\left(X_{i} - \mathbb{E}\left[X_{i}\right]\right) \geq e^{st}\right) \\
&\leq \exp\left[-\dfrac{4t^{2}}{\sum_{i = 1}^{n}\left(b_{i} - a_{i}\right)^{2}} + \dfrac{16t^{2}}{\left[\sum_{i = 1}^{n}\left(b_{i} - a_{i}\right)^{2}\right]^{2}}\sum_{i = 1}^{n}\dfrac{\left(b_{i} - a_{i}\right)^{2}}{8}\right] \\
&\leq \exp\left[-\dfrac{2t^{2}}{\sum_{i = 1}^{n}\left(b_{i} - a_{i}\right)^{2}} \right]
\end{align}
Instead if we chose $Z = -S + \mathbb{E}\left[S\right]$, we get
\begin{align}
\operatorname{Pr}\left(-S + \mathbb{E}\left[S\right] \geq t\right) &= \operatorname{Pr}\left(S - \mathbb{E}\left[S\right] \leq -t\right) \\
&\leq \exp\left[-\dfrac{2t^{2}}{\sum_{i = 1}^{n}\left(b_{i} - a_{i}\right)^{2}} \right]
\end{align}
therefore combining these two inequalities using Boole's inequality gives
\begin{equation}
\operatorname{Pr}\left(\left|S - \mathbb{E}\left[S\right]\right| \geq t\right) \leq 2\exp\left[-\dfrac{2t^{2}}{\sum_{i = 1}^{n}\left(b_{i} - a_{i}\right)^{2}} \right]
\end{equation}
\end{proof}

\section{Probability Generating Functions}

The probability generating function of a discrete random variable $X$ taking on values in the non-negative integers with probability mass function $\operatorname{Pr}\left(X = x\right)$ is defined by
\begin{align}
\psi_{X}\left(z\right) &= \mathbb{E}\left[z^{X}\right] \\
&= \sum_{x = 0}^{\infty}z^{x}\operatorname{Pr}\left(X = x\right)
\end{align}
which is the $z$-transform of the probability mass function.

\begin{theorem}
Let $X = X_{1} + \dots + X_{n}$ be a sum of independent non-negative random variables. Then
\begin{equation}
\psi_{X}\left(z\right) = \psi_{X_{1}}\left(z\right)\times\dots\times \psi_{X_{n}}\left(z\right)
\end{equation} 
\end{theorem}
\begin{proof}
By definition,
\begin{align}
\psi_{X}\left(z\right) &= \mathbb{E}\left[z^{X_{1} + \dots + X_{n}}\right] \\
&= \mathbb{E}\left[z^{X_{1}}\times\dots\times z^{X_{n}}\right]
\end{align}
Using independence,
\begin{align}
\psi_{X}\left(z\right) &= \mathbb{E}\left[z^{X_{1}}\right]\times\dots\times \mathbb{E}\left[z^{X_{n}}\right] \\
&= \psi_{X_{1}}\left(z\right)\times\dots\times \psi_{X_{n}}\left(z\right)
\end{align}
\end{proof}

\section{Characteristic Functions}

The characteristic function of a random variable $X$ is a complex-valued function in a real-valued argument $t$, defined as
\begin{equation}
\varphi_{X}\left(t\right) = \mathbb{E}\left[e^{-itX}\right]
\end{equation}
where $i = \sqrt{-1}$. By Euler's formula, we can see that it is complex-valued.
\begin{equation}
\varphi_{X}\left(t\right) = \mathbb{E}\left[\cos\left(tX\right)\right] + i\mathbb{E}\left[\sin\left(tX\right)\right]
\end{equation}
If $X$ has a probability density function $f_{X}\left(x\right)$, then the characteristic function is
\begin{equation}
\varphi_{X}\left(t\right) = \int_{-\infty}^{\infty}e^{-itx}f_{X}\left(x\right)dx
\end{equation}
which is the Fourier transform of $f_{X}\left(x\right)$. Hence the characteristic function encodes the full distribution of $X$. The characteristic function of a random variable always exists. We can show this by considering a change of variables $x = Q\left(p\right)$ where $Q\left(p\right)$ is the quantile function. So
\begin{gather}
p = Q^{-1}\left(x\right) = F_{X}\left(x\right) \\
\dfrac{dp}{dx} = f_{X}\left(x\right) \\
dp = f_{X}\left(x\right)dx
\end{gather}
Hence
\begin{equation}
\int_{-\infty}^{\infty}e^{-itx}f_{X}\left(x\right)dx = \int_{0}^{1}e^{-itQ\left(p\right)}dp
\end{equation}
This is a proper integral, and since the magnitude of the integrand $\left|e^{-itQ\left(p\right)}\right|$ is bounded from looking at Euler's formula, this integral exists. \\

If the moment generating function $\phi_{X}\left(t\right)$ of $X$ exists, then the relationship between the characteristic function and the moment generating function is given by
\begin{gather}
\varphi_{X}\left(t\right) = \phi_{X}\left(-it\right) \\
\phi_{X}\left(t\right) = \varphi_{X}\left(-it\right)
\end{gather}

\subsection{Sums of Random Variables with Characteristic Functions}

Let $X = X_{1} + \dots  + X_{n}$ be a sum of independent random variables. Then
\begin{align}
\varphi_{X}\left(t\right) &= \mathbb{E}\left[e^{-it\left(X_{1} + \dots + X_{n}\right)}\right] \\
&= \mathbb{E}\left[e^{-itX_{1}}\times\dots\times e^{-itX_{n}}\right]
\end{align}
By independence,
\begin{align}
\varphi_{X}\left(t\right) &= \mathbb{E}\left[e^{-itX_{1}}\right]\times\dots\times\mathbb{E}\left[e^{-itX_{n}}\right] \\
&= \varphi_{X_{1}}\left(t\right)\times\dots\times\varphi_{X_{n}}\left(t\right)
\end{align}
Hence the characteristic function of the sum of independent random variables is just the product of the characteristic functions. This property is shared with the moment generating function.

\subsection{Subindependence}

Using characteristic functions, `subindependence' can be defined as a weaker version of independence and uncorrelatedness. Two random variables $X$ and $Y$ are said to be subindependent if their characteristic functions satisfy
\begin{equation}
\varphi_{X+Y}\left(t\right) = \varphi_{X}\left(t\right)\varphi_{Y}\left(t\right)
\end{equation}
Hence if $X$ and $Y$ are independent, then they are also subindependent. However if $X$ and $Y$ are subindpendent, they are not necessarily independent, nor even uncorrelated (because the covariance may not exist, for example in the case of Cauchy random variables). However if $X$ and $Y$ are subindependent and the covariance exists, then they are uncorrelated.

\subsection{Characteristic Functions of Gaussians}

The characteristic function of a Gaussian is another (unnormalised) Gaussian. We can show this as follows for a standard Gaussian random variable $X \sim \mathcal{N}\left(0, 1^{2}\right)$. The characteristic function is given by
\begin{equation}
\varphi_{X}\left(t\right) = \dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-itx}e^{-x^{2}/2}dx
\end{equation}
A slight (although not necessary) simplification is to write
\begin{align}
\varphi_{X}\left(t\right) &= \dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-x^{2}/2}\left(\cos\left(tx\right) - i\sin\left(tx\right)\right)dx \\
&= \dfrac{1}{\sqrt{2\pi}}\left(\int_{-\infty}^{\infty}e^{-x^{2}/2}\cos\left(tx\right)dx - \green\cancelto{0}{\black i\int_{-\infty}^{\infty}e^{-x^{2}/2}\sin\left(tx\right)dx}\black\right) \\
&= \dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-x^{2}/2}\cos\left(tx\right)dx
\end{align}
since $e^{-x^{2}/2}\sin\left(tx\right)$ is an odd function. We then apply the trick of differentiating $\varphi_{X}\left(t\right)$:
\begin{equation}
\varphi_{X}'\left(t\right) = \dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}-e^{-x^{2}/2}x\sin\left(tx\right)dx
\end{equation}
Let $v' = -xe^{-x^{2}/2}$ and $u = \sin\left(tx\right)$ so $v = e^{-x^{2}/2}$ and $u' = t\cos\left(tx\right)$. Differentiating by parts, this gives
\begin{align}
\varphi_{X}'\left(t\right) &= \dfrac{1}{\sqrt{2\pi}}\left(\left[uv\right]_{x = -\infty}^{x = \infty} - \int_{-\infty}^{\infty}u'vdx\right) \\
&= \dfrac{1}{\sqrt{2\pi}}\left(\green\cancelto{0}{\black\left[e^{-x^{2}/2}\sin\left(tx\right)\right]_{x = -\infty}^{x = \infty}}\black - \int_{-\infty}^{\infty}t\cos\left(tx\right)e^{-x^{2}/2}dx\right) \\
&=  \dfrac{-t}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-x^{2}/2}\cos\left(tx\right)dx \\
&= -t\varphi_{X}\left(t\right)
\end{align}
This is a differential equation with solution $\varphi_{X}\left(t\right) = e^{-t^{2}/2}$, which can be shown through direct verification.

\section{Cumulants}

\subsection{Cumulant Generating Functions \cite{Small2010}}

The cumulant generating function $K_{X}\left(t\right)$ of a random variable $X$ is defined as the natural logarithm of the moment generating function.
\begin{equation}
K_{X}\left(t\right) = \log\phi_{X}\left(t\right)
\end{equation}
By a Taylor series expansion about $t = 0$, the cumulant generating function can be written as
\begin{equation}
K_{X}\left(t\right) = \kappa_{0} + \kappa_{1}t + \kappa_{2}\dfrac{t^{2}}{2!} + \kappa_{3}\dfrac{t^{3}}{3!} + \dots
\end{equation}
The $n$\textsuperscript{th} cumulant of $X$ is then defined as the $n$\textsuperscript{th} coefficient of the series expansion for $K_{X}\left(t\right)$, which is $\kappa_{n}$. This is the same as taking the $n$\textsuperscript{th} derivative of $K\left(t\right)$ and evaluating at zero. Note that $\kappa_{0} = 0$ since $\phi_{X}\left(0\right) = 1$.

\begin{theorem}
Let $X = X_{1} + \dots  + X_{n}$ be a sum of independent random variables. Then
\begin{align}
K_{X}\left(t\right) = K_{X_{1}}\left(t\right) + \dots + K_{X_{n}}\left(t\right)
\end{align}
\end{theorem}
\begin{proof}
Using the definition of the cumulant generating function and the property of the moment generating function for the sum $X$,
\begin{align}
K_{X}\left(t\right) &= \log\phi_{X}\left(t\right) \\
&= \log\prod_{i = 1}^{n}\phi_{X_{i}}\left(t\right) \\
&= \sum_{i = 1}^{n}\log\phi_{X_{i}}\left(t\right) \\
&= \sum_{i = 1}^{n}K_{X_{i}}\left(t\right)
\end{align}
\end{proof}

\subsubsection{Relation Between Moments and Cumulants}

The relation between moments and cumulants can be analysed as follows. Denote the $n$\textsuperscript{th} moment of $X$ by $\mu_{n}$. A Taylor series expansion of the moment generating function can be written as
\begin{equation}
\phi_{X}\left(t\right) = 1 + \mu_{1}t + \mu_{2}\dfrac{t^{2}}{2!} + \mu_{3}\dfrac{t^{3}}{3!} + \dots
\end{equation}
then writing $\phi_{X}\left(t\right) = \exp K\left(t\right) $ gives
\begin{align}
1 + \mu_{1}t + \mu_{2}\dfrac{t^{2}}{2!} + \mu_{3}\dfrac{t^{3}}{3!} + \dots &= \exp\left(\kappa_{1}t + \kappa_{2}\dfrac{t^{2}}{2!} + \kappa_{3}\dfrac{t^{3}}{3!} + \dots\right) \\
&= \exp\left(\kappa_{1}t\right)\exp\left(\kappa_{2}\dfrac{t^{2}}{2!}\right)\exp\left(\kappa_{3}\dfrac{t^{3}}{3!}\right)\dots \\
&= \prod_{i = 1}^{n}\exp\left(\kappa_{i}\dfrac{t^{i}}{i!}\right)
\end{align}
Using the series expansion for $e^{x} = 1 + x + \dfrac{x^{2}}{2!} + \dfrac{x^{3}}{3!} + \dots$,
\begin{equation}
1 + \mu_{1}t + \mu_{2}\dfrac{t^{2}}{2!} + \mu_{3}\dfrac{t^{3}}{3!} + \dots = \prod_{i = 1}^{n}\left(1 + \kappa_{i}\dfrac{t^{i}}{i!} + \kappa_{i}^{2}\dfrac{t^{2i}}{2!\left(i!\right)^{2}} + \dots\right)
\end{equation}
Collecting powers of $t$ and equating, we can see that
\begin{align}
\mu_{1}t &= \kappa_{1}t \\
\mu_{2}\dfrac{t^{2}}{2!} &= \dfrac{\left(\kappa_{1}^{2} + \kappa_{2}\right)t^{2}}{2!} \\
\mu_{3}\dfrac{t^{3}}{3!} &= \dfrac{\kappa_{3}t^{3}}{3!} + \dfrac{\kappa_{1}^{3}t^{3}}{3!} + \kappa_{1}t\dfrac{\kappa_{2}t^{2}}{2!}
\end{align}
Hence
\begin{align}
\mu_{1} &= \kappa_{1} \\
\mu_{2} &= \kappa_{1}^{2} + \kappa_{2} \\
\mu_{3} &= \kappa_{3} + 3\kappa_{1}\kappa_{2} + \kappa_{1}^{3}
\end{align}
From this, we get $\kappa_{2} = \mu_{2} - \kappa_{1}^{2} = \mathbb{E}\left[X^{2}\right] - \mathbb{E}\left[X\right]^{2}$ which is the same as the variance of $X$. The third cumulant is given by
\begin{equation}
\kappa_{3} = \mu_{3} - 3\kappa_{1}\kappa_{2} - \kappa_{1}^{3}
\end{equation}
which is identical to the third central moment because
\begin{align}
\mathbb{E}\left[\left(X - \mu_{1}\right)^{3}\right] &= \mathbb{E}\left[X^{3} - 3X^{2}\mu_{1} + 3X\mu_{1}^{2} - \mu_{1}^{3}\right] \\
&= \mathbb{E}\left[X^{3}\right] - 3\mu_{1}\left(\mathbb{E}\left[X^{2}\right] - \mu_{1}^{2}\right) - \mu_{1}^{3} \\
&= \mathbb{E}\left[X^{3}\right] - 3\mu_{1}\operatorname{Var}\left(X\right) - \mu_{1}^{3} \\
&= \mu_{3} - 3\kappa_{1}\kappa_{2} - \kappa_{1}^{3}
\end{align}
However, $n$\textsuperscript{th} cumulants higher than the third are generally not equal to the $n$\textsuperscript{th} central moments.

\subsection{Law of Total Cumulance}

The Law of Total Cumulance generalises the Law of Iterated Expectations and the Law of Total Covariance.

\subsection{Edgeworth Series Expansions}

The (probabilists') Hermite polynomials are a polynomial sequence defined by
\begin{equation}
\operatorname{He}_{n}\left(x\right) = \left(-1\right)^{n}e^{x^{2}/2}\dfrac{d^{n}}{dx^{n}}e^{-x^{2}/2}
\end{equation}
The first few Hermite polynomials obtained via evaluation are as follows.
\begin{align}
\operatorname{He}_{0}\left(x\right) &= e^{x^{2}/2}e^{-x^{2}/2} \\
&= 1
\end{align}
\begin{align}
\operatorname{He}_{1}\left(x\right) &= -e^{x^{2}/2}\times -xe^{-x^{2}/2} \\
&= x
\end{align}
\begin{align}
\operatorname{He}_{2}\left(x\right) &= e^{x^{2}/2}\times -\dfrac{d}{dx}xe^{-x^{2}/2} \\
&= e^{x^{2}/2}\left(x^{2}e^{-x^{2}/2} - e^{-x^{2}/2}\right) \\
&= x^{2} - 1
\end{align}
A recurrence relation can also be obtained for the Hermite polynomials. Note that
\begin{align}
\operatorname{He}_{n - 1}'\left(x\right) &= \left(-1\right)^{n-1}\dfrac{d}{dx}\left[e^{x^{2}/2}\dfrac{d^{n-1}}{dx^{n-1}}e^{-x^{2}/2}\right] \\
&= \left(-1\right)^{n-1}xe^{x^{2}/2}\dfrac{d^{n-1}}{dx^{n-1}}e^{-x^{2}/2} + \left(-1\right)^{n-1}e^{x^{2}/2}\dfrac{d^{n}}{dx^{n}}e^{-x^{2}/2} \\
&= x\operatorname{He}_{n-1}\left(x\right) - \left(-1\right)^{n}e^{x^{2}/2}\dfrac{d^{n}}{dx^{n}}e^{-x^{2}/2} \\
&= x\operatorname{He}_{n-1}\left(x\right) - \operatorname{He}_{n}\left(x\right)
\end{align}
Hence
\begin{equation}
\operatorname{He}_{n + 1}\left(x\right) = x\operatorname{He}_{n}\left(x\right)
- \operatorname{He}_{n}\left(x\right)
\end{equation}

\section{Central Limit Theorem}

\subsection{Lindberg-Levy Central Limit Theorem \cite{Kallenberg1997}}

Let $X_{1}, X_{2}, \dots, X_{n}$ be random variables that are i.i.d. with $X$, where $\mathbb{E}\left[X\right] = \mu$ and $\operatorname{Var}\left(X\right) = \sigma^{2}$. Define the sum $S_{n} := X_{1} + \dots + X_{n}$. Then
\begin{equation}
\dfrac{S_{n} - n\mu}{\sigma\sqrt{n}} \overset{\mathrm{d}}{\to} Z \sim \mathcal{N}\left(0, 1^{2}\right)
\end{equation}
as $n\to\infty$. Equivalently, if we define the `normalised' sequence $\widetilde{X}_{n} := \dfrac{X_{n} - \mu}{\sigma}$, then
\begin{equation}
\dfrac{1}{\sqrt{n}}\left(\widetilde{X}_{1} + \dots + \widetilde{X}_{n}\right) \overset{\mathrm{d}}{\to} Z \sim \mathcal{N}\left(0, 1^{2}\right) 
\end{equation}
as $n\to\infty$. Note that
\begin{gather}
\mathbb{E}\left[\widetilde{X}\right] = 0 \\
\operatorname{Var}\left(\widetilde{X}\right) = \mathbb{E}\left[\widetilde{X}^{2}\right] = 1 \\
\mathbb{E}\left[\dfrac{\widetilde{X}}{\sqrt{n}}\right] = 0 \\
\operatorname{Var}\left(\dfrac{\widetilde{X}}{\sqrt{n}}\right) =  \mathbb{E}\left[\dfrac{\widetilde{X}^{2}}{n}\right] = \dfrac{1}{n}
\end{gather}
\begin{proof}
It suffices to show that the characteristic function of the sum $\dfrac{\widetilde{X}_{1}}{\sqrt{n}} + \dots + \dfrac{\widetilde{X}_{n}}{\sqrt{n}}$ converges to the characteristic function of the standard Gaussian distribution as $n\to\infty$. Firstly, an $n$\textsuperscript{th} order Taylor expansion of the characteristic function of an arbitrary random variable $Y$ about $t = 0$ is given by
\begin{equation}
\varphi_{Y}\left(t\right) = \sum_{k = 0}^{n}\dfrac{t^{k}\varphi_{Y}^{\left(k\right)}\left(0\right)}{k!} + o\left(t^{n}\right)
\end{equation}
where $\varphi_{Y}^{\left(k\right)}\left(t\right)$ denotes the $k$\textsuperscript{th} derivative of $\varphi_{Y}\left(t\right)$. Note that the Little-o notation means that the term $o\left(t^{n}\right)$ goes to zero faster than $t^{n}$ as $t\to 0$. Evaluating the $k$\textsuperscript{th} derivative gives
\begin{equation}
\varphi_{Y}^{\left(k\right)}\left(t\right) = \mathbb{E}\left[\left(-iY\right)^{k}e^{-itY}\right]
\end{equation}
and in particular $\varphi_{Y}^{\left(k\right)}\left(0\right) = \mathbb{E}\left[\left(-iY\right)^{k}\right]$. Hence
\begin{equation}
\varphi_{Y}\left(t\right) = \sum_{k = 0}^{n}\dfrac{\left(-it\right)^{k}\mathbb{E}\left[Y^{k}\right]}{k!} + o\left(t^{n}\right)
\end{equation}
and for the random variable $\widetilde{X}/\sqrt{n}$, the second order Taylor expansion is:
\begin{align}
\varphi_{\widetilde{X}/\sqrt{n}}\left(t\right) &= 1 + \dfrac{-it\mathbb{E}\left[\widetilde{X}/\sqrt{n}\right]}{1} - \dfrac{t^{2}\mathbb{E}\left[\widetilde{X}^{2}/n\right]}{2} + o\left(\dfrac{t^{2}}{n}\right) \\
&= 1 - \dfrac{t^{2}}{2n} + o\left(\dfrac{t^{2}}{n}\right)
\end{align}
Since the characteristic function of a sum of random variables is the product of the characteristic functions, we apply this and take $n\to\infty$.
\begin{align}
\lim_{n\to\infty}\left[\varphi_{\widetilde{X}/\sqrt{n}}\left(t\right)\right]^{n} &= \lim_{n\to\infty}\left[1 - \dfrac{t^{2}}{2n} + o\left(\dfrac{t^{2}}{n}\right)\right]^{n} \\
&= e^{-t^{2}/2}
\end{align}
which gives the characteristic function of the standard Gaussian distribution, where we have also used the fact $\lim_{n\to\infty}\left(1 + \dfrac{x}{n}\right)^{n} = e^{x}$.
\end{proof}
The Central Limit Theorem also implies several other equivalent statements, which can be obtained by shifting and scaling the random variables.
\begin{gather}
S_{n} - n\mu \overset{\mathrm{d}}{\to} \sigma\sqrt{n}Z \sim \mathcal{N}\left(0, \sigma^{2}n\right) \\
S_{n} \overset{\mathrm{d}}{\to} \sigma\sqrt{n}Z + n\mu \sim \mathcal{N}\left(n\mu, \sigma^{2}n\right) \\
\dfrac{S_{n}}{n} \overset{\mathrm{d}}{\to} \dfrac{\sigma\sqrt{n}Z + n\mu}{n}\sim \mathcal{N}\left(\mu, \dfrac{\sigma^{2}}{n}\right)
\end{gather}

\subsection{Lindberg-Feller Central Limit Theorem}

\section{Multivariate Gaussian Identities}

\subsection{Conditional Gaussian Densities}

A multivariate $D$-dimensional Gaussian distribution can be denoted by the density function
\begin{equation}
p\left(x;\mu,\Sigma\right)=\left(2\pi\right)^{-D/2}\left|\Sigma\right|^{-1/2}\exp\left(-\dfrac{1}{2}\left(x-\mu\right)^{\top}\Sigma^{-1}\left(x-\mu\right)\right)
\end{equation}
where $x$ is a random vector, and $\mu$ and $\Sigma$ are the mean and covariance respectively. Here, $\left|\cdot\right|$ denotes determinant. We can also denote a Gaussian using $x\sim\mathcal{N}\left(\mu, \Sigma\right)$. For a joint distribution between two Gaussian random vectors $x$ and $y$, we can denote it as
\begin{equation}
\begin{bmatrix}x\\
y
\end{bmatrix}\sim\mathcal{N}\left(\begin{bmatrix}\mu_{x}\\
\mu_{y}
\end{bmatrix},\begin{bmatrix}A & C\\
C^{\top} & B
\end{bmatrix}\right)=\mathcal{N}\left(\begin{bmatrix}\mu_{x}\\
\mu_{y}
\end{bmatrix},\begin{bmatrix}\tilde{A} & \tilde{C}\\
\tilde{C}^{\top} & \tilde{B}
\end{bmatrix}^{-1}\right)
\end{equation}
where $x\sim\mathcal{N}\left(\mu_{x},A\right)$, $y\sim\mathcal{N}\left(\mu_{y},B\right)$, and the inverse of the block partitioned matrix can be expressed using the block inversion formula:
\begin{equation}
\begin{bmatrix}\tilde{A} & \tilde{C}\\
\tilde{C}^{\top} & \tilde{B}
\end{bmatrix}=\begin{bmatrix}A^{-1}+A^{-1}C\left(B-C^{\top}A^{-1}C\right)^{-1}C^{\top}A^{-1} & -A^{-1}C\left(B-C^{\top}A^{-1}C\right)^{-1}\\
-\left(B-C^{\top}A^{-1}C\right)^{-1}C^{\top}A^{-1} & \left(B-C^{\top}A^{-1}C\right)^{-1}
\end{bmatrix}
\end{equation}
If we want the conditional distribution of $x$ on $y$, this is
\begin{equation}
\left.x\middle|y\right.\sim\mathcal{N}\left(\mu_{x}+CB^{-1}\left(y-\mu_{y}\right),A-CB^{-1}C^{\top}\right) =\mathcal{N}\left(\mu_{x}-\tilde{A}^{-1}\tilde{C}\left(y-\mu_{y}\right),\tilde{A}^{-1}\right)
\end{equation}

\subsection{Product of Gaussian Densities}
Suppose we take the product of two Gaussian distributions. Note that this is different from the distribution of two Gaussian random variables, which will not be Gaussian distributed. The product of two Gaussian distributions will be another (unormalised) Gaussian:
\begin{equation}
\mathcal{N}\left(x; a,A\right)\mathcal{N}\left(x;b,B\right)=Z^{-1}\mathcal{N}\left(x;c,C\right)
\end{equation}
where
\begin{gather}
C=\left(A^{-1}+B^{-1}\right)^{-1} \\
c=C\left(A^{-1}a+B^{-1}b\right)
\end{gather}
and the normalising factor looks like a Gaussian:
\begin{equation}
Z^{-1}=\left(2\pi\right)^{-D/2}\left|A+B\right|^{-1/2}\exp\left(-\dfrac{1}{2}\left(a-b\right)^{\top}\left(A+B\right)^{-1}\left(a-b\right)\right)
\end{equation}
We can show the above. First replace the distributions using the definition of their density functions, and substitute the definitions of $Z$, $C$ and $c$. We get
\begin{multline}
\dfrac{\exp\left[-\dfrac{1}{2}\left(x-a\right)^{\top}A^{-1}\left(x-a\right)\right]}{\left(2\pi\right)^{D/2}\left|A\right|^{1/2}}\cdot\dfrac{\exp\left[-\dfrac{1}{2}\left(x-b\right)^{\top}B^{-1}\left(x-b\right)\right]}{\left(2\pi\right)^{D/2}\left|B\right|^{1/2}} = \\
\dfrac{\exp\left[-\dfrac{1}{2}\left(a-b\right)^{\top}\left(A+B\right)^{-1}\left(a-b\right)\right]}{\left(2\pi\right)^{D/2}\left|A+B\right|^{1/2}}\times \\
\dfrac{\exp\left[-\dfrac{1}{2}\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)^{\top}\left(A^{-1}+B^{-1}\right)\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)\right]}{\left(2\pi\right)^{D/2}\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|^{1/2}}
\end{multline}
First let's show equivalence over the denominators. Equating the denominators gives
\begin{gather}
\left(2\pi\right)^{D/2}\left|A\right|^{1/2}\left(2\pi\right)^{D/2}\left|B\right|^{1/2}=\left(2\pi\right)^{D/2}\left|A+B\right|^{1/2}\left(2\pi\right)^{D/2}\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|^{1/2} \\
\left|A\right|^{1/2}\left|B\right|^{1/2}=\left|A+B\right|^{1/2}\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|^{1/2} \\
\left|A\right|\left|B\right|=\left|A+B\right|\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|
\end{gather}
We can use a property of determinants that the determinant of the inverse is the inverse of the determinant. Hence
\begin{equation}
\left|A+B\right|=\left|A\right|\left|B\right|\left|A^{-1}+B^{-1}\right|
\end{equation}
We can apply the determinant form of the matrix inversion lemma:
\begin{equation}
\left|Z+UWV^{\top}\right|=\left|Z\right|\left|W\right|\left|W^{-1}+V^{\top}Z^{-1}U\right|
\end{equation}
(note that this $Z$ is not the same as the one defined above). Letting $Z = A$, $W = B$, $U = I$, $V = I$, we have
\begin{equation}
\left|A+B\right|=\left|A\right|\left|B\right|\left|A^{-1}+B^{-1}\right|
\end{equation}
Thus we get the same as above, hence the denominators are equal. We now begin the more tedious process of proving equivalence over the numerators. Grouping exponentials, the terms inside should be equal, ie. (after taking out the $-1/2$)
\begin{multline}
\left(x-a\right)^{\top}A^{-1}\left(x-a\right)+\left(x-b\right)^{\top}B^{-1}\left(x-b\right) \\
=\left(a-b\right)^{\top}\left(A+B\right)^{-1}\left(a-b\right)+ \\
\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)^{\top}\left(A^{-1}+B^{-1}\right)\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)
\end{multline}
Expand the quadratics to get lengthy expressions for the LHS and RHS
\begin{multline}
RHS = a^{\top}\left(A+B\right)^{-1}a-b^{\top}\left(A+B\right)^{-1}a-a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+ \red x^{\top}\left(A^{-1}+B^{-1}\right)x\black \\
-\left[\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right]^{\top}\left(A^{-1}+B^{-1}\right)x-x^{\top}\left(A^{-1}+B^{-1}\right)\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right) \\
+\left[\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right]^{\top}\left(A^{-1}+B^{-1}\right)\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{multline}
LHS=\red x^{\top}A^{-1}x\black -a^{\top}A^{-1}x-x^{\top}A^{-1}a+a^{\top}A^{-1}a+ \red x^{\top}B^{-1}x\black -b^{\top}B^{-1}x-x^{\top}B^{-1}b+b^{\top}B^{-1}b
\end{multline}
The highlighted terms in \red red \black cancel out. So we have (in addition to grouping some similar terms such as $b^{\top}\left(A+B\right)^{-1}a$ and $a^{\top}\left(A+B\right)^{-1}b$ together since they are scalar and $A$, $B$ are symmetric):
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b \\
-2x^{\top}\blue\left(A^{-1}+B^{-1}\right)\left(A^{-1}+B^{-1}\right)^{-1}\black \left(A^{-1}a+B^{-1}b\right) \\
+\left(A^{-1}a+B^{-1}b\right)^{\top}\blue\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}+B^{-1}\right)\black \left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{equation}
LHS=-a^{\top}A^{-1}x-x^{\top}A^{-1}a+a^{\top}A^{-1}a-b^{\top}B^{-1}x-x^{\top}B^{-1}b+b^{\top}B^{-1}b
\end{equation}
The highlighted terms in \blue blue \black cancel out because they are the inverses of each other. Also by grouping similar terms in the LHS, we get
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b \\
\green -2x^{\top}\left(A^{-1}a+B^{-1}b\right)\black +\left(A^{-1}a+B^{-1}b\right)^{\top}\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{equation}
LHS=\green -2x^{\top}A^{-1}a\black +a^{\top}A^{-1}a \green -2x^{\top}B^{-1}b\black +b^{\top}B^{-1}b
\end{equation}

Note now that another set of highlighted terms in \green green \black cancel out on both sides. So
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+ \\
\left(A^{-1}a+B^{-1}b\right)^{\top}\red\left(A^{-1}+B^{-1}\right)^{-1}\black\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{equation}
LHS=a^{\top}A^{-1}a+b^{\top}B^{-1}b
\end{equation}

We deal with the highlighted term in \red red \black above using the matrix inversion lemma:
\begin{equation}
\left(A^{-1}+B^{-1}\right)^{-1}=A-A\left(A+B\right)^{-1}A
\end{equation}
Substituting this into the RHS:
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b \\
+\left(a^{\top}A^{-1}+b^{\top}B^{-1}\right)\left(A-A\left(A+B\right)^{-1}A\right)\left(A^{-1}a+B^{-1}b\right)
\end{multline}
Expanding out the quadratic, this gives the very length expression
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+a^{\top}A^{-1}AA^{-1}a \\
+a^{\top}A^{-1}AB^{-1}b+b^{\top}B^{-1}AA^{-1}a+b^{\top}B^{-1}AB^{-1}b-a^{\top}A^{-1}A\left(A+B\right)^{-1}AA^{-1}a \\
-b^{\top}B^{-1}A\left(A+B\right)^{-1}AA^{-1}a-a^{\top}A^{-1}A\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
We can notice instances where $A^{-1}A$ or $AA^{-1}$ appears and cancel them out.
\begin{multline}
RHS=\blue a^{\top}\left(A+B\right)^{-1}a\black -2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+ \green a^{\top}A^{-1}a \black \\
+a^{\top}B^{-1}b+b^{\top}B^{-1}a+b^{\top}B^{-1}AB^{-1}b- \blue a^{\top}\left(A+B\right)^{-1}a\black -b^{\top}B^{-1}A\left(A+B\right)^{-1}a \\
-a^{\top}\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
We can cancel out some \blue blue \black terms within the RHS, as well as cancel the $\green a^{\top}A^{-1}a \black$ from the LHS. We are left with
\begin{multline}
RHS=-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+a^{\top}B^{-1}b+b^{\top}B^{-1}a+b^{\top}B^{-1}AB^{-1}b \\
-b^{\top}B^{-1}A\left(A+B\right)^{-1}a-a^{\top}\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
\begin{equation}
LHS=b^{\top}B^{-1}b
\end{equation}
We can also group similar terms in the RHS, giving
\begin{multline}
RHS=-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+2a^{\top}B^{-1}b+ \\
b^{\top}B^{-1}AB^{-1}b-2a^{\top}\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
Next we subtract the LHS from the RHS and factorise out $a^{\top}\left(\cdot\right)b$ and $b^{\top}\left(\cdot\right)b$:
\begin{multline}
RHS-LHS=a^{\top}\green\underbrace{\black\left(-2\left(A+B\right)^{-1}+2B^{-1}-2\left(A+B\right)^{-1}AB^{-1}\right)}_{\green=0}\black b \\
+b^{\top}\green\underbrace{\black\left(\left(A+B\right)^{-1}+B^{-1}AB^{-1}-B^{-1}A\left(A+B\right)^{-1}AB^{-1}\right)}_{\green =0}\black b
\end{multline}
Thus to show equivalence, we need to show that the terms inside the brackets are equal to zero. This requires carrying out some manipulations. Starting from the first term:
\begin{align}
-2\left(A+B\right)^{-1}+2B^{-1}-2\left(A+B\right)^{-1}AB^{-1}&=0 \\
-\left(A+B\right)^{-1}+B^{-1}-\left(A+B\right)^{-1}AB^{-1}&=0 \\
B^{-1}&=\left(A+B\right)^{-1}\left(I+AB^{-1}\right) \\
A+B&=\left(I+AB^{-1}\right)B \\
A+B&=B+A
\end{align}
For the second term:
\begin{align}
\left(A+B\right)^{-1}+B^{-1}AB^{-1}-B^{-1}A\left(A+B\right)^{-1}AB^{-1}&=0 \\
B^{-1}\left(AB^{-1}-A\left(A+B\right)^{-1}AB^{-1}-I\right)&=-\left(A+B\right)^{-1} \\
\left(AB^{-1}-A\left(A+B\right)^{-1}AB^{-1}-I\right)\left(A+B\right)&=-B \\
AB^{-1}A-A\left(A+B\right)^{-1}AB^{-1}A-A+A-A\left(A+B\right)^{-1}A-B&=-B \\
AB^{-1}A-A\left(A+B\right)^{-1}AB^{-1}A-A\left(A+B\right)^{-1}A&=0 \\
A\left(B^{-1}-\left(A+B\right)^{-1}AB^{-1}-\left(A+B\right)^{-1}\right)A&=0
\end{align}
Since $A \neq 0$ (we have already taken the inverse of $A$ up until this stage):
\begin{align}
B^{-1}-\left(A+B\right)^{-1}AB^{-1}-\left(A+B\right)^{-1}&=0 \\
B^{-1}&=\left(A+B\right)^{-1}\left(AB^{-1}+I\right) \\
A+B&=\left(AB^{-1}+I\right)B \\
A+B&=A+B
\end{align}
Hence the terms inside the brackets are zero, so the LHS equals the RHS. This finally shows equivalence over the numerators.

\subsection{Marginalisation of Gaussians}

Suppose we have distributions $p\left(x\right)$ and $p\left(y\middle|x\right)$, and we want to obtain the distribution $p\left(y\right)$. We can do this by first computing the joint distribution $p\left(x, y\right) = p\left(x\right)p\left(y\middle|x\right)$ and integrating over $x$ as follows
\begin{equation}
p\left(y\right) = \int p\left(x, y\right)dx
\end{equation}
This is known as marginalisation. Suppose $p\left(x\right)$ and $p\left(y\middle|x\right)$ are Gaussians in the sense that
\begin{gather}
p\left(x\right) = \mathcal{N}_{x}\left(a, A^{-1}\right) \\
p\left(y\middle|x\right) = \mathcal{N}_{y}\left(C^{\top}x, B^{-1}\right)
\end{gather}
Note that $x$ and $y$ do not have to be of the same dimension. The general technique to evaluate the integral is from the joint distribution, decompose it into $p\left(x, y\right) = p\left(y\right)p\left(x\middle|y\right)$. Then
\begin{align}
\int p\left(x, y\right)dx &= \int p\left(y\right)p\left(x\middle|y\right)dx
\\
&= p\left(y\right) \int p\left(x\middle|y\right)dx \\
&= p\left(y\right)
\end{align}
since the integral of $p\left(x\middle|y\right)$ evaluates to 1. Without worrying about normalising constants, we substitute the exponential expressions for the Gaussian distributions
\begin{align}
p\left(y\right) &\propto \int\mathcal{N}_{x}\left(a,A^{-1}\right)\mathcal{N}_{y}\left(C^{\top}x,B^{-1}\right)dx \\
&\propto \int\exp\left[-\dfrac{1}{2}\left(\left(x-a\right)^{\top}A\left(x-a\right)+\left(y-C^{\top}x\right)^{\top}B\left(y-C^{\top}x\right)\right)\right]dx \\
&\propto \int\exp\left[-\dfrac{1}{2}\left(x^{\top}Ax-2a^{\top}Ax+a^{\top}Aa+y^{\top}By-2y^{\top}BC^{\top}x+x^{\top}CBC^{\top}x\right)\right]dx \\
&\propto \int\exp\left[-\dfrac{1}{2}\left(x^{\top}\left(A+CBC^{\top}\right)x-2\left(a^{\top}A+y^{\top}BC^{\top}\right)x+a^{\top}Aa+y^{\top}By\right)\right]dx
\end{align}
To decompose the distributions, we complete the square:
\begin{lemma}[Completing the Square]
\begin{equation}
\dfrac{1}{2}x^{\top}Mx + d^{\top}x + e = \dfrac{1}{2}\left(x - m\right)^{\top}M\left(x - m\right) + v
\end{equation}
where
\begin{gather}
m = -C^{-1}d \\
v= e - \dfrac{1}{2}d^{\top}M^{-1}d
\end{gather}
\end{lemma}
So by letting $M = A + CBC^{\top}$, $d = -Aa - CBy$, and $e = \dfrac{1}{2}a^{\top}Aa + \dfrac{1}{2}y^{\top}By$, this gives
\begin{gather}
m = \left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right) \\
v = \dfrac{1}{2}a^{\top}Aa + \dfrac{1}{2}y^{\top}By - \dfrac{1}{2}\left(Aa + CBy\right)^{\top}\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right)
\end{gather}
So we can write
\begin{equation}
p\left(y\right)\propto\int\exp\left[-\dfrac{1}{2}\left(\left(x-m\right)^{\top}M\left(x-m\right)+v\right)\right]dx
\end{equation}
and from this we can see that
\begin{align}
p\left(x\middle|y\right) &= \mathcal{N}_{x}\left(m, M^{-1}\right) \\
&= \mathcal{N}_{x}\left(\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right), \left(A + CBC^{\top}\right)^{-1}\right) \\
\end{align}
For the remaining terms, these can be taken out of the integral
\begin{align}
p\left(y\right) &\propto \exp\left(-\dfrac{1}{2}v \right)\int \mathcal{N}_{x}\left(\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right), \left(A + CBC^{\top}\right)^{-1}\right) dx \\
&\propto \exp\left(-\dfrac{1}{2}v \right) \\
&\propto \exp\left[-\dfrac{1}{2}\left(a^{\top}Aa + y^{\top}By- \left(Aa + CBy\right)^{\top}\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right)\right) \right] \\
&\begin{multlined} \propto \exp\left[-\dfrac{1}{2}\left(y^{\top}By-y^{\top}BC^{\top}\left(A+CBC^{\top}\right)^{-1}CBy-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right.\right. \\
\left.\left. +a^{\top}Aa-a^{\top}A\left(A+CBC^{\top}\right)^{-1}Aa\right)\right]
\end{multlined}
\end{align}
Getting rid of the terms which don't depend on $y$
\begin{align}
p\left(y\right) &\propto \exp\left[-\dfrac{1}{2}\left(y^{\top}By-y^{\top}BC^{\top}\left(A+CBC^{\top}\right)^{-1}CBy-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right)\right] \\
&\propto \exp\left[-\dfrac{1}{2}\left(y^{\top}\left(B - BC^{\top}\left(A+CBC^{\top}\right)^{-1}CB\right)y-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right)\right]
\end{align}
Note by the matrix inversion lemma, $B - BC^{\top}\left(A+CBC^{\top}\right)^{-1}CB = \left(B^{-1} + C^{\top}A^{-1}C\right)^{-1}$. This yields the simplification
\begin{equation}
p\left(y\right) \propto \exp\left[-\dfrac{1}{2}\left(y^{\top}\left(B^{-1} + C^{\top}A^{-1}C\right)^{-1}y-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right)\right]
\end{equation}
Once again we can complete the square in $y$. Doing so (and ignoring the terms which don't depend on $y$ due to proportionality) gives
\begin{multline}
p\left(y\right) \propto \exp\left[-\dfrac{1}{2}\left(y - \left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa\right)^{\top}\cdot\left(B^{-1} + C^{\top}A^{-1}C\right)^{-1} \right. \\
\left. \cdot\left(y - \left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa\right)\right]
\end{multline}
This gives the distribution for $p\left(y\right)$
\begin{equation}
p\left(y\right) = \mathcal{N}_{y}\left(\left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa, B^{-1} + C^{\top}A^{-1}C\right)
\end{equation}
To simplify the mean,we can show that
\begin{equation}
\left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa = C^{\top}a
\end{equation}
Expanding the LHS gives
\begin{align}
C^{\top}\left(A+CBC^{\top}\right)^{-1}Aa + C^{\top}A^{-1}CBC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa &= C^{\top}a \\
C^{\top}\green\underbrace{\black\left[\left(A+CBC^{\top}\right)^{-1}A + A^{-1}CBC^{\top}\left(A+CBC^{\top}\right)^{-1}A\right]}_{\green=I}\black a &= C^{\top}a
\end{align}
So we require $\left(A+CBC^{\top}\right)^{-1}A + A^{-1}CBC^{\top}\left(A+CBC^{\top}\right)^{-1}A = I$ by hypothesis. Some manipulation yields
\begin{gather}
\left(I + A^{-1}CBC^{\top}\right)\left(A+CBC^{\top}\right)^{-1}A = I \\
I + A^{-1}CBC^{\top} = A^{-1}\left(A+CBC^{\top}\right) \\
I + A^{-1}CBC^{\top} = I + A^{-1}CBC^{\top}
\end{gather}
Therefore we finally have
\begin{equation}
p\left(y\right) = \mathcal{N}_{y}\left(C^{\top}a, B^{-1} + C^{\top}A^{-1}C\right)
\end{equation}

\section{Exponential Families}

\section{Random Matrices}

\subsection{Wishart Distribution}

\section{Stochastic Processes}

A continuous time stochastic process $X\left(t\right)$ assigns a function of time $x\left(t\right)$ to each outcome in the sample space. The function $x\left(t\right)$ is said to be a sample path, ie. a realisation of the stochastic process. At some fixed time $t_{0}$, the variable $X\left(t_{0}\right)$ is a random variable. A discrete time stochastic process $X_{k}$ assigns a sequence to each outcome in the sample space, where similarly for some fixed integer $k_{0}$, the variable $X_{k_{0}}$ is a random variable. Note that $X\left(t_{0}\right)$ and $X_{k_{0}}$ can be discrete or continuous-valued random variables (or both), so we can consider four different types of stochastic processes (excluding hybrids):
\begin{enumerate}
\item Continuous-time continuous-valued
\item Continuous-time discrete-valued
\item Discrete-time continuous-valued
\item Discrete-time discrete-valued
\end{enumerate}

\subsection{Properties of Stochastic Processes}

\subsubsection{Mean Function}

The mean function $\mu_{X}\left(t\right)$ for a continuous time stochastic process $X\left(t\right)$ is defined by
\begin{equation}
\mu_{X}\left(t\right) = \mathbb{E}\left[X\left(t\right)\right]
\end{equation}
and similarly for a discrete time stochastic process.

\subsubsection{Autocovariance Function}

The autocovariance function of a stochastic process $X\left(t\right)$ is the covariance between the process at time $t$ and the process at time shifted by some $\tau$. Formally,
\begin{align}
C_{X}\left(t, \tau\right) &= \operatorname{Cov}\left(X\left(t\right), X\left(t + \tau\right)\right) \\
&= \mathbb{E}\left[X\left(t\right)X\left(t+\tau\right)\right] - \mu_{X}\left(t\right)\mu_{X}\left(t + \tau\right)
\end{align}
An alternative notation instead expresses the covariance at two times $t$ and $s$, in which case
\begin{equation}
C_{X}\left(t, s\right) = \mathbb{E}\left[X\left(t\right)X\left(s\right)\right] - \mu_{X}\left(t\right)\mu_{X}\left(s\right)
\end{equation}
Analogously for a random sequence at time $m$ with time shift $k$,
\begin{equation}
C_{X}\left(m, k\right) = \mathbb{E}\left[X_{m}X_{m + k}\right] - \mu_{X}\left(m\right)\mu_{X}\left(m + k\right)
\end{equation}

\subsubsection{Autocorrelation Function}

The autocorrelation function of a stochastic process $X\left(t\right)$ for time $t$ and difference $\tau$ is defined as
\begin{equation}
R_{X}\left(t, \tau\right) = \mathbb{E}\left[X\left(t\right)X\left(t+\tau\right)\right]
\end{equation}
and analogously for a discrete random sequence with time $m$ and difference $k$:
\begin{equation}
R_{X}\left(m, k\right) =  \mathbb{E}\left[X_{m}X_{m + k}\right]
\end{equation}
It is clear that a zero-mean stochastic process has autocovariance function equal to autocorrelation function. Note that the autocorrelation function is distinction from the autocorrelation coefficient, the latter which refers to a function for the traditional (normalised) correlation coefficient.

\subsubsection{Autocorrelation Coefficient}

The autocorrelation coefficient for a stochastic process $X\left(t\right)$ with times $t$ and $s$ is defined as
\begin{equation}
r_{X}\left(t, s\right) = \dfrac{C_{X}\left(t, s\right)}{\sqrt{C_{X}\left(t, t\right)C_{X}\left(s, s\right)}}
\end{equation}
which is the correlation coefficient between random variables $X\left(t\right)$ and $X\left(s\right)$. Analogously for a random sequence
\begin{equation}
r_{X}\left(m, n\right) = \dfrac{C_{X}\left(m, n\right)}{\sqrt{C_{X}\left(m, m\right)C_{X}\left(n, n\right)}}
\end{equation}

\subsubsection{Cross-correlation Function}

For two continuous stochastic processes $X\left(t\right)$ and $Y\left(t\right)$, the cross-correlation function with time $t$ and difference $\tau$ is defined as
\begin{equation}
R_{XY}\left(t, \tau\right) = \mathbb{E}\left[X\left(t\right)Y\left(t + \tau\right)\right]
\end{equation}
For two continuous random sequences $X_{n}$ and $Y_{n}$,
\begin{equation}
R_{XY}\left(m, k\right) = \mathbb{E}\left[X_{m}Y_{m + k}\right]
\end{equation}
The autocorrelation function can be thought of as a special case of the cross-correlation function where $X$ and $Y$ are identical processes.

\subsubsection{Second-order Processes}

Second-order processes have finite second moments, ie. for a continuous time process:
\begin{equation}
\mathbb{E}\left[X\left(t\right)^{2}\right] < \infty
\end{equation}
for all $t$.

\subsection{Stationarity}

\subsubsection{Strict Stationarity}

Let $X\left(t\right)$ be a continuous time stochastic process where $f_{X\left(t_{1}\right), \dots, X\left(t_{m}\right)}\left(x_{1}, \dots, x_{m}\right)$ is the joint probability density function for the process at the set of time instants $t_{1}, \dots, t_{m}$. The process $X\left(t\right)$ is said to be strictly stationary if and only if for all sets of time instants and any time difference $\tau$,
\begin{equation}
f_{X\left(t_{1}\right), \dots, X\left(t_{m}\right)}\left(x_{1}, \dots, x_{m}\right) = f_{X\left(t_{1} + \tau\right), \dots, X\left(t_{m} + \tau\right)}\left(x_{1}, \dots, x_{m}\right)
\end{equation}
Let $X_{n}$ be a discrete time random sequence where $p_{X_{n_{1}}, \dots, X_{n_{m}}}\left(x_{1}, \dots, x_{m}\right)$ is the joint probability mass function for the process at the set of integer time instants $n_{1}, \dots, n_{m}$. The process $X_{n}$ is said to be strictly stationary if and only if for all sets of time instants and any time difference $k$,
\begin{equation}
p_{X_{n_{1}}, \dots, X_{n_{m}}}\left(x_{1}, \dots, x_{m}\right) = p_{X_{n_{1} + k}, \dots, X_{n_{m} + k}}\left(x_{1}, \dots, x_{m}\right)
\end{equation}

\subsubsection{Wide-Sense Stationarity}

Wide-sense stationarity (or weak stationarity) is a `weaker' form of strict stationarity. A wide-sense stationary process requires that the mean function be a constant, ie.
\begin{equation}
\mathbb{E}\left[X\left(t\right)\right] = \mu_{X}
\end{equation}
and the autocorrelation function only depend on the time difference, ie.
\begin{equation}
R_{X}\left(t, \tau\right) = R_{X}\left(\tau\right)
\end{equation}

A wide-sense stationary process does not need to be strictly stationary. For example, a sequence of uncorrelated random variables with the same mean and variance is wide-sense stationary by definition, but it will not be strictly stationary if these random variables are not identically distributed. \\

A strictly stationary process does not always also imply it is wide-sense stationary. For example, a sequence of i.i.d Cauchy random variables is strictly stationary, but not wide-sense stationary because the Cauchy distribution does not have a finite second moment. Generally however, second-order processes which are strictly stationary are also wide-sense stationary.

\subsubsection{Joint Wide-Sense Stationarity}

Two stochastic processes are jointly wide-sense stationary if both processes are wide-sense stationary and additionally the cross-correlation between the two only depends on the time difference, ie.
\begin{equation}
R_{XY}\left(t, \tau\right) = R_{XY}\left(\tau\right)
\end{equation}

\subsection{Gaussian Processes}

\chapter{Bayesian Probability \& Statistics}

\section{Cox's Theorem}

\section{Extensions to Bayes' Theorem}

\subsection{Bayes' Theorem for Three Events}

There are multiple ways of writing the chain rule of probability for three events $A$, $B$ and $C$.
\begin{align}
\operatorname{Pr}\left(A, B, C\right) &= \operatorname{Pr}\left(A, B\middle|C\right)\operatorname{Pr}\left(C\right) \\
&= \operatorname{Pr}\left(A, C\middle|B\right)\operatorname{Pr}\left(B\right) \\
&= \operatorname{Pr}\left(B, C\middle|A\right)\operatorname{Pr}\left(A\right) \\
&= \operatorname{Pr}\left(A\middle|B, C\right)\operatorname{Pr}\left(B, C\right) \\
&= \operatorname{Pr}\left(B\middle|A, C\right)\operatorname{Pr}\left(A, C\right) \\
&= \operatorname{Pr}\left(C\middle|A, B\right)\operatorname{Pr}\left(A, B\right)
\end{align}
This means that there are $\mathstrut^{6}\mathsf{C}_{2} = 15$ different ways to write Bayes' theorem for three events by choosing any two and rearranging, for example:
\begin{equation}
\operatorname{Pr}\left(A, B\middle|C\right) = \dfrac{\operatorname{Pr}\left(B, C\middle|A\right)\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(C\right)}
\end{equation}
and
\begin{equation}
\operatorname{Pr}\left(A\middle|B, C\right) = \dfrac{\operatorname{Pr}\left(B, C\middle|A\right)\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(B, C\right)}
\end{equation}

\section{Bayesian Priors}

\subsection{Principle of Indifference}

The principle of indifference says that if an experiment yields $n$ mutually exclusive outcomes, and nothing else is known, then the prior probability of each outcome should be assigned $1/n$.

\subsection{Cromwell's Rule}

Cromwell's rule asserts that prior probabilities of zero should be avoided, except for statements which are logically true or false.

\subsection{Uninformative Priors}

An uninformative prior gives no subjective information. An improper prior may be used as an uninformative prior. An improper prior is a prior whose sum or integral does not necessarily sum to 1. From Bayes' theorem, it can be seen that scaling all prior probabilities or densities by some constant will still give the same result for the posterior. So it is still possible to use improper priors in Bayesian inference. An example of an improper prior is a uniform distribution over an infinite interval.

\subsection{Conjugate Priors}

\subsection{Empirical Bayes}

\section{Bayesian Updating}

\subsection{Rule of Succession}

The rule of succession is the application of Bayes' theorem to the probability of success/fail after repeated trials. Suppose $X_{1}, \dots, X_{n + 1}$ are independent binary random variables. Then suppose that after $n$ trials, $s$ successes were observed. Then under a uniform prior on the success probability $\theta$, the probability of the next trial being a success is given by
\begin{equation}
\operatorname{Pr}\left(X_{n + 1} = 1\middle|X_{1}, \dots, X_{n + 1}\right) = \dfrac{s + 1}{n + 2}
\end{equation}
To show this, first write the likelihood of $\theta$ given $s$ successes as
\begin{align}
\mathcal{L}\left(\theta\middle|s\right) &= \operatorname{Pr}\left(X_{1} + \dots + X_{n} = s\middle|\theta\right) \\
&= \theta^{s}\left(1 - \theta\right)^{n - s}
\end{align}
The prior density $p\left(\theta\right)$ has function
\begin{equation}
p\left(\theta\right) = \begin{cases} 1, & 0\leq \theta \leq 1 \\ 0, & \mathrm{otherwise}\end{cases}
\end{equation}
The posterior density $p\left(\theta\middle|X_{1} + \dots + X_{n} = s\right)$ takes the form
\begin{equation}
p\left(\theta\middle|X_{1} + \dots + X_{n} = s\right) = \dfrac{\theta^{s}\left(1 - \theta\right)^{n - s}}{\int_{0}^{1}\theta^{s}\left(1 - \theta\right)^{n - s}d\theta}
\end{equation}
Note that the denominator is a Beta function, and in fact the posterior density is a Beta distribution. Since $n$ and $s$ are integers, then posterior distribution can be written in terms of factorials as
\begin{equation}
p\left(\theta\middle|X_{1} + \dots + X_{n} = s\right) = \dfrac{\left(n + 1\right)!}{s!\left(n  - s\right)!}\theta^{s}\left(1 - \theta\right)^{n - s}
\end{equation}
where in terms of the traditional parameters of the Beta distribution we have $\alpha = s + 1$ and $\beta = n - s + 1$. Hence the mean of the Beta posterior density is
\begin{equation}
\mathbb{E}\left[\theta\middle|X_{1} + \dots + X_{n} = s\right] = \dfrac{\alpha}{\alpha + \beta} = \dfrac{s + 1}{n + 2} 
\end{equation}
As $X_{n + 1}$ is a binary variable, the probability of success is simply given by this mean. Note that when $s = 0$ and $n = 0$ (ie. no trials have been conducted), the expectation becomes $\dfrac{1}{2}$, in agreement with having used a uniform prior on the probability of success. \\

Now, a more intuitive guess for the probability of success is the observed sample proportion of success $\dfrac{s}{n}$. We can show that this results in using the improper prior
\begin{equation}
p\left(\theta\right) \propto \dfrac{1}{\theta\left(1 - \theta\right)}
\end{equation}
over $0 \leq \theta \leq 1$. The posterior density is then proportional to
\begin{equation}
p\left(\theta\middle|X_{1} + \dots + X_{n} = s\right) \propto \theta^{s - 1}\left(1 - \theta\right)^{n - s - 1}
\end{equation}
where the normalising constant is the Beta function $\operatorname{B}\left(s, n\right)$, hence (provided $s \neq 0$ and $s \neq n$) the posterior density is a Beta distribution with parameters $\alpha = s$ and $\beta = n - s$, which has a mean of $\dfrac{s}{n}$.

\subsection{Odds Ratio Updating}

The odds ratio of an event $A$ is defined as $\dfrac{\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(\overline{A}\right)}$. The odds ratio can be used as a way to simplify Bayesian updating. From some evidence $B$, the likelihood ratio is defined as $\dfrac{\operatorname{Pr}\left(B\middle|A\right)}{\operatorname{Pr}\left(B\middle|\overline{A}\right)}$. Then from Bayes' theorem the posterior probabilities are:
\begin{gather}
\operatorname{Pr}\left(A\middle|B\right) = \dfrac{\operatorname{Pr}\left(B\middle|A\right)\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(B\right)} \\
\operatorname{Pr}\left(\overline{A}\middle|B\right) = \dfrac{\operatorname{Pr}\left(B\middle|\overline{A}\right)\operatorname{Pr}\left(\overline{A}\right)}{\operatorname{Pr}\left(B\right)}
\end{gather}
Dividing these two probabilities gives
\begin{equation}
\dfrac{\operatorname{Pr}\left(A\middle|B\right)}{\operatorname{Pr}\left(\overline{A}\middle|B\right)} = \dfrac{\operatorname{Pr}\left(B\middle|A\right)}{\operatorname{Pr}\left(B\middle|\overline{A}\right)}\cdot\dfrac{\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(\overline{A}\right)}
\end{equation}
Hence this shows that the posterior odds ratio can be expressed as a multiplication between the likelihood ratio and the prior odds ratio.

\subsection{Log Odds Updating}

The log odds is the logarithm of the odds ratio, also known as the logit.
\begin{align}
\operatorname{logit}\left(\operatorname{Pr}\left(A\right)\right) &= \log\left(\dfrac{\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(\overline{A}\right)}\right) \\
&= \log\left(\dfrac{\operatorname{Pr}\left(A\right)}{1 - \operatorname{Pr}\left(A\right)}\right) \\
&= \log\operatorname{Pr}\left(A\right) - \log\left(1 - \operatorname{Pr}\left(A\right)\right)
\end{align}
The posterior log odds can be obtain by addition of the log likelihood ratio and the prior log odds.
\begin{gather}
\log\left(\dfrac{\operatorname{Pr}\left(A\middle|B\right)}{\operatorname{Pr}\left(\overline{A}\middle|B\right)}\right) = \log\left(\dfrac{\operatorname{Pr}\left(B\middle|A\right)}{\operatorname{Pr}\left(B\middle|\overline{A}\right)}\cdot\dfrac{\operatorname{Pr}\left(A\right)}{\operatorname{Pr}\left(\overline{A}\right)}\right) \\
\operatorname{logit}\left(\operatorname{Pr}\left(A\middle|B\right)\right) = \log\left(\dfrac{\operatorname{Pr}\left(B\middle|A\right)}{\operatorname{Pr}\left(B\middle|\overline{A}\right)}\right) + \operatorname{logit}\left(\operatorname{Pr}\left(A\right)\right)
\end{gather}

\subsection{Bayes Filters \cite{Thrun2005}}

Consider a discrete time stochastic process for a state variable $x_{k}$, where the evolution of the state can be expressed with the distribution $p\left(x_{k + 1}\middle|x_{k}\right)$. The state $x_{k}$ is not directly measured but instead another variable $z_{k}$ is observed with the distribution $p\left(z_{k}\middle|x_{k}\right)$. The task in Bayes filtering is to recover the distribution for $x_{k}$ given only measurements up to an including $z_{k}$. From Bayes' theorem we can write
\begin{align}
p\left(x_{k}\middle|z_{k}\right) &= \dfrac{p\left(z_{k}\middle|x_{k}\right)p\left(x_{k}\right)}{p\left(z_{k}\right)} \\
&\propto p\left(z_{k}\middle|x_{k}\right)p\left(x_{k}\right)
\end{align}
It is evident that we need only be concerned with finding $p\left(z_{k}\middle|x_{k}\right)p\left(x_{k}\right)$, because it is the same as the posterior $p\left(x_{k}\middle|z_{k}\right)$ up to a normalising constant. Since the observation distribution $p\left(z_{k}\middle|x_{k}\right)$ is assumed given, then $p\left(x_{k}\right)$ is the only remaining distribution required. From the update density $p\left(x_{k}\middle|x_{k - 1}\right)$ we can perform marginalisation:
\begin{equation}
p\left(x_{k}\right) = \int p\left(x_{k}\middle|x_{k - 1}\right)p\left(x_{k - 1}\right)dx_{k - 1}
\end{equation}
Note that this means the distribution $p\left(x_{k - 1}\right)$ is needed, thus the algorithm requires an initial guess of the state given by $p\left(x_{0}\right)$. When an input $u_{k}$ is involved in the state evolution, this allows for the distributions $p\left(x_{k + 1}\middle|x_{k}, u_{k}\right)$ and $p\left(z_{k}\middle|x_{k}, u_{k}\right)$ to be given. We require
\begin{equation}
p\left(x_{k}\middle|z_{k}, u_{k}\right) \propto p\left(z_{k}\middle|x_{k}, u_{k}\right)p\left(x_{k}\middle|u_{k}\right)
\end{equation}
We assume that $p\left(x_{k}\middle|u_{k}\right) = p\left(x_{k}\right)$, ie. the input has no effect on the current state (only the subsequent state) however $u_{k}$ is still allowed to be a deterministic function of $x_{k}$ because
\begin{equation}
p\left(x_{k}\middle|u_{k}\right) = \dfrac{p\left(u_{k}\middle|x_{k}\right)p\left(x_{k}\right)}{p\left(u_{k}\right)} = p\left(x_{k}\right)
\end{equation}
Then
\begin{equation}
p\left(x_{k}\middle|z_{k}, u_{k}\right) \propto p\left(z_{k}\middle|x_{k}, u_{k}\right)p\left(x_{k}\right)
\end{equation}
and $p\left(x_{k}\right)$ can be obtained by
\begin{equation}
p\left(x_{k}\right) = \int\int p\left(x_{k}\middle|x_{k - 1}, u_{k - 1}\right)p\left(x_{k - 1}, u_{k - 1}\right)dx_{k - 1}du_{k - 1}
\end{equation}
As before if $u_{k - 1}$ is a deterministic function of $x_{k - 1}$ and $x_{k - 1}$ is unaffected by $u_{k - 1}$ then
\begin{equation}
p\left(x_{k}\right) = \int p\left(x_{k}\middle|x_{k - 1}, u_{k - 1}\right)p\left(x_{k - 1}\right)dx_{k - 1}
\end{equation}

\section{Bayesian Networks}

\subsection{Factor Graphs}

\subsection{Probabilistic Graphical Models}

\section{Bayesian Inference}

\subsection{Maximum a Posteriori Estimation}

\subsection{Bayes Estimators}

\subsection{Credible Intervals}

\subsection{Bayesian Regularisation}

\subsection{Bayesian Linear Regression \cite{Rasmussen2006}}

Consider a linear model of the form with weights $\mathbf{w}$:
\begin{equation}
f\left(x\right) = x^{\top}\mathbf{w}
\end{equation}
with the data generating process
\begin{equation}
y=f\left(x\right)+\varepsilon
\end{equation}
where the noise distribution is given by $\varepsilon\sim\mathcal{N}\left(0,\sigma_{\mathrm{n}}^{2}\right)$. Denote the data by $\mathcal{D}=\left(X,\mathbf{y}\right)$ where $X =\begin{bmatrix}x_{1} & x_{2} & \dots & x_{n}\end{bmatrix}$ consists of the feature vectors $x_{1}, \dots, x_{n}$ aggregated column-wise and $\mathbf{y}$ is a vector of responses. Constructing the likelihood under an i.i.d. assumption:
\begin{align}
\mathcal{L}\left(\mathcal{D}\middle|X,\mathbf{w}\right) &= p\left(\mathbf{y}\middle|X,\mathbf{w}\right) \\
&= \prod_{i=1}^{n}p\left(y_{i}\middle|x_{i},\mathbf{w}\right)
\end{align}
Since $\varepsilon_{i}\sim\mathcal{N}\left(0,\sigma_{\mathrm{n}}^{2}\right)$, then each
\begin{align}
y_{i} &= f\left(x_{i}\right)+\varepsilon_{i} \\
&\sim \mathcal{N}\left(f\left(x_{i}\right),\sigma_{\mathrm{n}}^{2}\right) \\
&\sim \mathcal{N}\left(x_{i}^{\top}\mathbf{w},\sigma_{\mathrm{n}}^{2}\right)
\end{align}
The form of the Gaussian density is given by
\begin{equation}
p\left(y_{i}\middle|x_{i},\mathbf{w}\right)=\dfrac{1}{\sigma_{\mathrm{n}}\sqrt{2\pi}}\exp\left[-\dfrac{\left(y_{i}-x_{i}^{\top}\mathbf{w}\right)^{2}}{2\sigma_{\mathrm{n}}^{2}}\right]
\end{equation}
hence the likelihood becomes
\begin{align}
\prod_{i=1}^{n}p\left(y_{i}\middle|x_{i},\mathbf{w}\right) &= \prod_{i=1}^{n}\dfrac{1}{\sigma_{\mathrm{n}}\sqrt{2\pi}}\exp\left[-\dfrac{\left(y_{i}-x_{i}^{\top}\mathbf{w}\right)^{2}}{2\sigma_{\mathrm{n}}^{2}}\right] \\
&= \dfrac{1}{\sigma_{\mathrm{n}}\sqrt{2\pi}}\exp\left[-\sum_{i=1}^{n}\dfrac{\left(y_{i}-x_{i}^{\top}\mathbf{w}\right)^{2}}{2\sigma_{\mathrm{n}}^{2}}\right] \\
&= \dfrac{1}{\sigma_{\mathrm{n}}\sqrt{2\pi}}\exp\left[-\dfrac{1}{2\sigma_{\mathrm{n}}^{2}}\sum_{i=1}^{n}\left(y_{i}-x_{i}^{\top}\mathbf{w}\right)^{2}\right] \\
&= \dfrac{1}{\sigma_{\mathrm{n}}\sqrt{2\pi}}\exp\left[-\dfrac{1}{2\sigma_{\mathrm{n}}^{2}}\sum_{i=1}^{n}\left\Vert \mathbf{y}-X^{\top}\mathbf{w}\right\Vert ^{2}\right]
\end{align}
Therefore the likelihood can be written compactly as
\begin{equation}
p\left(\mathbf{y}\middle|X,\mathbf{w}\right)=\mathcal{N}\left(X^{\top}\mathbf{w},\sigma_{\mathrm{n}}^{2}I\right)
\end{equation}
Suppose the prior for the weights is a Gaussian:
\begin{gather}
\mathbf{w}\sim\mathcal{N}\left(\mathbf{0},\Sigma_{\mathrm{p}}\right) \\
p\left(\mathbf{w}\right)\propto\exp\left(-\dfrac{1}{2}\mathbf{w}^{\top}\Sigma_{\mathrm{p}}^{-1}\mathbf{w}\right)
\end{gather}
By Bayes' theorem, then the posterior distribution for the weights is
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y},X\right)=\dfrac{p\left(\mathbf{y}\middle|X,\mathbf{w}\right)p\left(\mathbf{w}\middle|X\right)}{p\left(\mathbf{y}\middle|X\right)}
\end{equation}
where $\mathbf{w}$ and $\mathbf{y}$ are the variables being exchanged, and all conditioned on $X$. Since we can assume that $X$ and $\mathbf{w}$ are independent, then
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y},X\right)=\dfrac{p\left(\mathbf{y}\middle|X,\mathbf{w}\right)p\left(\mathbf{w}\right)}{p\left(\mathbf{y}\middle|X\right)}
\end{equation}
This can be rewritten as
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y},X\right)=\dfrac{p\left(\mathbf{y}\middle|X,\mathbf{w}\right)p\left(\mathbf{w}\right)}{\int p\left(\mathbf{y}\middle|X,\mathbf{w}\right)p\left(\mathbf{w}\right)d\mathbf{w}}
\end{equation}
to express that the marginal likelihood $p\left(\mathbf{y}\middle|X\right)$ is obtained via marginalisation of the likelihood multiplied by prior. Hence $p\left(\mathbf{y}\middle|X\right)$ is a constant with respect to $\mathbf{w}$, and the posterior is proportional to the numerator:
\begin{align}
p\left(\mathbf{w}\middle|\mathbf{y},X\right) &\propto p\left(\mathbf{y}\middle|X,\mathbf{w}\right)p\left(\mathbf{w}\right) \\
&\propto \exp\left[-\dfrac{1}{2\sigma_{\mathrm{n}}^{2}}\left(\mathbf{y}-X^{\top}\mathbf{w}\right)^{\top}\left(\mathbf{y}-X^{\top}\mathbf{w}\right)\right]\exp\left(-\dfrac{1}{2}\mathbf{w}^{\top}\Sigma_{\mathrm{p}}^{-1}\mathbf{w}\right) \\
&\propto \exp\left[-\dfrac{1}{2}\left[\dfrac{\left(\mathbf{y}-X^{\top}\mathbf{w}\right)^{\top}\left(\mathbf{y}-X^{\top}\mathbf{w}\right)}{\sigma_{\mathrm{n}}^{2}}+\mathbf{w}^{\top}\Sigma_{\mathrm{p}}^{-1}\mathbf{w}\right]\right] \\
&\propto \exp\left[-\dfrac{1}{2}\left[\dfrac{1}{\sigma_{\mathrm{n}}^{2}}\left(\mathbf{y}^{\top}\mathbf{y}-\mathbf{y}^{\top}X^{\top}\mathbf{w}-\mathbf{w}^{\top}X\mathbf{y}+\mathbf{w}^{\top}XX^{\top}\mathbf{w}+\mathbf{w}^{\top}\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}\mathbf{w}\right)\right]\right] \\
&\propto \exp\left[-\dfrac{1}{2}\left[\mathbf{w}^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\mathbf{w}+\dfrac{\mathbf{y}^{\top}\mathbf{y}-\mathbf{y}^{\top}X^{\top}\mathbf{w}-\mathbf{w}^{\top}X\mathbf{y}}{\sigma_{\mathrm{n}}^{2}}\right]\right]
\end{align}
We now complete the square by first writing
\begin{multline}
p\left(\mathbf{w}\middle|\mathbf{y},X\right) \propto \exp\left[-\dfrac{1}{2}\left[\mathbf{w}^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\mathbf{w}\right.\right. \\
-\dfrac{1}{\sigma_{\mathrm{n}}^{2}}\mathbf{y}^{\top}X^{\top}\green\underbrace{\black\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)}_{\green I}\black\mathbf{w} \\ 
\left.\left.-\dfrac{1}{\sigma_{\mathrm{n}}^{2}}\mathbf{w}^{\top}\green\underbrace{\black \left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1}}_{\green I}\black X\mathbf{y}+\dfrac{\mathbf{y}^{\top}\mathbf{y}}{\sigma_{\mathrm{n}}^{2}}\right]\right]
\end{multline}
Define $\overline{\mathbf{w}} := \dfrac{1}{\sigma_{\mathrm{n}}^{2}}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1}X\mathbf{y}$ and by noting that $XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}$ is symmetric, we have $\overline{\mathbf{w}}^{\top}=\dfrac{1}{\sigma_{\mathrm{n}}^{2}}\mathbf{y}^{\top}X^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1}$. Then we can see that
\begin{multline}
p\left(\mathbf{w}\middle|\mathbf{y},X\right) \propto \exp\left[-\dfrac{1}{2}\left[\mathbf{w}^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\mathbf{w}\right.\right. \\
-\green\underbrace{\black\dfrac{1}{\sigma_{\mathrm{n}}^{2}}\mathbf{y}^{\top}X^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1}}_{\green \overline{\mathbf{w}}^{\top}}\black\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\mathbf{w} \\ 
\left.\left.-\mathbf{w}^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\green\underbrace{\black \dfrac{1}{\sigma_{\mathrm{n}}^{2}}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1} X\mathbf{y}}_{\green \overline{\mathbf{w}}}\black +\dfrac{\mathbf{y}^{\top}\mathbf{y}}{\sigma_{\mathrm{n}}^{2}}\right]\right]
\end{multline}
and we can finish completing the square by writing
\begin{align}
p\left(\mathbf{w}\middle|\mathbf{y},X\right) &\begin{multlined}\propto\exp\left[-\dfrac{1}{2}\left[\left(\mathbf{w}-\overline{\mathbf{w}}\right)^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\left(\mathbf{w}-\overline{\mathbf{w}}\right) \right.\right. \\
\left.\left. -\overline{\mathbf{w}}^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\overline{\mathbf{w}}+\dfrac{\mathbf{y}^{\top}\mathbf{y}}{\sigma_{\mathrm{n}}^{2}}\right]\right]
\end{multlined} \\
&\begin{multlined} \propto\exp\left[-\dfrac{1}{2}\left[\left(\mathbf{w}-\overline{\mathbf{w}}\right)^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\left(\mathbf{w}-\overline{\mathbf{w}}\right) \right.\right. \\
\left.\left. -\dfrac{1}{\sigma_{\mathrm{n}}^{4}}\mathbf{y}^{\top}X^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1}X\mathbf{y}+\dfrac{\mathbf{y}^{\top}\mathbf{y}}{\sigma_{\mathrm{n}}^{2}}\right]\right]
\end{multlined} \\
&\propto\exp\left[-\dfrac{1}{2}\left(\mathbf{w}-\overline{\mathbf{w}}\right)^{\top}\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)\left(\mathbf{w}-\overline{\mathbf{w}}\right)\right]
\end{align}
where the terms not involving $\mathbf{w}$ have been ignored due to proportionality. Hence it can be deduced
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y},X\right)=\mathcal{N}\left(\overline{\mathbf{w}},\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)^{-1}\right)
\end{equation}
For ease of notation, let $A=\left(\dfrac{XX^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}\right)$ so that $\overline{\mathbf{w}}=\dfrac{1}{\sigma_{\mathrm{n}}^{2}}A^{-1}X\mathbf{y}$ and
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y},X\right)=\mathcal{N}\left(\dfrac{1}{\sigma_{\mathrm{n}}^{2}}A^{-1}X\mathbf{y},A^{-1}\right)
\end{equation}
This posterior distribution represents a distribution over the possible linear models. To obtain a predictive distribution $f_{*} := f\left(x_{*}\right)$ at a test point $x_{*}$, we can integrate over the posterior:
\begin{equation}
p\left(f_{*}\middle|x_{*},X,\mathbf{y}\right)=\int p\left(f_{*}\middle|x_{*},\mathbf{w},X,\mathbf{y}\right)p\left(\mathbf{w}\middle|x_{*},X,\mathbf{y}\right)d\mathbf{w}
\end{equation}
By the i.i.d. assumption, $p\left(f_{*}\middle|x_{*},\mathbf{w},X,\mathbf{y}\right) = p\left(f_{*}\middle|x_{*},\mathbf{w}\right)$ and also by independence of the weights and the test point, $p\left(\mathbf{w}\middle|x_{*},X,\mathbf{y}\right) = p\left(\mathbf{w}\middle|X,\mathbf{y}\right)$ which gives
\begin{equation}
p\left(f_{*}\middle|x_{*},X,\mathbf{y}\right)= \int p\left(f_{*}\middle|x_{*},\mathbf{w}\right)p\left(\mathbf{w}\middle|X,\mathbf{y}\right)d\mathbf{w}
\end{equation}
where $p\left(\mathbf{w}\middle|X,\mathbf{y}\right)$ as above and $p\left(f_{*}\middle|x_{*},\mathbf{w}\right) = \mathcal{N}\left(x_{*}^{\top}\mathbf{w}, \sigma_{\mathrm{n}}^{2}\right)$. But rather than evaluating the integral, note that $f_{*}$ is just a linear transformation of the random variable $\mathbf{w}$, given by $f_{*}=x_{*}^{\top}\mathbf{w}$. By using the property of affine transformations of Gaussian random variables, this gives
\begin{equation}
p\left(f_{*}\middle|x_{*},X,\mathbf{y}\right)=\mathcal{N}\left(\dfrac{1}{\sigma_{\mathrm{n}}^{2}}x_{*}^{\top}A^{-1}X\mathbf{y},x_{*}^{\top}A^{-1}x_{*}\right)
\end{equation}

\subsubsection{Bayesian Basis Function Linear Regression}

A regression model can be linear in a basis of $x$, denoted $\boldsymbol{\phi}\left(x\right)$, given by
\begin{equation}
f\left(x\right)=\boldsymbol{\phi}\left(x\right)^{\top}\mathbf{w}
\end{equation}
Defining the data matrix aggregated column-wise $\boldsymbol{\Phi}=\begin{bmatrix}\boldsymbol{\phi}\left(x_{1}\right) & \boldsymbol{\phi}\left(x_{2}\right) & \dots & \boldsymbol{\phi}\left(x_{n}\right)\end{bmatrix}$, the predictive distribution for a test point $x_{*}$ is now given by
\begin{equation}
p\left(f_{*}\middle|x_{*},X,\mathbf{y}\right)=\mathcal{N}\left(\dfrac{1}{\sigma_{\mathrm{n}}^{2}}\boldsymbol{\phi}\left(x_{*}\right)^{\top}A^{-1}\boldsymbol{\Phi}\mathbf{y},\boldsymbol{\phi}\left(x_{*}\right)^{\top}A^{-1}\boldsymbol{\phi}\left(x_{*}\right)\right)
\end{equation}
where $A=\dfrac{\boldsymbol{\Phi}\boldsymbol{\Phi}^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}}{\sigma_{\mathrm{n}}^{2}}$. Denote $\boldsymbol{\phi}_{*} := \boldsymbol{\phi}\left(x_{*}\right)$ for convenience. We can show that the mean can be written as
\begin{equation}
\dfrac{1}{\sigma_{\mathrm{n}}^{2}}\boldsymbol{\phi}_{*}^{\top}A^{-1}\boldsymbol{\Phi}\mathbf{y}=\boldsymbol{\phi}_{*}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\left(K+\sigma_{\mathrm{n}}^{2}I\right)^{-1}\mathbf{y}
\end{equation}
where $K=\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}$. To do this, it suffices to show that
\begin{equation}
\dfrac{A^{-1}\boldsymbol{\Phi}}{\sigma_{\mathrm{n}}^{2}}=\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\left(K+\sigma_{\mathrm{n}}^{2}I\right)^{-1}
\end{equation}
Using the definition of $A$:
\begin{gather}
\left(\boldsymbol{\Phi}\boldsymbol{\Phi}^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}\right)^{-1}\boldsymbol{\Phi}=\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\left(K+\sigma_{\mathrm{n}}^{2}I\right)^{-1} \\
\boldsymbol{\Phi}\left(K+\sigma_{\mathrm{n}}^{2}I\right)=\left(\boldsymbol{\Phi}\boldsymbol{\Phi}^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}\right)\Sigma_{\mathrm{p}}\boldsymbol{\Phi} \\
\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}+\sigma_{\mathrm{n}}^{2}I\right)=\left(\boldsymbol{\Phi}\boldsymbol{\Phi}^{\top}+\sigma_{\mathrm{n}}^{2}\Sigma_{\mathrm{p}}^{-1}\right)\Sigma_{\mathrm{p}}\boldsymbol{\Phi} \\
\boldsymbol{\Phi}\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}+\sigma_{\mathrm{n}}^{2}\boldsymbol{\Phi}=\boldsymbol{\Phi}\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}+\sigma_{\mathrm{n}}^{2}\boldsymbol{\Phi}
\end{gather}
as required. We also find an alternative form for the predictive covariance. Again using the definition of $A$:
\begin{equation}
\boldsymbol{\phi}_{*}^{\top}A^{-1}\boldsymbol{\phi}_{*}=\boldsymbol{\phi}_{*}^{\top}\left(\Sigma_{\mathrm{p}}^{-1}+\boldsymbol{\Phi}\sigma_{\mathrm{n}}^{-2}I\boldsymbol{\Phi}^{\top}\right)^{-1}\boldsymbol{\phi}_{*}
\end{equation}
Now apply the matrix inversion lemma:
\begin{equation}
\left(Z+UWV^{\top}\right)^{-1}=Z^{-1}-Z^{-1}U\left(W^{-1}+V^{\top}Z^{-1}U\right)^{-1}V^{\top}Z^{-1}
\end{equation}
using
\begin{gather}
Z=\Sigma_{\mathrm{p}}^{-1} \\
U=\boldsymbol{\Phi} \\
W=\sigma_{\mathrm{n}}^{-2}I \\
V=\boldsymbol{\Phi}
\end{gather}
to give
\begin{equation}
A^{-1}=\Sigma_{\mathrm{p}}-\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\left(\sigma_{\mathrm{n}}I+\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}
\end{equation}
Then use the definition of $K$:
\begin{equation}
A^{-1}=\Sigma_{\mathrm{p}}-\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\left(K+\sigma_{\mathrm{n}}^{2}I\right)^{-1}\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}
\end{equation}
Hence an alternative form for the predictive covariance is
\begin{equation}
\boldsymbol{\phi}_{*}^{\top}A^{-1}\boldsymbol{\phi}_{*}=\boldsymbol{\phi}_{*}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\phi}_{*}-\boldsymbol{\phi}_{*}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\left(K+\sigma_{\mathrm{n}}^{2}I\right)^{-1}\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\phi}_{*}
\end{equation}
The predictive distribution has the new form
\begin{equation}
p\left(f_{*}\middle|x_{*},X,\mathbf{y}\right)=\mathcal{N}\left(\blue\boldsymbol{\phi}_{*}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\black\left(\blue K\black +\sigma_{\mathrm{n}}^{2}I\right)^{-1}\mathbf{y}, \blue\boldsymbol{\phi}_{*}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\phi}_{*}\black-\blue\boldsymbol{\phi}_{*}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\Phi}\black\left(\blue K\black+\sigma_{\mathrm{n}}^{2}I\right)^{-1}\blue\boldsymbol{\Phi}^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\phi}_{*}\black\right)
\end{equation}
Note that the terms in \blue blue \black will be of the form $\boldsymbol{\phi}\left(x\right)^{\top}\Sigma_{\mathrm{p}}\boldsymbol{\phi}\left(x'\right)$ where $x$ and $x'$ can be either from the test set or training set.

\subsection{Type II Maximum Likelihood}

\subsection{Hierarchical Bayes Modelling \cite{Rasmussen2006}}

Consider a model over parameters $\mathbf{w}$ (eg. weights in a neural network), hyperparameters $\boldsymbol{\theta}$ (eg. regularisation in a cost function) and a discrete set of structures $\mathcal{H}_{i}$ (eg. number of layers and nodes). We can conduct inference on these using a hierarchical approach. On the lowest level (level 1) is inference over the weights, given inputs $X$, outputs $\mathbf{y}$ and $\boldsymbol{\theta}$, $\mathcal{H}_{i}$.
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|\mathbf{w}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}
\end{equation}
As a common assumption is that $\mathbf{w}$ and $X$ are independent
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|\mathbf{w}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|\boldsymbol{\theta}, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}
\end{equation}
A property of hyperparameters is that they can be selected before we gather training data and before we begin training. It follows that $\boldsymbol{\theta}$ should be independent of $\mathbf{y}$, giving
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|\mathbf{w}, X, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|\boldsymbol{\theta}, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}
\end{equation}
In level 2, we conduct inference over $\boldsymbol{\theta}$.
\begin{equation}
p\left(\boldsymbol{\theta}\middle|\mathbf{y}, X, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) p\left(\boldsymbol{\theta}\middle| X, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)}
\end{equation}
Again, we argue that $\boldsymbol{\theta}$ should be independent of the data $X$, so
\begin{equation}
p\left(\boldsymbol{\theta}\middle|\mathbf{y}, X, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) p\left(\boldsymbol{\theta}\middle| \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)}
\end{equation}
Note that the marginal likelihood in level 1 can be expressed as in integral over $\mathbf{w}$:
\begin{equation}
p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \int p\left(\mathbf{y}\middle|\mathbf{w}, X, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|\boldsymbol{\theta}, \mathcal{H}_{i}\right) d\mathbf{w}
\end{equation}
Notice that this level 1 marginal likelihood is the same as the level 2 likelihood. In practical terms, we will conduct inference on $\boldsymbol{\theta}$ by integrating over $\mathbf{w}$ using some prior for $\mathbf{w}$ to find the likelihood. We also require a hyperprior $p\left(\boldsymbol{\theta}\middle|\mathcal{H}_{i}\right)$ for $\boldsymbol{\theta}$. Once this is done, then we may perform inference on $\mathbf{w}$. \\

In level 3, inference is performed over the model structures. As $\mathcal{H}_{i}$ was defined to be discrete, we use $P\left(\cdot\right)$ to denote the probability mass function (in contrast to $p\left(\cdot\right)$ for the probability density function). Bayes' theorem for the posterior of $\mathcal{H}_{i}$ gives
\begin{equation}
P\left(\mathcal{H}_{i}\middle|\mathbf{y}, X\right) = \dfrac{p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)P\left(\mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X\right)}
\end{equation}
The likelihood function $p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)$ is the same as the marginal likelihood for level 2, and can be computed using the integral
\begin{equation}
p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right) = \int p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) p\left(\boldsymbol{\theta}\middle| \mathcal{H}_{i}\right) d\boldsymbol{\theta}
\end{equation}
As $P\left(\mathcal{H}_{i}\right)$ is a probability mass function, the level 3 marginal likelihood is computed using the sum
\begin{equation}
p\left(\mathbf{y}\middle|X\right) = \sum_{i}p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)P\left(\mathcal{H}_{i}\right)
\end{equation}
ie. it is a weighted average of some probability density functions, and acts as a normalising constant for the posterior of $\mathcal{H}_{i}$.

\section{Berstein-von Mises Theorem}

\section{Bayesian Classifiers}

\section{Subjective Probability \cite{Schervish1995}}

\chapter{Markov Processes}

\section{Markov Chains}

\subsection{Countable State Discrete Time Markov Chains}

\subsection{Countable State Continuous Time Markov Chains}

\subsection{Uncountable State Discrete Time Markov Chains}

\subsection{Uncountable State Continuous Time Markov Chains}

\subsection{Gauss-Markov Processes}

\section{Markov Networks}

\section{Hidden Markov Models}

\subsection{Baum-Welch Algorithm}

\section{Markov Decision Processes}

\subsection{Partially Observable Markov Decision Processes}

\section{Inference of Markov Processes}

\chapter{Measure Theoretic Probability}

\section{Probability Spaces}

\subsection{Concepts in Probability Spaces}

\subsubsection{Elementary Events}
An elementary event is an event which contains only a single outcome in the sample space. For example, in the experiment that a coin is flipped twice, the outcomes are $\left\{H, T\right\}$, $\left\{T, H\right\}$, $\left\{H, H\right\}$ and $\left\{T, T\right\}$. The event `heads followed by tails' is an elementary event because there is only a single outcome associated to it: $\left\{H, T\right\}$. However, the event `at least one heads' is not an elementary event because there are three outcomes associated to it: $\left\{H, T\right\}$, $\left\{T, H\right\}$ and $\left\{H, H\right\}$.

\subsubsection{Power Sets}
The power set of a set $S$ is the set of all subsets of $S$, including the empty set and $S$ itself. If $S$ is a finite set with cardinality $\left|S\right| = n$, then the number of subsets of $S$ is $2^{n}$ (related to the binomial theorem). This motivates the notation for the power set of $S$ as $2^{S}$. \\

The power set of all functions from $Y$ to $X$ can be denoted $X^{Y}$.

\subsubsection{Countable Sets}
A countable set $S$ has the same cardinality $\left|S\right|$ as some subset of the natural numbers. Intuitively speaking, a set is countable we can in some way assign numbers $1, 2, 3, \dots$ to each element in $S$ (ie. we can `enumerate' the elements of $S$). A finite set will always be countable, but some infinite sets can still be countably infinite. For example, the set of natural numbers itself is trivially countably infinite. We can also enumerate through the set of positive rational numbers in the following way:
\begin{equation*}
\begin{array}{ccccccccc}
1/1 &  & 1/2 & \rightarrow & 1/3 &  & 1/4 & \rightarrow & 1/5\\
\downarrow & \nearrow &  & \swarrow &  & \nearrow &  & \swarrow\\
2/1 &  & 2/2 &  & 2/3 &  & 2/4 &  & \iddots\\
 & \swarrow &  & \nearrow &  & \swarrow &  & \nearrow\\
3/1 &  & 3/2 &  & 3/3 &  & 3/4\\
\downarrow & \nearrow &  & \swarrow &  & \nearrow\\
4/1 &  & 4/2 &  & 4/3\\
 & \swarrow &  & \nearrow\\
5/1 &  & 5/2\\
\downarrow & \nearrow\\
6/1
\end{array}
\end{equation*}

Similarly, the set of all rational numbers is also countable. One possible enumeration is that we can imagine starting at 0, and then enumerating back a forth between a second `layer' of a grid of negative rational numbers behind the grid of positive rational numbers. \\

An example of an uncountable set is the set of real numbers.

\subsubsection{Measurable Sets}
An example of sets which are not measurable are Vitali sets. A Vitali set $V$ is a subset of the interval $\left[0, 1\right]$ such that for each real number $r \in \mathbb{R}$, there is exactly one number $v \in V$ such that $v - r$ is a rational number. A numerical example of this is with the real number $10 + 1/\sqrt{2}$, we can choose $1/\sqrt{2} \in \left[0, 1\right]$ such that $1/\sqrt{2} - \left(10 - 1/\sqrt{2}\right)$ is rational. There are uncountably many Vitali sets, but all Vitali sets will satisfy this property. \\

We can show that Vitali sets are unmeasureable in the following way. Define the enumeration of all rational numbers in $\left[-1, 1\right]$ to be $q_{1}, q_{2}, \dots$. Define the translated Vitali sets $V_{k} := V + q_{k} = \left\{v + q_{k}: v \in V\right\}$ for $k = 1, 2, \dots$ so that $v_{k} = v + q_{k} \in V_{k}$. These sets must be disjoint in order to satisfy the definition that there is exactly one $v \in V$ for each real number. If shifting $V$ by $q_{k}$ causes $V$ and $V_{k}$ to have elements in common, then it implies $V$ is not a Vitali set. Another way to state this is that there is no gap between any two elements in $V$ equal to a rational number. If this were the case, then there could be more than one $v \in V$ that could make $v - r$ rational. Hence shifting $V$ by a rational number $q_{k}$ will ensure $V_{k}$ and $V$ will be disjoint. We can write the following inclusion
\begin{equation}
\bigcup_{k}V_{k} \subseteq \left[-1, 2\right]
\end{equation}
since $v \in \left[0, 1\right]$ and $q_{k} \in \left[-1, 1\right]$, then $-1 \leq v + q_{k} \leq 2$. Furthermore, consider any real number $v_{i} \in \left[0, 1\right]$. Then by definition there will be exactly one $v \in \left[0, 1\right]$ such that $v - v_{i} = -q_{i}$, where $q_{i} \in \left[-1,1\right]$ since $-1 \leq v - v_{i} \leq 1 \Rightarrow -1 \leq q_{i} \leq 1$. Therefore $v_{i} \in V_{i}$ and we can write
\begin{equation}
\left[0, 1\right] \subseteq \bigcup_{k}V_{k} \subseteq \left[-1, 2\right]
\end{equation}
Suppose we can take the Lebesque measure $\lambda\left(\cdot\right)$ of these inclusions.
\begin{equation}
1 \leq \sum_{k = 1}^{\infty}\lambda\left(V_{k}\right) \leq 3
\end{equation}
Since the Lebesque measure is translation invariant, we have $\lambda\left(V_{k}\right) = \lambda\left(V\right)$ (a constant) and then
\begin{equation}
1 \leq \sum_{k = 1}^{\infty}\lambda\left(V\right) \leq 3
\end{equation}
This results in a contradiction. The infinite sum must either be zero or infinity, but neither is between 1 and 3. So a Vitali set is not Lebesgue measurable.

\subsubsection{Borel Sets}

A Borel set is a set in topological space $\Omega$ (eg. sample space) that can be formed from the operations of countable union, countable intersection, and relative complement (ie. the relative complement of $A$ in $B$ is denoted $A\setminus B$) of open (or equivalently, of closed sets) in $\Omega$.

\subsubsection{$\sigma$-algebras}

The $\sigma$-algebra of a set $\Omega$ is a collection of subsets of $\Omega$ which:
\begin{itemize}
\item includes the empty subset.
\item is closed under complement (ie. the complement of a member of the $\sigma$-algebra is also a member of the $\sigma$-algebra).
\item is closed under countable unions (ie. the countable union of members of the $\sigma$-algebra is also a member of the $\sigma$-algebra).
\item is closed under countable intersections (ie. the countable intersection of members of the $\sigma$-algebra is also a member of the $\sigma$-algebra).
\end{itemize}
In general, the $\sigma$-algebra of $\Omega$ is a subset of the power set of $\Omega$.

\subsubsection{Sub-$\sigma$-algebras}

Suppose $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$. Then another $\sigma$-algebra $\mathcal{F}_{1}$ on $\Omega$ is said to be a sub-$\sigma$-algebra of $\mathcal{F}$ if $\mathcal{F}_{1} \subseteq \mathcal{F}$.

\subsubsection{Borel $\sigma$-algebras}

The Borel $\sigma$-algebra of a set $\Omega$ is the collection of all Borel sets of $\Omega$. The Borel $\sigma$-algebra gives the smallest $\sigma$-algebra containing all open sets (or equivalently, all closed sets) of $\Omega$.
\begin{itemize}
\item In the case where $\Omega$ is a countable set, then the power set is identical to the Borel $\sigma$-algebra.
\item If $\Omega$ is the real line $\mathbb{R}$, then the Borel $\sigma$-algebra includes all `reasonable' (ie. measurable) open and closed intervals, as well as their countable union/intersection and relative complement. 
\end{itemize}

\subsubsection{Boundary Sets}

The boundary set of $\mathcal{B}$ is the set of points in the closure of $\mathcal{B}$ but not in the interior of $\mathcal{B}$. The boundary set is denoted $\partial\mathcal{B}$.

\subsubsection{Continuity Sets}

For a random variable (or vector) $X$, a Borel set $\mathcal{B}$ is called a continuity set if
\begin{equation}
\mathbb{P}\left(X\in \partial\mathcal{B}\right) = 0
\end{equation}
For example, if $X$ is a Bernoulli random variable, then $\left[0, 1\right]$ nor $\left(0, 1\right)$ are considered continuity sets, however $\left(-1, -0.5\right)$, $\left[0.1, 0.9\right)$ and $\left[1.1, \infty\right)$ would be considered continuity sets. A continuity set can be made of any points at which the cumulative distribution function of $X$ is continuous.

\subsection{Probability Triple $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$}
In measure theoretic probability, a probability space is a measure space denoted with the three-tuple $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ where $\Omega$ is the sample space, $\mathcal{F}$ is the event space and $\mathbb{P}$ is a probability measure. \\

The set $\Omega$ contains each elementary event. The event space $\mathcal{F}$ is formed by taking a $\sigma$-algebra of $\Omega$. A typical choice of $\sigma$-algebra is the Borel $\sigma$-algebra. The function $\mathbb{P}: \mathcal{F} \to \left[0, 1\right]$ is called a probability measure, which maps events to probabilities.

\subsubsection{Probability Axioms}

Let $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ be a probability space with sample space $\Omega$, event space $\mathcal{F}$ and probability measure $\mathbb{P}$. The axioms of probability (sometimes known as Kolgomorov's axioms) are:
\begin{enumerate}
\item The probability of an event $E \in \mathcal{F}$ is a nonnegative real number
\begin{equation}
0 \leq \mathbb{P}\left(E\right) \in \mathbb{R}
\end{equation}
for all $E \in \mathcal{F}$.
\item The probability that at least one of the elementary events in the entire sample space will occur is 1.
\begin{equation}
\mathbb{P}\left(\Omega\right) = 1
\end{equation}
\item Any countable sequence of disjoint (mutually exclusive) events $E_{1}$, $E_{2}$, $\dots$ satisfies
\begin{equation}
\mathbb{P}\left(\bigcup_{i = 1}^{\infty}E_{i}\right) = \sum_{i = 1}^{\infty}\mathbb{P}\left(E_{i}\right)
\end{equation}
\end{enumerate}
From the axioms, we can deduce further properties
\begin{itemize}
\item If $A \subseteq B$, the we have the monotonicity property
\begin{equation}
\mathbb{P}\left(A\right) \leq \mathbb{P}\left(B\right)
\end{equation}
This can be seen by defining events $E_{1} = A$, $E_{2} = B\setminus A$ where $A\subseteq B$, and $E_{i} = \O$ for all $i\geq 3$. These sets are all disjoint (we may alternatively define disjoint sets to be sets whose intersection is the empty set). Additionally, we have $\bigcup_{i = 1}^{\infty}E_{i} = B$. By the third axiom
\begin{align}
\sum_{i = 1}^{\infty}\mathbb{P}\left(E_{i}\right) &= \mathbb{P}\left(A\right) + \mathbb{P}\left(B\setminus A\right) + \sum_{i = 3}^{\infty}\mathbb{P}\left(\O\right) \\
&= \mathbb{P}\left(B\right)
\end{align}
The terms $\mathbb{P}\left(B\setminus A\right)$ and $\sum_{i = 3}^{\infty}\mathbb{P}\left(\O\right)$ are non-negative, hence $\mathbb{P}\left(A\right) \leq \mathbb{P}\left(B\right)$.
\item The probability of the empty set is zero.
\begin{equation}
\mathbb{P}\left(\O\right) = 0
\end{equation}
In deriving the above, we saw that the sum in $\mathbb{P}\left(A\right) + \mathbb{P}\left(B\setminus A\right) + \sum_{i = 3}^{\infty}\mathbb{P}\left(\O\right)$ was convergent. Therefore it must be that $\mathbb{P}\left(\O\right) = 0$, otherwise the sum would be infinite.
\item The numeric bound applies to any event $E \in \mathcal{F}$:
\begin{equation}
0 \leq \mathbb{P}\left(E\right) \leq 1
\end{equation}
This can be shown by applying the non-negativity axiom and the monotonicity property to any subset of $\Omega$ since $\mathbb{P}\left(\Omega\right) = 1$.
\end{itemize}

\subsubsection{Almost Sure Events}

An event $E\in\mathcal{F}$ is said to happen almost surely if the event of $E$ not happening has probability measure zero, that is $\mathbb{P}\left(\overline{E}\right) = 0$. Alternatively, we can say that $E$ has probability measure $\mathbb{P}\left(E\right) = 1$.

\subsection{Lebesgue Integration}

\subsubsection{Measurable Functions}

A real valued function $f: \Omega \to \mathbb{R}$ is said to be measurable if for every $B\in\mathcal{B}$ (where $\mathcal{B}$ is a $\sigma$-algebra of $\mathbb{R}$), the preimage of $B$ under $f$ is an element of the $\sigma$-algebra of $\Omega$, denoted $\mathcal{F}$. Explicitly,
\begin{equation}
f^{-1}\left(B\right) := \left\{x \in \Omega \middle| f\left(x\right) \in B\right\} \in \mathcal{F}
\end{equation}
for all $B \in\mathcal{B}$. Measurability is subject to the choice of $\sigma$-algebras $\mathcal{B}$ and $\mathcal{F}$, but we normally take these to be the Borel $\sigma$-algebras. An example of a non-measurable function is an indicator function for a non-measurable set.

\subsubsection{Lebesgue Integral over Probability Spaces}

In a probability space $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$, the Lebesgue integral of a measurable function $f\left(x\right)$ over a measurable subset $E$ of $\Omega$ with respect to the measure $\mathbb{P}$ may be denoted as
\begin{equation}
\int_{E}f\left(x\right)\mathrm{d}\mathbb{P}\left(x\right)
\end{equation}

\subsubsection{Simple Functions}

Given a probability space $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$, suppose there is an indicator function $\mathbb{I}_{E}\left(\omega\right): \Omega \to \mathbb{R}$ where $E$ is a measurable subset of $\Omega$. That is,
\begin{equation}
\mathbb{I}_{E}\left(\omega\right) = \begin{cases} 1, & \omega \in E \\ 0, & \mathrm{otherwise}\end{cases}
\end{equation}
To decide what value the Lebesgue integral of this function should take, the only reasonable choice is to set the integral as
\begin{equation}
\int_{\Omega}\mathbb{I}_{E}\left(\omega\right)d\mathbb{P}\left(\omega\right) = \mathbb{P}\left(E\right)
\end{equation}
A simple function is a finite linear combination of indicator functions for sets $E_{1}, \dots, E_{n}$ that are a partition of $\Omega$ is denoted
\begin{equation}
f_{n}\left(\omega\right) = \sum_{i = 1}^{n}a_{i}\mathbb{I}_{E_{i}}\left(\omega\right)
\end{equation}
where $a_{i}$ are non-negative coefficients. The integrals for such functions can be defined as
\begin{equation}
\int_{\Omega}f_{n}\left(\omega\right)d\mathbb{P}\left(\omega\right) = \sum_{i}^{n}a_{i}\mathbb{P}\left(E_{i}\right)
\end{equation}

\subsubsection{Monotone Convergence Theorem}

For non-negative functions which cannot be simply written as a finite linear combination of indicator functions, one can take the limit of a sequence of simple functions as $n \to \infty$. The Lebesgue integral of such functions can then be defined as
\begin{equation}
\int_{\Omega}f\left(\omega\right)d\mathbb{P}\left(\omega\right) = \sup_{f_{n}\left(\omega\right) = \sum_{i = 1}^{n}a_{i}\mathbb{I}_{E_{i}}\left(\omega\right)}\left\{\int_{\Omega}f_{n}\left(\omega\right)d\mathbb{R}\left(\omega\right)\middle|0\leq f_{n}\left(\omega\right) \leq f\left(\omega\right)\forall \omega \in \Omega\right\}
\end{equation}
ie. take the Lebesgue integral of the `largest' simple function bounded by $f\left(\omega\right)$. The monotone convergence theorem asserts that if there is a non-decreasing sequence of non-negative functions (ie. $0 \leq f_{n}\left(\omega\right) \leq f_{n + 1}\left(\omega\right)$ for all $\omega \in \Omega$ and for all $n \geq 0$), and if $f_{n}\left(\omega\right)$ converges to $f\left(\omega\right)$ pointwise:
\begin{equation}
\lim_{n\to\infty}f_{n}\left(\omega\right) = f\left(\omega\right)
\end{equation}
for all $\omega \in \Omega$, then the limit of the Lebesgue integral of $f_{n}\left(\omega\right)$ as $n \to \infty$ is the same as the definition from above:
\begin{equation}
\lim_{n\to\infty}\int_{\Omega}f_{n}\left(\omega\right)d\mathbb{P}\left(\omega\right) = \int_{\Omega}f\left(\omega\right)d\mathbb{P}\left(\omega\right)
\end{equation}
Note that the sequence of simple functions as $n\to\infty$ is non-decreasing by virtue of the coefficients $a_{i}$ being defined to be non-negative.

\subsubsection{Lebesgue Integral of Signed Functions}

The Lebesgue integral can be generalised to functions which can take on negative values. Define $f^{+}\left(\omega\right) = \sup\left\{f\left(\omega\right), 0\right\}$ (ie. the positive part) and $f^{-}\left(\omega\right) = \sup\left\{-f\left(\omega\right), 0\right\}$ (ie. the negative part). Note that both $f^{+}\left(\omega\right) \geq 0$ and $f^{-}\left(\omega\right) \geq 0$. Then the Lebesgue integral can be defined as
\begin{equation}
\int_{\Omega}f\left(\omega\right)d\mathbb{P}\left(\omega\right) = \int_{\Omega}f^{+}\left(\omega\right)d\mathbb{P}\left(\omega\right) - \int_{\Omega}f^{-}\left(\omega\right)d\mathbb{P}\left(\omega\right)
\end{equation}
whenever at least one of the integrals is finite. Note that the Lebesque integral generalises the Riemann integral, so the Lebesgue integral of a function is the same as the Riemann integral, if the Riemann integral is defined/exists.

\subsection{Random Variables}

In measure theoretic probability, a real valued random variable $X$ is defined as a measurable function $X: \Omega \to \mathbb{R}$. For each $\omega \in \Omega$, the function $X\left(\omega\right)$ assigns a number to each outcome in the sample space. For example, in an experiment involving a sequence of coin tosses, let the sample space be $\Omega = \left\{H, T, HH, HT, TH, TT\right\}$. We can define the random variable $X\left(\omega\right)$ to be the number of heads tossed. In that case, we have $X\left(H\right) = 1$, $X\left(T\right) = 0$, $X\left(HH\right) = 2$, $X\left(HT\right) = 1$, $X\left(TH\right) = 1$, $X\left(TT\right) = 0$. \\

The function $X^{-1}: \mathcal{B} \to \mathcal{F}$ is a mapping from the Borel $\sigma$-algebra of $\mathbb{R}$ to a $\sigma$-algebra of $\Omega$. It is defined to be
\begin{equation}
X^{-1}\left(B\right) = \left\{\omega\in\Omega\middle|X\left(\omega\right)\in B\right\}
\end{equation}
That is, given a Borel set $B$, the function finds all the elementary events $\omega\in\Omega$ which lead to the random variable $X$ being in $B$. For the example sample space $\Omega$ above, we can say that $X^{-1}\left(\left\{2\right\}\right) = \left\{HH\right\}$ and $X^{-1}\left(\left\{1\right\}\right) = \left\{H, HT, TH\right\}$.

\subsubsection{$\sigma$-algebra Generated by a Random Variable}

Let $X: \Omega\to\mathbb{R}$ be a random variable. Denote by $\mathcal{B}$ the Borel $\sigma$-algebra of $\mathbb{R}$. Then the $\sigma$-algebra generated by $X$, denoted $\sigma\left(X\right)$, is defined as
\begin{equation}
\sigma\left(X\right) = \left\{X^{-1}\left(B\right)\middle|B\in\mathcal{B}\right\}
\end{equation}
That is, for every Borel set of $\mathbb{R}$ we find the subset of $\Omega$ which leads to $X$ being in the Borel set. The collection of all these subsets is known as the $\sigma$-algebra generated by $X$. This will also be known as the smallest $\sigma$-algebra for which $X$ is measurable.

\subsection{Expectation}

The expectation of a random variable $X\left(\omega\right)$ is defined using the Lebesgue integral
\begin{equation}
\mathbb{E}\left[X\right] = \int_{\Omega}X\left(\omega\right)\mathrm{d}\mathbb{P}\left(\omega\right)
\end{equation}

\section{Borel-Cantelli Lemma}

\subsection{Limit Supremum of a Sequence of Events}

Let $E_{1}, E_{2}, \dots$ be a sequence of events in some probability space. The limit supremum of the sequence of events is the set of all outcomes where the event $E_{n}$ occurs infinitely many times in the infinite sequence of events. That is,
\begin{align}
\limsup_{n\to\infty}E_{n} &= \bigcup_{k = 1}^{\infty}E_{k} \cap \bigcup_{k = 2}^{\infty}E_{k} \cap \dots \\
&= \bigcap_{n = 1}^{\infty}\bigcup_{k = n}^{\infty}E_{k}
\end{align}

\subsection{Statement of Lemma in Probability Spaces}

For a sequence of events $E_{1}, E_{2}, \dots$ in a probability space, if the sum of probabilities is finite
\begin{equation}
\sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) < \infty
\end{equation}
then the probability the event occurs infinitely often is zero:
\begin{equation}
\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right) = 0
\end{equation}
\begin{proof}
Since $\sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) < \infty$, then the series of probabilities converges, meaning $\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right) \to 0$ as $N \to \infty$. Hence taking the greatest lower bound of the series gives
\begin{equation}
\inf_{N \geq 1}\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right) = 0
\end{equation}
Evaluating $\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right)$ yields
\begin{equation}
\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right) = \mathbb{P}\left(\bigcap_{N = 1}^{\infty}\bigcup_{n = N}^{\infty}E_{n}\right)
\end{equation}
Note that by the chain rule of probability, $\operatorname{Pr}\left(A \cap B\right) = \operatorname{Pr}\left(B \middle| A\right)\operatorname{Pr}\left(A\right) \leq \operatorname{Pr}\left(A\right)$ so we can use the generalisation this argument to state
\begin{align}
\mathbb{P}\left(\bigcap_{N = 1}^{\infty}\bigcup_{n = N}^{\infty}E_{n}\right) &\leq \inf_{N \geq 1}\mathbb{P}\left(\bigcup_{n = N}^{\infty}E_{n}\right) \\
&\leq \inf_{N \geq 1}\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right) = 0
\end{align}
where the latter inequality comes from the generalisation of the concept $\operatorname{Pr}\left(A \cup B\right) \leq \operatorname{Pr}\left(A\right) + \operatorname{Pr}\left(B\right)$ (this is called Boole's inequality). Hence $\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right) = 0$
\end{proof}
An alternative proof is provided:
\begin{proof}
Let $\mathbb{I}_{n}$ denote the indicator function for the event $E_{n}$. Then by the linearity of expectation
\begin{align}
\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right] &= \sum_{n = 1}^{\infty}\mathbb{E}\left[\mathbb{I}_{n}\right] \\
&= \sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) < \infty
\end{align}
Suppose there is a non-zero probability that $E_{n}$ occurs infinitely often, meaning $\mathbb{P}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right) > 0$. However if we attempt to find $\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right]$ by Lebesgue integration this gives
\begin{align}
\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right] &= \int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P} + \int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} \neq \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P} \\
&\geq \int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P}
\end{align}
Since there is a non-zero probability that $\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty$, then the integral is infinite, ie.
\begin{equation}
\int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P} = \infty
\end{equation}
This results in $\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right] \geq \infty$, which is a contradiction.
\end{proof}

\subsection{Converse Borel-Cantelli Lemma}

We have the converse result that if the events $E_{1}, E_{2}, \dots$ are independent and the sum of the probabilities diverges to infinity, ie.
\begin{equation}
\sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) = \infty
\end{equation}
then then probability that $E_{n}$ occurs infinitely many times is $1$, ie.
\begin{equation}
\mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = 1
\end{equation}
\begin{proof}
We can show that there is zero probability that $E_{n}$ will occur a finite amount of times, ie.
\begin{equation}
1 - \mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = 0
\end{equation}
This probability can be rewritten as
\begin{equation}
1 - \mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = \lim_{N \to \infty}\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right)
\end{equation}
where $\overline{E}_{n}$ is the complement of $E_{n}$. Intuitively, this expresses the probability that there is some integer large enough such that any subsequent $E_{n}$ can no longer occur. Since the $E_{n}$ are independent, we can show
\begin{align}
\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) &= \prod_{n = N}^{\infty}\mathbb{P}\left(\overline{E}_{n}\right) \\
&= \prod_{n = N}^{\infty}\left(1 - \mathbb{P}\left(E_{n}\right)\right) \\
&\leq \prod_{n = N}^{\infty}\exp\left(-\mathbb{P}\left(E_{n}\right)\right)
\end{align}
where we have used the fact that $1 - x \leq e^{-x}$ for $x \geq 0$. Hence
\begin{align}
\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) &\leq \prod_{n = N}^{\infty}\exp\left(-\mathbb{P}\left(E_{n}\right)\right) \\
&\leq \exp\left(-\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right)\right) \\
&\leq 0 \\
\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) &= 0
\end{align}
Therefore $1 - \mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = \lim_{N \to \infty}\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) = 0$, completing the proof.
\end{proof}

\section{Dominated Convergence Theorem}

\section{Radon-Nikodym Theorem}

\subsection{Smoothing Law}

\section{Karhunen-Lo\`eve Theorem}


\chapter{Advanced Statistics}

\section{Copulas}

A copula is a multivariate probability distribution on support the unit hypercube, with uniform marginal distributions. An example of a copula is the Frank distribution, with cumulative distribution function
\begin{equation}
H\left(x, y; \psi\right) = \begin{cases}
\begin{array}{c}
\dfrac{1}{\psi}\log\left(1+\dfrac{\left(e^{\psi x}-1\right)\left(e^{\psi y}-1\right)}{e^{\psi}-1}\right)\\
xy,
\end{array} & \begin{array}{c}
\psi\neq0\\
\psi=0
\end{array}\end{cases}
\end{equation}
where $\psi$ is a parameter which determines the dependence structure of the copula. We can verify that the following properties hold for this distribution: $H\left(0, 0; \psi\right) = 0$, $H\left(1, 1; \psi\right) = 1$. Additionally, we can verify that $H\left(1, y; \psi\right) = y$, $H\left(x, 1; \psi\right) = x$ which are the cumulative distribution functions of the uniform marginal distributions. \\

The usefulness of copulas in statistics starts with the property that for some continuous random variable $X$ with cumulative distribution function $F_{X}\left(x\right)$, then the transformed random variable $F_{X}\left(X\right)$ will be uniformly distributed on $\left[0, 1\right]$. The reason for this is related to the inverse transform sampling theorem. Thus if we observe some data and suspect that their marginal distributions follow some distributions (eg. $F_{X}\left(x\right)$ and $F_{Y}\left(y\right)$), then we can fit the data to a copula. \\

For example with the Frank copula, we can fit it to a distribution of the form $H\left(F_{X}\left(x\right), F_{Y}\left(y\right); \psi\right)$. Since by construction the random variables $F_{X}\left(X\right)$ and $F_{Y}\left(Y\right)$ have uniform marginals, then the random variables $X$ and $Y$ have cumulative distribution functions $F_{X}\left(x\right)$ and $F_{Y}\left(y\right)$ respectively. The parameter $\psi$ can then be used to model the dependencies of the variables based on what is observed in the data. The maximum likelihood technique is typically used for estimating the parameters of the copula, which can be jointly estimated with any parameters of the marginal distributions.

\section{Empirical Measures}

\subsection{Empirical Distribution Function}

\subsection{Glivenko-Cantelli Theorem}

\section{Order Statistics}

\subsection{Distribution of a Single Order Statistic}

Let $X_{1}, \dots, X_{n}$ be i.i.d. random variables with cumulative distribution function $F\left(x\right)$. If a sample of these random variables are arranged in ascending order denoted as follows:
\begin{equation}
X_{\left(1\right)} \leq \dots \leq X_{\left(k\right)} \leq \dots \leq X_{\left(n\right)}
\end{equation}
then we call $X_{\left(k\right)}$ the $k$\textsuperscript{th} order statistic. The cumulative distribution of $X_{\left(k\right)}$, denoted $F_{\left(k\right)}$, can be expressed in terms of $F\left(x\right)$. By definition,
\begin{equation}
F_{\left(k\right)}\left(x\right) = \operatorname{Pr}\left(X_{\left(r\right)} \leq x\right)
\end{equation}
which can be rewritten as
\begin{equation}
F_{\left(k\right)}\left(x\right) = \operatorname{Pr}\left(X_{\left(1\right)} \leq \dots \leq X_{\left(k\right)} \leq x\right)
\end{equation}
This is the probability that at least $k$ out of the $X_{1}, \dots, X_{n}$ are less than or equal to $x$, which can be formulated using the upper cumulative binomial probability with `success' probability $F\left(x\right)$.
\begin{equation}
F_{\left(k\right)}\left(x\right) = \sum_{i = k}^{n}\begin{pmatrix}n \\ i\end{pmatrix}F\left(x\right)^{i}\left[1 - F\left(x\right)\right]^{n - i}
\end{equation}
or a lower cumulative binomial probability with success probability $1 - F\left(x\right)$, by making the change of variables $j = n - i$:
\begin{equation}
F_{\left(k\right)}\left(x\right) = \sum_{j = 0}^{n - k}\begin{pmatrix}n \\ j\end{pmatrix}\left[1 - F\left(x\right)\right]^{j}F\left(x\right)^{n - j}
\end{equation}
Using the alternative form of the cumulative binomial distribution in terms of the regularised incomplete Beta function, this gives
\begin{equation}
F_{\left(k\right)}\left(x\right) = \dfrac{1}{\operatorname{B}\left(k, n - k + 1\right)}\int_{0}^{F\left(x\right)}t^{k - 1}\left(1 - t\right)^{n - k}dt
\end{equation}
If $X$ is continuous with probability density function $f\left(x\right)$, this form can be used to derive the probability density function of the $k$\textsuperscript{th} order statistic using the Fundamental Theorem of Calculus:
\begin{align}
f_{\left(k\right)}\left(x\right) &= \dfrac{dF_{\left(k\right)}\left(x\right)}{dx} \\
&= \dfrac{1}{\operatorname{B}\left(k, n - k + 1\right)}\dfrac{d}{dx}\int_{0}^{F\left(x\right)}t^{k - 1}\left(1 - t\right)^{n - k}dt \\
&= \dfrac{1}{\operatorname{B}\left(k, n - k + 1\right)}\dfrac{dF_{\left(k\right)}\left(x\right)}{dx}\dfrac{d}{dF_{\left(k\right)}\left(x\right)}\int_{0}^{F\left(x\right)}t^{k - 1}\left(1 - t\right)^{n - k}dt \\
&= \dfrac{1}{\operatorname{B}\left(k, n - k + 1\right)}f\left(x\right)F\left(x\right)^{k - 1}\left[1 - F\left(x\right)\right]^{n - k} \\
&= \dfrac{n!}{\left(k - 1\right)!\left(n - k\right)!}F\left(x\right)^{k - 1}\left[1 - F\left(x\right)\right]^{n - k}f\left(x\right)
\end{align}

\subsubsection{Order Statistics of the Uniform Distribution}

If a sample $X_{1}, \dots, X_{n}$ are i.i.d. uniform distributed on $\left(0, 1\right)$, then the cumulative distribution $F\left(x\right) = x$ for $0 < x < 1$ and the density function $f\left(x\right) = 1$ for $0 < x < 1$. Applying the expression for the $k$\textsuperscript{th} order statistic:
\begin{align}
f_{\left(k\right)}\left(x\right) &= \dfrac{n!}{\left(k - 1\right)!\left(n - k\right)!}x^{k - 1}\left(1 - x\right)^{n - k} \\
&= \dfrac{x^{k - 1}\left(1 - x\right)^{n - k}}{\operatorname{B}\left(k, n - k + 1\right)}
\end{align}
on support $\left(0, 1\right)$. This is the same form as the Beta distribution. Hence the $k$\textsuperscript{th} order statistic of uniform random variables is Beta-distributed with parameters $k$ and $n - k + 1$. Hence the mean of the $k$\textsuperscript{th} order statistic is
\begin{equation}
\mathbb{E}\left[X_{\left(k\right)}\right] = \dfrac{k}{n + 1}
\end{equation}

\subsection{FisherTippettGnedenko Theorem \cite{David2005}}


\section{Monte-Carlo Methods}

\subsection{Rejection Sampling}

\subsection{Importance Sampling}

Suppose Monte-Carlo sampling is used to estimate some quantity $\mu = \mathbb{E}\left[H\left(\mathbf{X}\right)\right]$ where $\mathbf{X}$ is a random vector with density $f\left(\mathbf{x}\right)$ and $H\left(\cdot\right)$ is a real-valued function. Then a straightforward estimator using $N$ i.i.d. samples $\mathbf{X}$ drawn from the density $f\left(\mathbf{x}\right)$ is
\begin{equation}
\hat{\mu} = \dfrac{1}{N}\sum_{i = 1}^{N}H\left(\mathbf{X}_{i}\right)
\end{equation}
However, there are some densities which may be `difficult' to sample from, such that the variance of $\hat{\mu}$ will be impractically large. This may arise for example in rare event simulation, where $H\left(\cdot\right)$ is an indicator function for a rare event $A$ and $\hat{\mu}$ is an estimator for $\operatorname{Pr}\left(A\right)$. Since $A$ is rare, a plain Monte-Carlo sample as above may not event yield a single occurrence of $A$. The idea behind importance sampling is to re-weight the sampling density in hope of reducing the variance of $\hat{\mu}$. This is done as follows. We call $f\left(\mathbf{x}\right)$ the \textit{target} or \textit{nominal} density, and introduce a density $g\left(\mathbf{x}\right)$ called the \textit{proposal} or \textit{importance} density. The density $g\left(\mathbf{x}\right)$ must satisfy the property that $H\left(\mathbf{x}\right)f\left(\mathbf{x}\right) = 0$ wherever $g\left(\mathbf{x}\right) = 0$. This is so that
\begin{align}
\mu &= \int H\left(\mathbf{x}\right)f\left(\mathbf{x}\right) d\mathbf{x} \\
&= \int_{\left\{H\left(\mathbf{x}\right)f\left(\mathbf{x}\right) \neq 0 \right\}} H\left(\mathbf{x}\right)f\left(\mathbf{x}\right) d\mathbf{x} \\
&= \int_{\left\{g\left(\mathbf{x}\right) \neq 0 \right\}} H\left(\mathbf{x}\right)\dfrac{f\left(\mathbf{x}\right)}{g\left(\mathbf{x}\right)}g\left(\mathbf{x}\right) d\mathbf{x} \\
&= \mathbb{E}_{g}\left[H\left(\mathbf{X}\right)\dfrac{f\left(\mathbf{X}\right)}{g\left(\mathbf{X}\right)}\right]
\end{align}
Hence we can sample from the density $g\left(\mathbf{x}\right)$ and estimate the average of the random variable $H\left(\mathbf{X}\right)\dfrac{f\left(\mathbf{X}\right)}{g\left(\mathbf{X}\right)}$. This gives the unbiased importance sampling estimator from $N$ i.i.d. samples of $\mathbf{X}$ drawn from $g\left(\mathbf{x}\right)$:
\begin{equation}
\hat{\mu}_{\mathrm{IS}} = \dfrac{1}{N}\sum_{i = 1}^{N}H\left(\mathbf{X}_{i}\right)\dfrac{f\left(\mathbf{X}_{i}\right)}{g\left(\mathbf{X}_{i}\right)}
\end{equation}
By slight abuse of nomenclature, the weighting ratio $w\left(\mathbf{x}\right) = \dfrac{f\left(\mathbf{X}\right)}{g\left(\mathbf{X}\right)}$ is called the \textit{likelihood ratio}. There is an optimal choice of importance density $g^{*}\left(\mathbf{x}\right)$ which minimises $\operatorname{Var}_{g}\left(\hat{\mu}_{\mathrm{IS}}\right)$. It can be shown that the optimal importance density is
\begin{align}
g^{*}\left(\mathbf{x}\right) &= \dfrac{H\left(\mathbf{x}\right)f\left(\mathbf{x}\right)}{\int H\left(\mathbf{x}\right)f\left(\mathbf{x}\right)d\mathbf{x}} \\
&= \dfrac{H\left(\mathbf{x}\right)f\left(\mathbf{x}\right)}{\mu}
\end{align}
Note that this implicitly requires either $H\left(\mathbf{x}\right) \geq 0$ or $H\left(\mathbf{x}\right) \leq 0$ (ie. it does not ever switch signs) to guarantee that the density $g^{*}\left(\mathbf{x}\right) \geq 0$. With this, we can show that when samples of $\mathbf{X}$ are drawn i.i.d. from $g^{*}\left(\mathbf{x}\right)$:
\begin{align}
\operatorname{Var}_{g^{*}}\left(\hat{\mu}_{\mathrm{IS}}\right) &= \operatorname{Var}_{g^{*}}\left(\dfrac{1}{N}\sum_{i = 1}^{N}H\left(\mathbf{X}_{i}\right)\dfrac{f\left(\mathbf{X}_{i}\right)}{g^{*}\left(\mathbf{X}_{i}\right)}\right) \\
&= \operatorname{Var}_{g^{*}}\left(\dfrac{1}{N}\sum_{i = 1}^{N}\mu\right) \\
&= 0
\end{align}
This means that just a single sample will yield the desired $\mu$. However it is usually not possible to find $g^{*}\left(\mathbf{x}\right)$ because it requires $\mu$ itself. In practice, good importance sampling densities are chosen to be `close' to the minimum variance density $g^{*}\left(\mathbf{x}\right)$ \cite{Kroese2011}.

\subsection{Gibbs Sampling}

\subsection{Metropolis Algorithm}

\subsection{Metropolis-Hastings Algorithm}

\section{Resampling Methods}

\subsection{Jackknife}

Given a sample of size $n$, the Jackknife method for an estimator involves aggregating the estimates for each size $n - 1$ subsample. Suppose the parameter to be estimated is the population mean. Formally, the Jackknife estimate involves first taking $n$ sample means with each observation removed:
\begin{equation}
\bar{x}_{i} = \dfrac{1}{n - 1}\sum_{j = 1, j\neq i}^{n}x_{j}
\end{equation}
for $i = 1, \dots, n$. The Jackknife estimate of the population mean is then the mean of all the subsample means:
\begin{equation}
\hat{\theta} = \dfrac{1}{n}\sum_{i = 1}^{n}\bar{x}_{i}
\end{equation}
The Jackknife estimate of the variance of the estimator can also be calculated using the distribution of $\bar{x}_{i}$ as follows:
\begin{equation}
\hat{\operatorname{Var}}\left(\hat{\theta}\right) = \dfrac{n - 1}{n}\sum_{i = 1}^{n}\left(\bar{x}_{i} - \hat{\theta}\right)^{2}
\end{equation}
This estimator is an unbiased estimator of the variance of the sample mean.
\begin{proof}
Notice that $\bar{x}_{i} = \dfrac{n\bar{x} - x_{i}}{n - 1} \Rightarrow \left(n - 1\right)\bar{x}_{i} = n\bar{x} - x_{i}$ where $\bar{x}$ is the sample mean. The term $\bar{x}_{i} - \hat{\theta}$ can be manipulated to become
\begin{align}
\bar{x}_{i} - \hat{\theta} &= \dfrac{n\bar{x} - x_{i}}{n - 1} - \dfrac{1}{n}\sum_{i = 1}^{n}\bar{x}_{i} \\
&= \dfrac{1}{n - 1}\left(n\bar{x} - x_{i} - \dfrac{1}{n}\sum_{i = 1}^{n}\left(n - 1\right)\bar{x}_{i}\right) \\
&= \dfrac{1}{n - 1}\left(n\bar{x} - x_{i} - \dfrac{1}{n}\sum_{i = 1}^{n}\left(n\bar{x} - x_{i}\right)\right) \\
&= \dfrac{1}{n - 1}\left(n\bar{x} - x_{i} - n\bar{x} + \dfrac{1}{n}\sum_{i = 1}^{n}x_{i}\right) \\
&= \dfrac{1}{n - 1}\left(\bar{x} - x_{i}\right)
\end{align}
Hence
\begin{align}
\hat{\operatorname{Var}}\left(\hat{\theta}\right) &= \dfrac{n - 1}{n}\sum_{i = 1}^{n}\left(\bar{x}_{i} - \hat{\theta}\right)^{2} \\
&= \dfrac{1}{n\left(n - 1\right)}\sum_{i = 1}^{n}\left(\bar{x} - x_{i}\right)^{2}
\end{align}
Recall that $\dfrac{1}{\left(n - 1\right)}\sum_{i = 1}^{n}\left(\bar{x} - x_{i}\right)^{2}$ is an unbiased estimator of the population variance, and so $\dfrac{1}{n\left(n - 1\right)}\sum_{i = 1}^{n}\left(\bar{x} - x_{i}\right)^{2}$ is an unbiased estimator of the variance of the sample mean.
\end{proof}

\subsection{Bootstrap \cite{Wasserman2013}}

The bootstrap method can be used to compute standard errors of very general estimators. Let $T_{n} = g\left(X_{1}, \dots, X_{n}\right)$ be a statistic. In order to estimate $\operatorname{Var}\left(T_{n}\right)$ with respect to the data generating process (which is not presumed known), we compute
\begin{equation}
\hat{\operatorname{Var}}\left(T_{n}\right) = \dfrac{1}{N}\sum_{i = 1}^{N}\left(T_{n, i}^{*} - \overline{T}_{n}^{*}\right)^{2}
\end{equation}
where $T_{n, i}^{*} = g\left(X_{1, i}^{*}, \dots, X_{n, i}^{*}\right)$, is the statistic calculated from the $i$\textsuperscript{th} resample of the data from the empirical distribution function defined by the original data $X_{1}, \dots, X_{n}$. $\overline{T}_{n}^{*}$ is the sample mean of the statistic across all simulations. Hence the bootstrap estimate contains two levels of approximation. The first approximation is made by using the empirical distribution to replace the data generating process. The second level of approximation is by estimating the variance of $T_{n}$ with respect to the empirical distribution by using a Monte Carlo average.

\subsection{Bootstrap Confidence Intervals}

\subsubsection{Normal Bootstrap Interval}

By assuming $T_{n}$ is normally distributed, a simple confidence interval can be formed using $T_{n} \pm z_{\alpha/2}\sqrt{\hat{\operatorname{Var}}\left(T_{n}\right)}$.

\subsubsection{Pivotal Bootstrap Interval}

Let $\theta$ denote the population parameter for $T$ and let $\hat{\theta}_{n}$ denote the sample statistic. Define the pivot $R_{n} = \hat{\theta}_{n} - \theta$ which can be thought of as a `zeroed' random variable. Let $\hat{\theta}_{n, 1}^{*}, \dots, \hat{\theta}_{n, N}^{*}$ denote resampled bootstrap replications of $\hat{\theta}_{n}$. There exists some CDF of the pivot
\begin{equation}
F_{R_{n}}\left(r\right) = \operatorname{Pr}\left(R_{n} \leq r\right)
\end{equation}
Then $F_{R_{n}}^{-1}\left(\cdot\right)$ is the quantile function and there exists some $a, b$ such that
\begin{gather}
F_{R_{n}}^{-1}\left(1 - \dfrac{\alpha}{2}\right) = \hat{\theta}_{n} - a \\
F_{R_{n}}^{-1}\left(\dfrac{\alpha}{2}\right) = \hat{\theta}_{n} - b
\end{gather}
We can then show
\begin{align}
\operatorname{Pr}\left(a \leq \theta \leq b\right) &= \operatorname{Pr}\left(\hat{\theta}_{n} - b \leq R_{n} \leq \hat{\theta}_{n} - a\right) \\
&= F_{R_{n}}\left(\hat{\theta}_{n} - a\right) - F_{R_{n}}\left(\hat{\theta}_{n} - b\right) \\
&= F_{R_{n}}\left(F_{R_{n}}^{-1}\left(1 - \dfrac{\alpha}{2}\right)\right) - F_{R_{n}}\left(F_{R_{n}}^{-1}\left(\dfrac{\alpha}{2}\right)\right) \\
&= 1 - \alpha
\end{align}
Hence $\left(a, b\right)$ is an exact confidence interval. We estimate $a$ and $b$ by first estimating $F_{R_{n}}\left(r\right)$ from resampling:
\begin{equation}
\hat{F}_{R_{n}}\left(r\right) = \dfrac{1}{N}\sum_{i = 1}^{N}\mathbb{I}\left(R_{n, i}^{*} \leq r\right)
\end{equation}
where $\mathbb{I}\left(\cdot\right)$ is the indicator and $R_{n, i}^{*} := \hat{\theta}_{n, i}^{*} - \hat{\theta}_{n}$. Let $r_{\alpha}^{*}$ denote the $\alpha$ sample quantile from resampled bootstrap replications of $R_{n, 1}^{*}, \dots, R_{n, N}^{*}$ and similarly let $\theta_{\alpha}^{*}$ denote the $\alpha$ sample quantile from resampled bootstrap replications of $\hat{\theta}_{n, 1}^{*}, \dots, \hat{\theta}_{n, N}^{*}$. Then $b$ may be estimated by
\begin{equation}
\hat{b} = \hat{\theta}_{n} - \hat{F}_{R_{n}}^{-1}\left(\dfrac{\alpha}{2}\right) = \hat{\theta}_{n} - r_{\alpha/2}^{*}
\end{equation}
Likewise, $a$ can be estimated by
\begin{equation}
\hat{a} = \hat{\theta}_{n} - \hat{F}_{R_{n}}^{-1}\left(1 - \dfrac{\alpha}{2}\right) = \hat{\theta}_{n} - r_{1 - \alpha/2}^{*}
\end{equation}
Note that $r_{\alpha}^{*} = \theta_{\alpha}^{*} - \hat{\theta}_{n}$ because  $R_{n, i}^{*} := \hat{\theta}_{n, i}^{*} - \hat{\theta}_{n}$. Hence a $1 - \alpha$ pivotal bootstrap confidence interval is given by $\left(2\hat{\theta}_{n} - \hat{\theta}_{1 - \alpha/2}^{*}, 2\hat{\theta}_{n} - \hat{\theta}_{ \alpha/2}^{*}\right)$.

\subsubsection{Percentile Bootstrap Interval}

The percentile bootstrap interval is an `intuitive' interval using percentiles from the bootstrapped distribution. This is justified as follows. Assume there exists a monotonic transformation $m\left(\cdot\right)$ such that the random variable formed by $U := m\left(T\right)$ is normally distributed with $U \sim \mathcal{N}\left(\phi, c^{2}\right)$ where $\phi := m\left(\theta\right)$ is the monotonically transformed population parameter. Note that we do not suppose that we know $m\left(\cdot\right)$, only that it exists. A confidence interval for $\phi$ would be given by $U \pm cz_{\alpha/2}$ since
\begin{align}
\operatorname{Pr}\left(U - cz_{\alpha/2} \leq \phi \leq U + cz_{\alpha/2}\right) &= \operatorname{Pr}\left(-z_{\alpha/2} \leq \dfrac{U - \phi}{c} \leq z_{\alpha/2}\right) \\
&= 1 - \alpha
\end{align}
The estimates of $U \pm cz_{\alpha/2}$ would come from bootstrapping. Denote $U_{i}^{*} = m\left(\theta_{n, i}^{*}\right)$ during resampling and let $u_{\alpha}^{*}$ be the $\alpha$ sample quantile of the resampled bootstrap replications of $U_{1}^{*}, \dots, U_{N}^{*}$. So then
\begin{align}
1 - \alpha &= \operatorname{Pr}\left(U - cz_{\alpha/2} \leq \phi \leq U + cz_{\alpha/2}\right) \\
&\approx \operatorname{Pr}\left(u_{\alpha/2}^{*} \leq \phi \leq u_{1 - \alpha/2}^{*}\right) \\
&\approx \operatorname{Pr}\left(m\left(u_{\alpha/2}^{*}\right) \leq m\left(\phi\right) \leq m\left(u_{1 - \alpha/2}^{*}\right)\right)
\end{align}
Since the monotonic transformation preserves quantiles, then
\begin{equation}
\operatorname{Pr}\left(m\left(u_{\alpha/2}^{*}\right) \leq m\left(\theta\right) \leq m\left(u_{1 - \alpha/2}^{*}\right)\right) = \operatorname{Pr}\left(\theta_{\alpha/2}^{*} \leq \phi \leq \theta_{1 - \alpha/2}^{*}\right)
\end{equation}
So the percentile bootstrap interval is given by $\left(\theta_{\alpha/2}^{*}, \theta_{1 - \alpha/2}^{*}\right)$.

\subsection{Permutation Tests}

\section{Principal Component Analysis}

\subsection{Principal Component Regression}

\subsection{Maximum Likelihood Principal Component Analysis \cite{Bishop2006}}

\section{Survival Analysis}

\section{Rao-Blackwell Estimators}

\section{Optimal Experimental Design}

\chapter{Stochastic Calculus}

\section{Martingales}

\section{It\^{o} Calculus}

\subsection{It\^{o} Integral}

\section{Stratonovich Integral}

\section{Kolmogorov-Chentsov Continuity Theorem}

\section{Stochastic Differential Equations}

\section{Diffusions}

\chapter{Combinatorial Probability}

\section{Inclusion-Exclusion Principle}

\section{Pigeonhole Principle}

\section{Partitions}

Consider the positive integer $n$, partitioned into the sum of $k \leq n$ positive integers by
\begin{equation}
n = n_{1} + n_{2} + \dots + n_{k}
\end{equation}
where $n_{1}, \dots, n_{k} \geq 1$ (ie. there are no `empty' partitions). We ask how many different partitions $n_{1}, \dots, n_{k}$ there are. Picture a `block' of $n$ objects, with spaces in between each object. There are $n - 1$ such spaces (since the sides are excluded). We then imaging placing $k - 1$ `separators' among these spaces, in order to divide the block into $k$ partitions. Then the number of ways to place these separators is the number of possible partitions, given by
\begin{equation}
\mathstrut^{n-1}\mathsf{C}_{k - 1} = \dfrac{\left(n - 1\right)!}{\left(k - 1\right)!\left(n - k\right)!}
\end{equation}
If instead empty partitions are allowed (ie. $n_{1}, \dots, n_{k} \geq 0$), then we can extend the above by enumerating through possibilities where some of the partitions are empty. Note that if at least one partition is empty, there can be as few as $1$ and as many as $k - 1$ empty partitions (since the sum must still equal $n$). For an arbitrary $i$ empty partitions, there are $\mathstrut^{k}\mathsf{C}_{i}$ different possibilities, and for each possibility we imagine counting $k - i$ non-empty partitions of $n$. Hence the number of ways is given by
\begin{equation}
\mathstrut^{n-1}\mathsf{C}_{k - 1} + \sum_{i = 1}^{k - 1}\mathstrut^{k}\mathsf{C}_{i}\times\mathstrut^{n-1}\mathsf{C}_{k - 1 - i} = \sum_{i = 0}^{k - 1}\mathstrut^{k}\mathsf{C}_{i}\times\mathstrut^{n-1}\mathsf{C}_{k - 1 - i}
\end{equation}

\subsection{Stirling Numbers of the Second Kind}

\section{Catalan Numbers \cite{Lange2010}}

\section{Derangements}

A rearrangement of a set such that no element appears in its original position is called a derangement. The number of derangements of a set of size $n$ is denoted $!n$. The number of derangements can be counted by considering the following problem: each element in $\left\{1, \dots, n\right\}$ is to be assigned a label from $1, \dots, n$. We are interested in the number of ways to assigned labels in which no element is assigned its own label. That is, each element has exactly one `forbidden' label which it cannot be assigned. Suppose element $1$ is assigned an arbitrary label $i \neq 1$. There are $n - 1$ such possibilities. Then we can group the remaining possibilities pertaining to element $i$ as follows:
\begin{itemize}
\item Element $i$ is reciprocally assigned label $1$. Then the remaining possibilities becomes the same as with $n - 2$ elements and $n - 2$ labels.
\item Element $i$ is not assigned label $1$. Then remaining possibilities becomes the same as with $n - 1$ elements and $n - 1$ labels, because each element still has exactly one forbidden label (label $1$ acts as element $i$'s forbidden label).
\end{itemize}
Hence the recurrence relation is obtained for the number of derangements $!n$:
\begin{equation}
!n = \left(n - 1\right)\left[!\left(n - 2\right) + !\left(n  - 1\right)\right]
\end{equation}
Letting $!0 = 1$ and $!1 = 0$, then this defines the entirety of the sequence in $n$ for the number of derangements. \\

An explicit formula for the number of derangements can also be obtained via the inclusion-exclusion principle. In a rearrangement of a set, we call an element which remains it its original position `fixed'. There are $n!$ possibilities in which there are at least zero fixed elements (same as the number of arrangements). There are $\mathstrut^{n}\mathsf{C}_{1}\left(n - 1\right)!$ possibilities in which there are at least one fixed element (because there are $\mathstrut^{n}\mathsf{C}_{1}$ ways to choose one fixed element and $\left(n  -1\right)!$ arrangements of the remaining $n - 1$ elements). Hence we can develop the general formula that there are $\mathstrut^{n}\mathsf{C}_{i}\left(n - i\right)!$ arrangements in which there are at least $i$ fixed elements. Applying the inclusion exclusion principle:
\begin{align}
!n &= n! - \mathstrut^{n}\mathsf{C}_{1}\left(n - 1\right)! + \mathstrut^{n}\mathsf{C}_{2}\left(n - 2\right)! - \dots \\
&= \sum_{i = 0}^{n}\left(-1\right)^{i}\mathsf{C}_{i}\left(n - i\right)! \\
&= \sum_{i = 0}^{n}\left(-1\right)^{i}\dfrac{n!}{i!\left(n - 1\right)!}\left(n - i\right)! \\
&= n!\sum_{i = 0}^{n}\dfrac{\left(-1\right)}{i!}
\end{align}

\part{Applications}

\chapter{Information Theory}

\section{Entropy}

For a discrete random variable $X$ with probability mass function $P\left(x\right)$, the self-information of event $\left\{X = x\right\}$ is defined as
\begin{equation}
\operatorname{I}\left(x\right) = \log\dfrac{1}{P\left(x\right)}
\end{equation}
The `information entropy' of a discrete random variable $X$ with probability mass function $P\left(x\right)$ is defined as the expected self-information:
\begin{align}
\operatorname{H}\left[X\right] &= \operatorname{E}\left[-\log P\left(X\right)\right] \\
&= -\sum_{i = 1}^{n}P\left(x_{i}\right)\log P\left(x_{i}\right) \\
&= \sum_{i = 1}^{n}P\left(x_{i}\right)\log \dfrac{1}{P\left(x_{i}\right)}
\end{align}
Note that since $0 \leq P\left(x_{i}\right) \leq 1$, then entropy is non-negative (and when $P\left(x_{i}\right) \rightarrow 0$, the term in the summation approaches zero). If the log used is base 2, then the units of entropy is measured in bits. If the log is the natural log, then the units are in `nats'. There are various interpretations of entropy.
\begin{itemize}
\item Entropy can be thought of as a measure of uncertainty (in a separate way to variance). The larger the entropy, the more uncertainty in the random variable. A deterministic variable is the most certain kind of random variable, and has an entropy of zero (minimum uncertainty) since $\log 1 = 0$.
\item Entropy measured in bits can be thought of as the lower bound on the number of bits it requires to transmit/store the outcome of an experiment. For example, in the deterministic case we require no bits because we already know the outcome of all experiments. Consider the outcome of a fair coin toss, with probability mass function
\begin{equation}
\operatorname{Pr}\left(X = x\right) = \begin{cases}
\begin{array}{c}
0.5,\\
0.5,\\
0,
\end{array} & \begin{array}{c}
x=0\\
x=1\\
\text{elsewhere}
\end{array}\end{cases}
\end{equation}
The calculation of entropy yields $\operatorname{H}\left[X\right] = 1$ bit. This agrees with the intuition that it requires a minimum of 1 bit to convey the outcome of a fair coin toss (ie. 0 for tails, 1 for heads). In the case of an unfair coin, for example with the probability mass function
\begin{equation}
\operatorname{Pr}\left(X = x\right) = \begin{cases}
\begin{array}{c}
0.25,\\
0.75,\\
0,
\end{array} & \begin{array}{c}
x=0\\
x=1\\
\text{elsewhere}
\end{array}\end{cases}
\end{equation}
The calculation of entropy for this distribution yields $\operatorname{H}\left[X\right] = 0.8113$ bits. This is a lower bound, so rounded up gives 1 bit. We still require at least 1 bit to convey the outcome of an unfair coin toss because the outcome is still binary (either we get heads or we don't). \\

Now imagine an experiment with four possible outcomes distributed uniformly (eg. two fair coin tosses). The calculation of entropy does not depend on the events, only the associated probabilities. So in the case the probabilities are $\left\{0.25, 0.25, 0.25, 0.25\right\}$, the entropy works out to be $2$ bits. Once again this agrees with the intuition that it would take a minimum of $2$ bits to store the outcome of two fair coin tosses.
\item Lastly, entropy can also be thought of as the expected value of a random variable that expresses the amount of information contained in each event (the random variable in question being $-\log P\left(X\right)$). To illustrate, consider the random variable with distribution
\begin{equation}
\operatorname{Pr}\left(X = x\right) = \begin{cases}
\begin{array}{c}
0.05,\\
0.95,\\
0,
\end{array} & \begin{array}{c}
x=0\\
x=1\\
\text{elsewhere}
\end{array}\end{cases}
\end{equation}
We regard the occurrence of the event $X = 0$ as containing more information than the occurrence of the event $X = 1$, since it is rarer and hence more of a `surprise' (the occurrence of $X = 0$ says more about the experiment than the occurrence $X = 1$). This is reflected in $\log\dfrac{1}{0.05} > \log\dfrac{1}{0.95}$.
\end{itemize}

\subsection{Joint Entropy}

For a pair of discrete random variables $X$ and $Y$ with joint probability mass function $p\left(x, y\right)$, the joint entropy is defined as
\begin{align}
\operatorname{H}\left[X, Y\right] &= -\mathbb{E}\left[\log p\left(X, Y\right)\right] \\
&= \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{1}{p\left(x, y\right)}
\end{align}
This generalises to discrete random vectors $\mathbf{X}$ so that
\begin{equation}
\operatorname{H}\left[\mathbf{X}\right] = -\mathbb{E}\left[\log p\left(\mathbf{X}\right)\right]
\end{equation}

\subsection{Conditional Entropy}

For a pair of discrete random variables $X$ and $Y$ with joint probability mass function $p\left(x, y\right)$ and conditional distribution $p\left(y\middle|x\right)$, the conditional entropy is defined as
\begin{align}
\operatorname{H}\left[Y\middle|X\right] &= - \mathbb{E}\left[\log p\left(Y\middle|X\right)\right] \\
&= \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{1}{p\left(y\middle|x\right)} \\
&= \sum_{x}p\left(x\right)\sum_{y}p\left(y\middle|x\right)\log\dfrac{1}{p\left(y\middle|x\right)}
\end{align}
If $X$ and $Y$ are independent, then $p\left(y\middle|x\right) = p\left(y\right)$ and
\begin{equation}
\operatorname{H}\left[Y\middle|X\right] = \operatorname{H}\left[Y\right]
\end{equation}
The conditional entropy of $X$ on itself is
\begin{align}
\operatorname{H}\left[X\middle|X\right] = 0
\end{align}
since $p\left(x\middle|x\right) = \operatorname{Pr}\left(X = x\middle|X = x\right) = 1$. This is in agreement with the intuition that knowing $X$ reveals perfect information about $X$.

\subsection{Chain Rule of Entropy}

The chain rule of entropy says that
\begin{equation}
\operatorname{H}\left[X, Y\right] = \operatorname{H}\left[X\right]+ \operatorname{H}\left[Y\middle|X\right]
\end{equation}
\begin{proof}
\begin{align}
\operatorname{H}\left[X, Y\right] &= \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{1}{p\left(x, y\right)} \\
&= \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{1}{p\left(y\middle|x\right)p\left(x\right)} \\
&= -\sum_{x}\sum_{x}p\left(x, y\right)\log p\left(x\right) -\sum_{x}\sum_{y}p\left(x, y\right)\log p\left(y\middle|x\right) \\
&= -\sum_{x}p\left(x\right)\log p\left(x\right) -\sum_{x}\sum_{y}p\left(x, y\right)\log p\left(y\middle|x\right) \\
&= \operatorname{H}\left[X\right]+ \operatorname{H}\left[Y\middle|X\right]
\end{align}
\end{proof}
A corollary is that if $X$ and $Y$ are independent,
\begin{equation}
\operatorname{H}\left[X, Y\right] = \operatorname{H}\left[X\right]+ \operatorname{H}\left[Y\right]
\end{equation}
and if $X$ and $Y$ are independent and identically distributed,
\begin{equation}
\operatorname{H}\left[X, Y\right] = 2\operatorname{H}\left[X\right]
\end{equation}
This generalises to discrete random vectors $\mathbf{X}$ and $\mathbf{Y}$ so that
\begin{equation}
\operatorname{H}\left[\mathbf{X}, \mathbf{Y}\right] = \operatorname{H}\left[\mathbf{X}\right]+ \operatorname{H}\left[\mathbf{Y}\middle|\mathbf{X}\right]
\end{equation}

\subsection{Entropy of Functions}

Let $X$ be a discrete random variable, and let $g\left(X\right)$ be a function of $X$. By the chain rule of entropy,
\begin{align}
\operatorname{H}\left[X, g\left(X\right)\right] &= \operatorname{H}\left[X\right] + \operatorname{H}\left[g\left(X\right)\middle|X\right] \\
&= \operatorname{H}\left[X\right]
\end{align}
since $\operatorname{H}\left[g\left(X\right)\middle|X\right] = 0$ (ie. all information about $g\left(X\right)$ is revealed through $X$). Also by the chain rule of entropy,
\begin{align}
\operatorname{H}\left[X, g\left(X\right)\right] &= \operatorname{H}\left[g\left(X\right)\right] + \operatorname{H}\left[X\middle|g\left(X\right)\right] \\
&\geq \operatorname{H}\left[g\left(X\right)\right]
\end{align}
since $\operatorname{H}\left[X\middle|g\left(X\right)\right]  \geq 0$. Hence this gives the inequality for entropy of functions of random variables:
\begin{equation}
\operatorname{H}\left[g\left(X\right)\right] \leq \operatorname{H}\left[X\right]
\end{equation}
This is intuitive because the distribution of $g\left(X\right)$ cannot be any less informative than $X$ itself. In the extreme case where $g\left(X\right)$ equals a constant, then $\operatorname{H}\left[g\left(X\right)\right] = 0$. This generalises to functions of random vectors so that
\begin{equation}
\operatorname{H}\left[g\left(\mathbf{X}\right)\right] \leq \operatorname{H}\left[\mathbf{X}\right]
\end{equation}

\subsubsection{Entropy of Sums}

Suppose there are two independent random variables $X$ and $Y$, and their sum is $Z = X + Y$. We can obtain upper and lower bounds on $\operatorname{H}\left[Z\right]$. Firstly, we can show
\begin{align}
\operatorname{H}\left[Z\middle|X\right] &= \sum_{x}\operatorname{Pr}\left(X = x\right)\sum_{z}\operatorname{Pr}\left(Z = z\middle|X = x\right)\log\dfrac{1}{\operatorname{Pr}\left(Z = z\middle|X = x\right)} \\
&= \sum_{x}\operatorname{Pr}\left(X = x\right)\sum_{z}\operatorname{Pr}\left(Y = z - x\middle|X = x\right)\log\dfrac{1}{\operatorname{Pr}\left(Y = z - x\middle|X = x\right)} \\
&= \sum_{x}\operatorname{Pr}\left(X = x\right)\sum_{y}\operatorname{Pr}\left(Y = y\middle|X = x\right)\log\dfrac{1}{\operatorname{Pr}\left(Y = y\middle|X = x\right)} \\
&= \operatorname{H}\left[Y\middle|X\right]
\end{align}
This shows that after knowing $X$, the uncertainty in information about $Z$ is due to $Y$. Then since $X$ and $Y$ are independent, $\operatorname{H}\left[Y\middle|X\right] = \operatorname{H}\left[Y\right]$. Then by the inequality $\operatorname{H}\left[Z\right] \geq \operatorname{H}\left[Z\middle|X\right]$ that conditioning never increases entropy (shown using mutual information), we can write
\begin{equation}
\operatorname{H}\left[Y\right] \leq \operatorname{H}\left[Z\right]
\end{equation}
and similarly 
\begin{equation}
\operatorname{H}\left[X\right] \leq \operatorname{H}\left[Z\right]
\end{equation}
Combining both cases gives $\max\left\{\operatorname{H}\left[X\right], \operatorname{H}\left[Y\right]\right\} \leq \operatorname{H}\left[Z\right]$. The interpretation of this is the intuitive idea that adding a random variable to another never reduces the uncertainty. This can also be visualised using the convolution, which smoothes out the density and makes it less concentrated, increasing the entropy. An upper bound can be obtained by first using the chain rule of entropy to write
\begin{align}
\operatorname{H}\left[X, Z\right] &= \operatorname{H}\left[Z\right] + \operatorname{H}\left[X\middle|Z\right] \\
&= \operatorname{H}\left[X\right] + \operatorname{H}\left[Z\middle|X\right]
\end{align}
Hence
\begin{align}
\operatorname{H}\left[Z\right] &= \operatorname{H}\left[X\right] + \operatorname{H}\left[Z\middle|X\right] - \operatorname{H}\left[X\middle| Z\right] \\
&= \operatorname{H}\left[X\right] + \operatorname{H}\left[Y\right] - \operatorname{H}\left[X\middle| Z\right]
\end{align}
since $\operatorname{H}\left[Z\middle|X\right] = \operatorname{H}\left[Y\right]$ as established above. Then since $\operatorname{H}\left[X\middle| Z\right] \geq 0$, then
\begin{equation}
\operatorname{H}\left[Z\right] \leq \operatorname{H}\left[X\right] + \operatorname{H}\left[Y\right]
\end{equation}
We can even establish conditions on $X$ and $Y$ for equality to hold. Since $Z = X + Y$ is a function of $\left(X, Y\right)$, then $\operatorname{H}\left[Z\right] \leq \operatorname{H}\left[X, Y\right]$ and by the chain rule of entropy $\operatorname{H}\left[X, Y\right] = \operatorname{H}\left[X\right] + \operatorname{H}\left[X\middle| Y\right] = \operatorname{H}\left[X\right] + \operatorname{H}\left[Y\right]$ due to independence. Therefore
\begin{equation}
\operatorname{H}\left[Z\right] \leq \operatorname{H}\left[X, Y\right] = \operatorname{H}\left[X\right] + \operatorname{H}\left[Y\right]
\end{equation}
and equality holds if we can show $\operatorname{H}\left[Z\right] = \operatorname{H}\left[X, Y\right]$. This requires that $\left(X, Y\right)$ be a function of $Z$, which implies that $X$ or $Y$ be constant.

\subsection{Cross Entropy}

For two probability mass functions $p\left(x_{i}\right)$ and $q\left(x_{i}\right)$, where $q\left(x_{i}\right)$ is the approximating distribution of $p\left(x_{i}\right)$, the cross entropy between $p$ and $q$ is defined as
\begin{align}
\operatorname{H}_{p, q} &= -\mathbb{E}\left[\log q\left(x_{i}\right)\right]\\
&= \sum_{i}p\left(x_{i}\right)\log\dfrac{1}{q\left(x_{i}\right)}
\end{align}
Similar properties of joint entropies apply to cross entropies of multivariate distributions. Suppose we have multivariate distributions $p\left(x_{1}, \dots, x_{n}\right)$ and the approximating distribution $q\left(x_{1}, \dots, x_{n}\right)$ (note the slight abuse in notation from before). Then the cross entropy is
\begin{equation}
\operatorname{H}_{p_{1:n}, q_{1:n}} = \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}, \dots, x_{n}\right)\log\dfrac{1}{q\left(x_{1}, \dots, x_{n}\right)}
\end{equation}
If $X_{1}, \dots, X_{n}$ are independent (subsequently this also implies $q\left(x_{1}, \dots, x_{n}\right) = q\left(x_{1}\right)\dots q\left(x_{n}\right)$ if the knowledge of independence is also incorporated into the approximating distribution), then
\begin{align}
\operatorname{H}_{p_{1:n}, q_{1:n}} &= \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log\dfrac{1}{q\left(x_{1}\right)\dots q\left(x_{n}\right)} \\
&= - \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log q\left(x_{1}\right)\dots q\left(x_{n}\right) \\
&= - \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log q\left(x_{1}\right) - \dots - \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log q\left(x_{n}\right) \\
&= - \sum_{x_{1}}p\left(x_{1}\right)\log q\left(x_{1}\right) - \dots - \sum_{x_{n}}p\left(x_{n}\right)\log q\left(x_{n}\right) \\
&= \sum_{i = 1}^{n}\sum_{x_{i}}p\left(x_{i}\right)\log\dfrac{1}{q\left(x_{i}\right)} \\
&= \sum_{i = 1}^{n}\operatorname{H}_{p_{i},q_{i}}
\end{align}

\subsection{Entropy Rate}

Let $\mathcal{X} = \left\{X_{1}, X_{2}, \dots, X_{n}\right\}$ be a random sequence. Then the entropy rate is defined as
\begin{equation}
\operatorname{H}\left[\mathcal{X}\right] = \lim_{n\to\infty}\dfrac{1}{n}\operatorname{H}\left[X_{1}, X_{2}, \dots, X_{n}\right]
\end{equation}
if the limit exists. If the sequence is i.i.d., then the entropy rate is
\begin{equation}
\operatorname{H}\left[\mathcal{X}\right] = \lim_{n\to\infty}\dfrac{1}{n}\operatorname{H}\left[X_{1}, X_{2}, \dots, X_{n}\right] = \lim_{n\to\infty}\dfrac{n\operatorname{H}\left[X\right]}{n} = \operatorname{H}\left[X\right]
\end{equation}

\subsection{Differential Entropy}

The continuous analogue of information entropy is differential entropy, where the discrete sum is converted to an integral over the probability density function $f\left(x\right)$.
\begin{equation}
\operatorname{H}\left[X\right] = -\int f\left(x\right)\log f\left(x\right) dx
\end{equation}
Note however that this measure of entropy does not share all the same properties as information entropy defined above. One such property is non-negativity. Since $f\left(x\right)$ may be greater than 1, then it is possible for $-\log f\left(x\right) < 0$. Despite this, we can still use differential entropy as a measure of uncertainty.

\subsection{Asymptotic Equipartition Property}

The asymptotic equipartition property is the analogue of the weak law of large numbers for entropy. If $X_{1}, \dots, X_{n}$ are i.i.d. with joint probability mass function $p\left(X_{1}, \dots, X_{n}\right)$, then
\begin{equation}
\dfrac{1}{n}\log\dfrac{1}{p\left(X_{1}, \dots, X_{n}\right)} \overset{p}{\to} \operatorname{H}\left[X\right]
\end{equation}
as $n\to\infty$.
\begin{proof}
Because of i.i.d., we can write
\begin{equation}
\dfrac{1}{n}\log\dfrac{1}{p\left(X_{1}, \dots, X_{n}\right)} = -\dfrac{1}{n}\sum_{i = 1}^{n}\log p\left(X\right)
\end{equation}
which converges in probability to $-\mathbb{E}\left[\log p\left(X\right)\right] = \operatorname{H}\left[X\right]$ due to the weak law of large numbers.
\end{proof}
The asymptotic equipartition property also holds for continuous random variables and differential entropy. If $X_{1}, \dots, X_{n}$ are i.i.d. with joint probability density function $f\left(X_{1}, \dots, X_{n}\right)$, then
\begin{equation}
\dfrac{1}{n}\log\dfrac{1}{f\left(X_{1}, \dots, X_{n}\right)} \overset{p}{\to} \operatorname{H}\left[X\right]
\end{equation}
as $n\to\infty$.

\section{Kullback-Leibler Divergence}

The Kullback-Leibler (KL) Divergence is a measure of relative entropy between two distributions, which roughly speaking gives a measure of the amount of information lost when approximating one distribution with the other distribution. For discrete probability distributions $P\left(x\right)$ and $Q\left(x\right)$, the Kullback-Leibler Divergence from $Q$ to $P$ is defined as
\begin{align}
\operatorname{KL}\left(P\|Q\right) &= -\sum_{i}P\left(x_{i}\right)\log\dfrac{Q\left(x_{i}\right)}{P\left(x_{i}\right)} \\
&= \sum_{i}P\left(x_{i}\right)\log\dfrac{P\left(x_{i}\right)}{Q\left(x_{i}\right)} \\
&= \sum_{i}P\left(x_{i}\right)\left[\log P\left(x_{i}\right) - \log Q\left(x_{i}\right)\right]
\end{align}
Here, $Q$ is treated as the approximating distribution and $P$ is the `true' distribution. The KL divergence is finite only for when $Q\left(x_{i}\right) = 0$ implies $P\left(x_{i}\right) = 0$ for all $i$. If there exists an $i$ for which $Q\left(x_{i}\right) = 0$ and $P\left(x_{i}\right) \geq 0$, we take $\operatorname{KL}\left(P\|Q\right) = \infty$ \cite{Cover2006}. A property of the KL divergence is always non-negative, ie. $\operatorname{KL}\left(P\|Q\right) \geq 0$. This is known as Gibb's inequality.
\begin{proof}
\begin{align}
\operatorname{KL}\left(P\|Q\right) &= \sum_{P\left(x_{i}\right) > 0}P\left(x_{i}\right)\log\dfrac{P\left(x_{i}\right)}{Q\left(x_{i}\right)} \\
&= -\sum_{P\left(x_{i}\right) > 0}P\left(x_{i}\right)\log\dfrac{Q\left(x_{i}\right)}{P\left(x_{i}\right)}
\end{align}
As $-\log$ is a convex function, then using Jensen's inequality
\begin{equation}
\operatorname{KL}\left(P\|Q\right) \geq -\log\sum_{P\left(x_{i}\right) > 0}P\left(x_{i}\right)\dfrac{Q\left(x_{i}\right)}{P\left(x_{i}\right)} = -\log\sum_{P\left(x_{i}\right) > 0}Q\left(x_{i}\right)
\end{equation}
Or
\begin{equation}
-\operatorname{KL}\left(P\|Q\right) \leq \log\sum_{P\left(x_{i}\right) > 0}Q\left(x_{i}\right)
\end{equation}
Since $\sum Q\left(x_{i}\right) \leq 1$, then
\begin{equation}
-\operatorname{KL}\left(P\|Q\right) \leq \log 1 = 0
\end{equation}
Hence
\begin{equation}
\operatorname{KL}\left(P\|Q\right) \geq 0
\end{equation}
\end{proof}
Intuitively, there is always information loss when approximating one distribution with another distribution, with only no information loss occurring ($\operatorname{KL}\left(P\|Q\right) = 0$ when the distributions of $P$ and $Q$ are identical). Also note that in general, $\operatorname{KL}\left(P\|Q\right) \neq \operatorname{KL}\left(Q\|P\right)$. An alternative way to write the KL divergence is as
\begin{align}
\operatorname{KL}\left(P\|Q\right) &= \sum_{i}P\left(x_{i}\right)\log P\left(x_{i}\right) - \sum_{i}P\left(x_{i}\right)\log Q\left(x_{i}\right) \\
&= \left[- \sum_{i}P\left(x_{i}\right)\log Q\left(x_{i}\right)\right] - \left[-\sum_{i}P\left(x_{i}\right)\log P\left(x_{i}\right)\right] \\
&= \operatorname{H}_{P, Q} - \operatorname{H}_{P}
\end{align}
where $\operatorname{H}_{P, Q}$ is the cross entropy of $P$ and $Q$, and $\operatorname{H}_{P}$ is the entropy of $P$. \\

The KL divergence is also easily generalised to multivariate distributions; consider the joint pmf $p\left(x, y\right)$ and the approximating joint pmf $q\left(x, y\right)$.
\begin{equation}
\operatorname{KL}\left(p\middle\Vert q\right) = \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{p\left(x, y\right)}{q\left(x, y\right)}
\end{equation}

The continuous analogue of KL divergence for probability density functions $p$ and $q$ is
\begin{equation}
\operatorname{KL}\left(p\|q\right) = \int p\left(x\right)\log\dfrac{p\left(x\right)}{q\left(x\right)}dx
\end{equation}

\subsection{Mutual Information}

Consider two discrete random variables $X$, $Y$ with joint probability mass function $p\left(x, y\right)$ and marginal probability mass functions $p\left(x\right)$ and $p\left(y\right)$ respectively. The mutual information $\operatorname{MI}\left(X;Y\right)$ is the KL divergence between the joint distribution and the approximating distribution $q\left(x, y\right) = p\left(x\right)p\left(y\right)$.
\begin{equation}
\operatorname{MI}\left(X;Y\right) = \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{p\left(x, y\right)}{p\left(x\right)p\left(y\right)}
\end{equation}
We can see that if $X$ and $Y$ are independent, ie. $p\left(x, y\right) = p\left(x\right)p\left(y\right)$, then the mutual information will be zero. In that sense, we can think of mutual information as information loss by assuming independence of random variables. Mutual information can be rewritten as
\begin{align}
\operatorname{MI}\left(X;Y\right) &= \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{p\left(x\middle|y\right)}{p\left(x\right)} \\
&= \sum_{x}\sum_{y}p\left(x, y\right)\log p\left(x\middle|y\right) - \sum_{x}\sum_{y}p\left(x, y\right)\log p\left(x\right) \\
&= \sum_{x}\sum_{y}p\left(x, y\right)p\left(x\right)\log p\left(x\middle|y\right) - \sum_{x}p\left(x\right)\log p\left(x\right) \\
&= \operatorname{H}\left[X\right] - \operatorname{H}\left[X\middle|Y\right]
\end{align}
By symmetry we can also show
\begin{equation}
\operatorname{MI}\left(X;Y\right) = \operatorname{H}\left[Y\right] - \operatorname{H}\left[Y\middle|X\right]
\end{equation}
Hence in this way, $\operatorname{MI}\left(X;Y\right)$ can be thought of as the information gain about $X$ from knowing $Y$, and likewise about $Y$ from knowing $X$. By using the chain rule of entropy, we have
\begin{equation}
\operatorname{MI}\left(X;Y\right) = \operatorname{H}\left[X\right] + \operatorname{H}\left[Y\right] - \operatorname{H}\left[X, Y\right]
\end{equation}
Also note that
\begin{equation}
\operatorname{MI}\left(X;X\right) = \operatorname{H}\left[X\right] - \operatorname{H}\left[X\middle|X\right] = \operatorname{H}\left[X\right] - 0 =  \operatorname{H}\left[X\right]
\end{equation}
Given that $\operatorname{MI}\left(X;Y\right) \geq 0$ by Gibb's inequality, we can also write
\begin{gather}
\operatorname{H}\left[X\right] \geq \operatorname{H}\left[X\middle|Y\right] \\
\operatorname{H}\left[Y\right] \geq \operatorname{H}\left[Y\middle|X\right]
\end{gather}
which expresses that conditioning never increases entropy.

\subsection{Asymptotic Equipartition Property for the KL Divergence}

A version of the asymptotic equipartition property exists for the KL divergence, which can be proved in a similar fashion. Let $X_{1}, \dots, X_{n}$ be i.i.d. with joint probability mass function $p\left(X_{1}, \dots, X_{n}\right)$. Let $q\left(X_{1}, \dots, X_{n}\right)$ be any other probability mass function on the support of $X$ (which plays the part of the approximating distribution). Then
\begin{equation}
\dfrac{1}{n}\log\dfrac{p\left(X_{1}, \dots, X_{n}\right)}{q\left(X_{1}, \dots, X_{n}\right)} \overset{p}{\to} \operatorname{KL}\left(p\middle\Vert q\right)
\end{equation}
as $n\to\infty$.

\subsection{Equivalence Between Minimum KL Divergence and MLE}

We show that finding a parameter which minimises the KL divergence between the parametrised distribution and a empirical distribution is equivalent to finding the maximum likelihood estimate. This is shown for the case where the family of distributions is continuous, by the result analogously holds for families of discrete distributions. Suppose some i.i.d. data $x_{1}, \dots, x_{n}$ is collected, resulting in an empirical distribution function with density function $\tilde{p}\left(x\right)$.
\begin{equation}
\tilde{p}\left(x\right) = \sum_{i = 1}^{n}\dfrac{1}{n}\delta\left(x - x_{i}\right)
\end{equation}
where $\delta\left(\cdot\right)$ is the Dirac distribution (ie. the empirical density is just a continuous representation using impulses of the empirical mass function). Suppose there is a family of models with probability density $q\left(x;\theta\right)$ on support $\mathcal{X}$ (which is implicitly assumed to be a superset of all the data), parametrised by $\theta$. We seek to minimise the KL divergence between the empirical density function $\tilde{p}\left(x\right)$ and the approximating distribution $q\left(x;\theta\right)$. By definition of the KL divergence,
\begin{equation}
\operatorname{KL}\left(\tilde{p}\left(x\right)\middle\Vert q\left(x;\theta\right)\right) = \mathbb{E}\left[\log\dfrac{\tilde{p}\left(x\right)}{q\left(x;\theta\right)}\right]
\end{equation}
where it is understood that the expectation is taken over the empirical density. Rewriting the KL divergence in terms of cross entropy and entropy,
\begin{equation}
\operatorname{KL}\left(\tilde{p}\left(x\right)\middle\Vert q\left(x;\theta\right)\right) = -\mathbb{E}\left[\log q\left(x;\theta\right)\right] + \mathbb{E}\left[\log \tilde{p}\left(x\right)\right]
\end{equation}
Note that the entropy of $\tilde{p}\left(x\right)$ is unaffected by $\theta$. Evaluating the (negative) cross-entropy term $\mathbb{E}\left[\log q\left(x;\theta\right)\right]$:
\begin{align}
\mathbb{E}\left[\log q\left(x;\theta\right)\right] &= \int_{\mathcal{X}}\tilde{p}\left(x\right)\log q\left(x;\theta\right)dx \\
&= \int_{\mathcal{X}}\sum_{i = 1}^{n}\dfrac{1}{n}\delta\left(x - x_{i}\right)\log q\left(x;\theta\right)dx \\
&= \dfrac{1}{n}\sum_{i = 1}^{n}\int_{\mathcal{X}}\delta\left(x - x_{i}\right)\log q\left(x;\theta\right)dx \\
&= \dfrac{1}{n}\sum_{i = 1}^{n}\log q\left(x_{i};\theta\right)
\end{align}
Hence
\begin{equation}
\argmin_{\theta}\operatorname{KL}\left(\tilde{p}\left(x\right)\middle\Vert q\left(x;\theta\right)\right) = \argmin_{\theta}\left\{-\sum_{i = 1}^{n}\log q\left(x_{i};\theta\right)\right\}
\end{equation}
which is the minimiser of the negative log likelihood when the data is i.i.d.

\subsection{Symmetrised KL Divergence}

The symmetrised KL divergence is defined as
\begin{equation}
\operatorname{KL}_{\mathrm{sym}}\left(P\middle\|Q\right) = \operatorname{KL}\left(P\middle\|Q\right) + \operatorname{KL}\left(Q\middle\|P\right)
\end{equation}
where it attains the property that $\operatorname{KL}_{\mathrm{sym}}\left(P\middle\|Q\right) = \operatorname{KL}_{\mathrm{sym}}\left(Q\middle\|P\right)$.

\subsection{Fischer Information Metric}

Consider a family of distributions $f\left(x;\theta\right)$ parametrised by the parameter vector $\theta$. For some particular $\theta'$, the Fischer information metric is defined as the second partial derivative with respect to $\theta'$ of the KL divergence between $f\left(x;\theta\right)$ and $f\left(x;\theta'\right)$ evaluated at $\theta$. That is,
\begin{align}
\left.\dfrac{\partial^{2}}{\partial\theta_{i}'\partial\theta_{j}'}\operatorname{KL}\left(f\left(x;\theta\right)\middle\Vert f\left(x;\theta'\right)\right)\right|_{\theta' = \theta} &= \left.\dfrac{\partial^{2}}{\partial\theta_{i}'\partial\theta_{j}'}\left(-\mathbb{E}\left[\log f\left(x;\theta'\right)\right] + \mathbb{E}\left[\log f\left(x;\theta\right)\right]\right)\right|_{\theta' = \theta} \\
&= -\left.\dfrac{\partial^{2}}{\partial\theta_{i}'\partial\theta_{j}'}\mathbb{E}\left[\log f\left(x;\theta'\right)\right]\right|_{\theta' = \theta} \\
&= -\int_{\mathcal{X}}f\left(x;\theta\right)\left.\dfrac{\partial^{2}}{\partial\theta_{i}'\partial\theta_{j}'}\log f\left(x;\theta'\right)\right|_{\theta' = \theta}dx \\
&= -\int_{\mathcal{X}}f\left(x;\theta\right)\dfrac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}\log f\left(x;\theta\right)dx \\
&= -\mathbb{E}\left[\dfrac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}\log f\left(x;\theta\right)\right]
\end{align}
Note that the Hessian can be formed from the matrix of all second partial derivatives. Then if we treat $\log f\left(x;\theta\right)$ as the log likelihood, then this gives equivalence to the Fisher information matrix. Performing a second order Taylor expansion for the KL divergence in $\theta'$ (call this $D\left(\theta'\right)$ for brevity of notation) about $\theta$ gives
\begin{align}
D\left(\theta'\right) \approx \dfrac{1}{2}\left(\theta' - \theta\right)^{\top}\left.\nabla_{\theta'}^{2}\operatorname{KL}\left(f\left(x;\theta\right)\middle\Vert f\left(x;\theta'\right)\right)\right|_{\theta' = \theta}\left(\theta' - \theta\right)
\end{align}
where we have used the facts that $D\left(\theta\right) = \operatorname{KL}\left(f\left(x;\theta\right)\middle\Vert f\left(x;\theta\right)\right) = 0$ and is a minimum, so the gradient also vanishes. Then the Fisher information metric may be interpreted as the curvature of the KL divergence with respect to $\theta'$ at $\theta$. The intuition is that the larger the Fisher information (ie. the larger the curvature), the more easily the parameter can be distinguished, hence the more `information' implied by $\theta$.

\section{Maximum Entropy Distributions}

\subsection{Principle of Maximum Entropy}

The principle of maximum entropy roughly states that, given limited information about a prior, then the best choice of prior distribution (out of all possible priors which satisfy the limited information given) is the distribution with maximum entropy. Intuitively, this distribution makes the least number of assumptions about the variable in question.

\subsection{Maximum Entropy Distributions on Bounded Support}
For any discrete distribution on support $\left\{a, a + 1, \dots, b - 1, b\right\}$, the discrete uniform distribution has the maximum entropy. For any continuous distribution on support $\left[a, b\right]$, the continuous uniform distribution has the maximum entropy. This agrees with the intuition that the uniform distributions contain the most amount of `surprise' (ie. everything is equally likely).

\subsection{Maximum Entropy Distributions on Unbounded Support}

For a prescribed mean, the exponential distribution has the maximum entropy among all continuous distributions supported on $\left[0, \infty\right)$. \\

For a prescribed mean and variance, the Gaussian distribution has the maximum entropy among all continuous distributions supported on $\left(-\infty, \infty\right)$. \\

For a prescribed mean, variance, skewness and kurtosis, the maximum entropy distribution among continuous distributions supported on $\left(-\infty, \infty\right)$ takes the form
\begin{equation}
f_{X}\left(x\right) \propto \exp\left(ax + bx^{2} + cx^{3} + dx^{4}\right)
\end{equation}
However there may be no solution (if the skewness and kurtosis lie in certain regions) and the solution (if it exists) can be a bimodal distribution \cite{Rockinger2002}.

\section{Coding Theory \cite{Cover2006}}

Suppose there is a random variable $X$ with discrete support $\mathcal{X}$. A \textit{source code} for $X$ is a mapping from $\mathcal{X}$ to a set $\mathcal{D}$ of finite length strings using symbols from an alphabet of length $D$ (called a $D$-ary alphabet). Each of these strings is called a codeword. A binary alphabet with symbols 0 and 1 is a case of $D = 2$. \\

A (source) code is said to be \textit{nonsingular} if every element in $\mathcal{X}$ maps uniquely to $\mathcal{D}$. The \textit{extension} of a code is the mapping from sequences of elements from $\mathcal{X}$ to concatenated strings from $\mathcal{D}$ corresponding to the sequence. A code is \textit{uniquely decodable} if its extension is non-singular (that is, it is enough to determine the sequence of events just by looking at the concatenated string). An example of uniquely decodable codes are \textit{prefix codes}, whereby no codeword is a prefix of any other codeword. Prefix codes can be decoded instantaneously, ie. without needing to look at the future sequence of codewords. \\

Based the distribution of $X$, denoted $p\left(x_{i}\right)$, an optimal code can be designed which minimises the expected codeword length. Denote these optimal lengths $\ell_{i}^{*}$. The entropy of $X$ (using log base $D$) gives a lower bound on the expected codeword length of the optimal prefix code using a $D$-ary alphabet \cite{Cover2006}.
\begin{equation}
\operatorname{H}\left[X\right] \leq \sum_{i}p\left(x_{i}\right)\ell_{i}^{*}
\end{equation}
Suppose however an optimal code is designed using an assumed/approximating distribution $q\left(x_{i}\right)$, yielding lengths $l_{i}^{*}$. Then the cross entropy between $p\left(x_{i}\right)$ and $q\left(x_{i}\right)$ gives the lower bound on actual expected codeword length of the optimal prefix code.
\begin{equation}
\operatorname{H}_{p, q} \leq \sum_{i}p\left(x_{i}\right)l_{i}^{*}
\end{equation}

\section{Information Criteria}

\subsection{Akaike Information Criterion}

The AIC of a model parametrised by $\theta$ can be written in terms of its log likelihood $\log\mathcal{L}\left(\theta\right)$.
\begin{equation}
\operatorname{AIC}\left(\theta\right) = -2\max_{\theta}\left\{\log\mathcal{L}\left(\theta\right)\right\} + 2\dim\left(\theta\right)
\end{equation}
where $\dim\left(\theta\right)$ is the length of the parameter vector $\theta$. To derive the AIC \cite{Ljung1999}, first consider a prediction error term $\varepsilon_{i}\left(\theta\right)$ parameterised by $\theta$, which is the prediction error of the $i$\textsuperscript{th} observation in the dataset $\mathcal{D}_{N}$, where $N$ is the sample size. Let $\ell\left(\varepsilon\right)$ be a cost on the prediction error. The estimator $\hat{\theta}_{N}$ is an extremum estimator for the mean cost $V_{N}\left(\theta, \mathcal{D}_{N}\right)$ given by
\begin{align}
\hat{\theta}_{N} &= \argmin_{\theta}V_{N}\left(\theta, \mathcal{D}_{N}\right) \\
&= \argmin_{\theta}\left\{\dfrac{1}{N}\sum_{i = 1}^{N}\ell\left(\varepsilon_{i}\left(\theta\right)\right)\right\}
\end{align}
Define $\bar{V}\left(\theta\right)$ as the limit of the mean cost as $N\to\infty$. That is,
\begin{equation}
\bar{V}\left(\theta\right) = \lim_{N\to\infty}\dfrac{1}{N}\sum_{i = 1}^{N}\ell\left(\varepsilon_{i}\left(\theta\right)\right)
\end{equation}
If the errors $\varepsilon_{i}$ are i.i.d. with the distribution of some `true error' $\varepsilon$, then by the Law of Large Numbers we can write this as:
\begin{equation}
\bar{V}\left(\theta\right) = \mathbb{E}\left[\ell\left(\varepsilon\left(\theta\right)\right)\middle|\theta\right]
\end{equation}
where the conditional expectation is taken over the data generating process. For an estimate $\hat{\theta}_{N}$, we seek to relate the terms $\mathbb{E}\left[V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right)\right]$ and $\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right]$, where the expectations are taken over the estimator $\hat{\theta}_{N}$. There is an important distinction between these two terms, which is that the term $\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right]$ expresses the expected mean cost as evaluated on the training data $\mathcal{D}_{N}$, while the term $\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right]$ represents the expected cost of prediction on unseen data (ie. validation data). Note that we assume that the training data and validation data follow the same data generating process. To obtain these expectations, first perform a Taylor series expansion of $\bar{V}\left(\theta\right)$ about the `true' parameter $\theta^{*}$.
\begin{equation}
\bar{V}\left(\hat{\theta}_{N}\right) \approx \bar{V}\left(\theta^{*}\right) + \dfrac{1}{2}\left(\hat{\theta}_{N} - \theta^{*}\right)^{\top}\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)\left(\hat{\theta}_{N} - \theta^{*}\right)
\end{equation}
Note that $\theta^{*}$ minimises $\bar{V}\left(\theta\right)$, so the gradient vanishes. Perform a similar expansion for $V_{N}\left(\theta, \mathcal{D}_{N}\right)$ about $\hat{\theta}_{N}$, noting that this is minimised at $\hat{\theta}_{N}$.
\begin{equation}
V_{N}\left(\theta^{*}, \mathcal{D}_{N}\right) \approx V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right) + \dfrac{1}{2}\left(\theta^{*} - \hat{\theta}_{N}\right)^{\top}\nabla_{\theta}^{2}\bar{V}\left(\hat{\theta}_{N}\right)\left(\theta^{*} - \hat{\theta}_{N}\right)
\end{equation}
Rearranging gives
\begin{equation}
V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right) \approx V_{N}\left(\theta^{*}, \mathcal{D}_{N}\right) - \dfrac{1}{2}\left(\hat{\theta}_{N} - \theta^{*}\right)^{\top}\nabla_{\theta}^{2}\bar{V}\left(\hat{\theta}_{N}\right)\left(\hat{\theta}_{N} - \theta^{*}\right)
\end{equation}
Take the expectation of the expression for $\bar{V}\left(\hat{\theta}_{N}\right)$:
\begin{equation}
\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right] \approx \bar{V}\left(\theta^{*}\right) + \dfrac{1}{2}\mathbb{E}\left[\operatorname{trace}\left(\left(\hat{\theta}_{N} - \theta^{*}\right)^{\top}\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)\left(\hat{\theta}_{N} - \theta^{*}\right)\right)\right]
\end{equation}
since $\bar{V}\left(\theta^{*}\right)$ is not a random quantity and the quadratic form is a scalar. Then since the trace is invariant to cyclic permutations,
\begin{equation}
\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right] \approx \bar{V}\left(\theta^{*}\right) + \dfrac{1}{2}\operatorname{trace}\left(\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)\mathbb{E}\left[\left(\hat{\theta}_{N} - \theta^{*}\right)^{\top}\left(\hat{\theta}_{N} - \theta^{*}\right)\right]\right)
\end{equation}
where we have used the facts that the trace is a linear operator (and so commutes with sums hence expectations), and $\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)$ is also not a random quantity. Then suppose that $\hat{\theta}_{N}$ has an asymptotic covariance of
\begin{equation}
\mathbb{E}\left[\left(\hat{\theta}_{N} - \theta^{*}\right)^{\top}\left(\hat{\theta}_{N} - \theta^{*}\right)\right] \to \dfrac{1}{N}C
\end{equation}
as $N\to\infty$. Then for large $N$,
\begin{equation}
\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right] \approx \bar{V}\left(\theta^{*}\right) + \dfrac{1}{2N}\operatorname{trace}\left(\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)C\right)
\end{equation}
Now take the expectation of the expression for $V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right)$:
\begin{equation}
\mathbb{E}\left[V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right)\right] \approx V_{N}\left(\theta^{*}, \mathcal{D}_{N}\right) - \dfrac{1}{2}\mathbb{E}\left[\operatorname{trace}\left(\left(\hat{\theta}_{N} - \theta^{*}\right)^{\top}\nabla_{\theta}^{2}\bar{V}\left(\hat{\theta}_{N}\right)\left(\hat{\theta}_{N} - \theta^{*}\right)\right)\right]
\end{equation}
as similar to before. For large $N$, $V_{N}\left(\theta^{*}, \mathcal{D}_{N}\right)\approx \bar{V}\left(\theta^{*}\right)$ and $\nabla_{\theta}^{2}\bar{V}\left(\hat{\theta}_{N}\right) \approx \nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)$ so
\begin{equation}
\mathbb{E}\left[V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right)\right] \approx \bar{V}\left(\theta^{*}\right) - \dfrac{1}{2N}\operatorname{trace}\left(\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)C\right)
\end{equation}
Combining both these expectations, we see that
\begin{equation}
\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right] \approx \mathbb{E}\left[V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right)\right] + \dfrac{1}{N}\operatorname{trace}\left(\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)C\right)
\end{equation}
If our criterion is to minimise the expected prediction error on validation data $\mathbb{E}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right]$, then we seek to minimise the right hand side. Suppose that the prediction error is chosen to be the log-likelihood, ie.
\begin{equation}
V_{N}\left(\theta, \mathcal{D}_{N}\right)  = -\dfrac{1}{N}\log\mathcal{L}\left(\theta; \mathcal{D}_{N}\right)
\end{equation}
The best estimate of $\mathbb{E}\left[V_{N}\left(\hat{\theta}_{N}, \mathcal{D}_{N}\right)\right]$ is $-\dfrac{1}{N}\log\mathcal{L}\left(\hat{\theta}_{N}; \mathcal{D}_{N}\right)$ itself (which becomes increasingly better for large $N$), while the asymptotic covariance $C$ is the inverse of the Fisher information (ie. in this case $C = \nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)^{-1}$). Therefore an estimate of the expected prediction error is
\begin{align}
\widehat{\mathbb{E}}\left[\bar{V}\left(\hat{\theta}_{N}\right)\right] &= -\dfrac{1}{N}\max_{\theta}\log\mathcal{L}\left(\theta; \mathcal{D}_{N}\right) + \dfrac{\operatorname{trace}\left(\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)\nabla_{\theta}^{2}\bar{V}\left(\theta^{*}\right)^{-1}\right)}{N} \\
&= -\dfrac{1}{N}\log\mathcal{L}\left(\hat{\theta}_{N}; \mathcal{D}_{N}\right) + \dfrac{\dim\left(\hat{\theta}_{N}\right)}{N}
\end{align}
this is the same as the expression above for AIC (up to multiplication by a constant). Intuitively, the AIC aims to trade off between a high likelihood and low model complexity (measured by the number of parameters) by introducing a penalty on the number of parameters, since too high model complexity may lead to overfitting or loss in interpretability of the model. The AIC can be used for model selection, where we select the model with the lowest AIC. There also exist formulae for corrected AIC based on sample size $N$ \cite{Claeskens2008}.

\subsection{Bayesian Information Criterion}

The BIC of a model parametrised by $\theta$ can be defined in terms of its maximum log likelihood $\log\mathcal{L}\left(\theta\right)$ from $n$ observations.
\begin{equation}
\operatorname{BIC}\left(\theta\right) = n\dim\left(\theta\right) - 2\max_{\theta}\left\{\log\mathcal{L}\left(\theta\right)\right\}
\end{equation}
The BIC is also used as a model selection technique, where we aim to select the model with lowest BIC (as defined above).

\subsection{Deviance Information Criterion}

\section{Sanov's Theorem}

\section{Minimum Description Length}

\section{$f$-Divergence}

\subsection{Total Variation Distance}

\subsection{Hellinger Distance}

\section{Information Geometry}

\chapter{Econometrics}

\section{Model Specification}

\subsection{Log Models}

Suppose we have a simple linear model of the form
\begin{equation}
\log y = \beta_{0} + \beta_{1}x + u
\end{equation}
where $u$ is the error term. Then $\beta_{1}\times 100\%$ may be interpreted as the percentage increase in $y$ for a 1 unit increase in $x$. To see this, first note $\dfrac{\partial \log y}{y} = \dfrac{1}{y}$ and $\dfrac{\partial \log y}{\partial x} = \beta_{1}$. This gives $\beta_{1} = \dfrac{\partial y}{\partial x}\cdot\dfrac{1}{y}$. So for a 1 unit increase in $x$ we have $\beta_{1} \approx \dfrac{\Delta y}{y}$ which is the relative change in $y$.

\subsection{Log-Log Models}

Given a log-log specification between two variables, eg.
\begin{equation}
\log y = \beta_{0} + \beta_{1}\log x + u
\end{equation}
we can interpret $\beta_{1}$ (or its estimate) as ratio of percentage changes. This is also known as the $y$-elasticity of $x$. That is, for a 1\% increase in $x$, there is a $\beta_{1}$ percent increase in $y$, holding all else constant. To see this, first write
\begin{equation}
\dfrac{\Delta y}{y} \div \dfrac{\Delta x}{x} = \beta_{1} 
\end{equation}
Then for small changes in $x$ and $y$
\begin{gather}
\dfrac{dy}{y} = \beta_{1}\dfrac{dx}{x} \\
\int\dfrac{1}{y}dy = \beta_{1}\int\dfrac{1}{x}dx \\
\log\left|y\right| = \beta_{1}\log\left|x\right|
\end{gather}
Note that this implicitly requires $x$ and $y$ to be positive variables.

\section{Instrumental Variables}

\section{Panel Data Regression}

\section{Time-Series Models}

\subsection{Autoregressive (AR) Models}
An autoregressive model of order $p$ is denoted $AR\left(p\right)$ and is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + \varepsilon_{t}
\end{equation}
where $\varphi_{1}, \dots, \varphi_{2}$ are parameters of the model, $c$ is a constant term and $\varepsilon_{t}$ is white noise.

\subsection{Moving Average (MA) Models}
A moving average model of order $q$ is denoted $MA\left(q\right)$ and is defined by
\begin{equation}
X_{t} = \mu + \varepsilon_{t} + \sum_{i = 1}^{q}\theta_{i}\varepsilon_{t - i}
\end{equation}
where $\mu$ is the mean of the series, $\theta_{1}, \dots, \theta_{q}$ are the parameters of the model and $\varepsilon_{t}, \dots, \varepsilon_{t - q}$ are white noise error terms.

\subsection{Autoregressive Moving Average (ARMA) Models}
An ARMA model is a generalisation of the autoregressive and moving average models, and is denoted by $ARMA\left(p, q\right)$. The model is defined by
\begin{equation}
X_{t} = c + \varepsilon_{t} + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + \sum_{i = 1}^{q}\theta_{i}\varepsilon_{t - i}
\end{equation}

\subsection{Autoregressive Integrated Moving Average (ARIMA) Models}
An ARIMA model is a generalisation of the ARMA model, and is denoted by $ARIMA\left(p, d, q\right)$. Denoting $L$ as the lag operator (ie. $L^{i}X_{t} = X_{t - i}$), the model is defined by
\begin{equation}
\left(1 - \sum_{i = 1}^{p}\varphi_{i}L^{i}\right)\left(1 - L\right)^{d}X_{t} = \left(1 + \sum_{i = 1}^{d}\theta_{i}L^{i}\right)\varepsilon_{t}
\end{equation}
Using the binomial expansion, we can write
\begin{align}
\left(1 - \sum_{i = 1}^{p}\varphi_{i}L^{i}\right)\left(1 - L\right)^{d} &= \left(1 - \sum_{i = 1}^{p}\varphi_{i}L^{i}\right)\sum_{j = 0}^{d} \begin{pmatrix}
d \\ j
\end{pmatrix} \left(-L\right)^{d - j}  \\
&= \sum_{j = 0}^{d} \begin{pmatrix}
d \\ j
\end{pmatrix} \left(-1\right)^{d - j}L^{d - j} - \sum_{i = 1}^{p}\sum_{j = 0}^{d}\varphi_{i}\begin{pmatrix}
d \\ j
\end{pmatrix}\left(-1\right)^{d- j}L^{d - j + i}
\end{align}
Hence an alternative specification of the model is
\begin{equation}
\sum_{j = 0}^{d} \begin{pmatrix}
d \\ j
\end{pmatrix} \left(-1\right)^{d - j}X_{t - d + j} - \sum_{i = 1}^{p}\sum_{j = 0}^{d}\varphi_{i}\begin{pmatrix}
d \\ j
\end{pmatrix}\left(-1\right)^{d- j}X_{t - d + j - i} = \varepsilon_{t} + \sum_{i = 1}^{d}\theta_{i}\varepsilon_{t - i}
\end{equation}

\subsection{Autoregressive Moving Average with Exogenous Inputs (ARMAX) Models}
An ARMAX model is a generalisation of the ARMA model, and is denoted by $ARMAX\left(p, q, b\right)$. The model is defined by
\begin{equation}
X_{t} = \varepsilon_{t} + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + \sum_{i = 1}^{q}\theta_{i}\varepsilon_{t - i} + \sum_{i = 1}^{b}\eta_{i}d_{t - i}
\end{equation}
where $\eta_{1}, \dots, \eta_{b}$ are parameters of the exogeneous input $d_{t}$.

\subsection{Autoregressive Conditional Heteroskedasticity (ARCH) Models}
An ARCH model is a generalisation of the AR model, and is denoted by $ARCH\left(q\right)$. The model defines for the error term
\begin{equation}
\varepsilon_{t} = \sigma_{t}z_{t}
\end{equation}
where $z_{t}$ is white noise and $\sigma^{2}_{t}$ is modelled by
\begin{equation}
\sigma^{2}_{t} = \omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i}
\end{equation}
where $\omega > 0$ is a constant and $\alpha_{1}, \dots, \alpha_{q} > 0$ are parameters. Hence the full model is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + z_{t}\sqrt{\omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i}}
\end{equation}

\subsection{Generalised Autoregressive Conditional Heteroskedasticity (GARCH) Models}
A GARCH model is a generalisation of the ARCH model, and is denoted by $GARCH\left(p, q\right)$. The term $\sigma^{2}_{t}$ is now modelled by
\begin{equation}
\sigma^{2}_{t} = \omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i} + \sum_{i = 1}^{p}\beta_{i}\sigma^{2}_{i - 1}
\end{equation}
where $\beta_{1}, \dots, \beta{p} > 0$ are parameters. The full model is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + z_{t}\sqrt{\omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i} + \sum_{i = 1}^{p}\beta_{i}\sigma^{2}_{i - 1}}
\end{equation}

\subsection{Vector Autoregressive (VAR) Models}
A VAR model is a generalisation of the AR model, and is denoted by  $VAR\left(p\right)$. The model is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}A_{i}X_{t - i} + \varepsilon_{t}
\end{equation}
where $X_{t}$ is a stochastic vector, $c$ is a constant vector, $A_{1}, \dots, A_{p}$ are square matrix parameters, and $\varepsilon_{t}$ is a zero-mean vector error term with no correlation across time, ie. $\mathbb{E}\left[\varepsilon_{t}\varepsilon_{t - k}^{\top}\right] = 0$ for any non-zero $k$.

\subsection{Nonlinear Autoregressive Exogeneous (NARX) Models}
A NARX model is a generalisation of the AR model, and is denoted by $NARX\left(p, b\right)$. The model is defined by
\begin{equation}
X_{t} = F\left(X_{t - 1}, \dots, X_{t - p}, d_{t}, d_{t - 1}, \dots, d_{t - b}\right) + \varepsilon_{t}
\end{equation}
where $F$ is a nonlinear function.

\section{Time-Series Regression}

\subsection{Residual Autocorrelation}

Also known as serial correlation in the residuals, residual autocorrelation is when there is evidence of correlation between the residuals and past lags. This suggests that
\begin{equation}
\mathbb{E}\left[U_{t}\middle|\mathcal{F}_{t - 1}\right] \neq 0
\end{equation}
where $U_{t}$ is the error term and $\mathcal{F}_{t - 1}$ is information up to and including time $t - 1$. What this is saying that if errors are autocorrelated, then in principle it is possible to predict future errors from past information. However this goes against the definition of the error, which is
\begin{equation}
U_{t} = Y_{t} - \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t - 1}\right]
\end{equation}
Taking $\mathbb{E}\left[\cdot\middle|\mathcal{F}_{t - 1}\right]$ of both sides, we get
\begin{equation}
\mathbb{E}\left[U_{t}\middle|\mathcal{F}_{t - 1}\right] = \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t - 1}\right] - \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t - 1}\right]
\end{equation}
The RHS evaluates to zero, however evidence of residual autocorrelation suggests the LHS is not zero, which results in a contradiction. Hence this gives the implication that the model specification is not the correct specification for the actual conditional mean.

\subsection{Granger Causality}

\subsection{Unit Root}

\subsubsection{Stationarity in AR(1) Models}

Consider an AR(1) model
\begin{equation}
\mathbb{E}\left[Y_{t} \middle|\mathcal{F}_{t-1}\right] = \beta_{0} + \beta_{1}Y_{t - 1}
\end{equation}
with $\left|\beta_{1}\right| < 1$. Using the Law of Iterated Expectations,
\begin{equation}
\mathbb{E}\left[Y_{t}\right] = \beta_{0} + \beta_{1}\mathbb{E}\left[Y_{t - 1}\right]
\end{equation}
Suppose this time series is weakly stationary, then we have $\mathbb{E}\left[Y_{t}\right] = \mathbb{E}\left[Y_{t - 1}\right] = \mu$. Solving for $\mu$ gives
\begin{equation}
\mu = \dfrac{\beta_{0}}{1 - \beta_{1}}
\end{equation}
Now also suppose we have homoskedasticity, ie. the conditional variance of $Y_{t}$ is a constant given by $\operatorname{Var}\left(Y_{t}\middle|\mathcal{F}_{t-1}\right) = \omega^{2}$. Then using the Law of Total Variance:
\begin{align}
\operatorname{Var}\left(Y_{t}\right) &= \mathbb{E}\left[\operatorname{Var}\left(Y_{t}\middle|\mathcal{F}_{t-1}\right)\right] + \operatorname{Var}\left( \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t-1}\right]\right) \\
&= \mathbb{E}\left[\omega^{2}\right] + \operatorname{Var}\left(\beta_{0} + \beta_{1}Y_{t - 1}\right) \\
&= \omega^{2} + \beta_{1}\operatorname{Var}\left(Y_{t - 1}\right)
\end{align}
From weak stationarity we have $\operatorname{Var}\left(Y_{t}\right) = \operatorname{Var}\left(Y_{t - 1}\right) = \sigma^{2}$. Solving for $\sigma^{2}$ gives
\begin{equation}
\sigma^{2} = \dfrac{\omega^{2}}{1 - \beta_{1}^{2}}
\end{equation}
To determine $\operatorname{Cov}\left(Y_{t}, Y_{t - j}\right)$ for an arbitrary lag length $j$, for convenience define $Y_{t}' := Y_{t} - \mu$ so $\mathbb{E}\left[Y_{t}'\right] = 0$ which then implies
\begin{equation}
\mathbb{E}\left[Y_{t}'\middle|\mathcal{F}_{t-1}\right] = \beta_{1}Y_{t - 1}'
\end{equation}
and
\begin{equation}
\mathbb{E}\left[Y_{t}'\middle|\mathcal{F}_{t-j}\right] = \beta_{1}^{j}Y_{t - j}'
\end{equation}
Multiplying both sides by $Y_{t - j}'$ gives
\begin{equation}
Y_{t - j}'\mathbb{E}\left[Y_{t}'\middle|\mathcal{F}_{t-j}\right] = \beta_{1}^{j}Y_{t - j}'^{2}
\end{equation}
Since $Y_{t - j}' \in \mathcal{F}_{t- j}$ then
\begin{equation}
\mathbb{E}\left[Y_{t}'Y_{t - j}'\middle|\mathcal{F}_{t-j}\right] = \beta_{1}^{j}Y_{t - j}'^{2}
\end{equation}
Taking expectations of both sides yields
\begin{equation}
\mathbb{E}\left[\mathbb{E}\left[Y_{t}'Y_{t - j}'\middle|\mathcal{F}_{t-j}\right]\right] = \beta_{1}^{j}\mathbb{E}\left[Y_{t - j}'^{2}\right]
\end{equation}
Then by the Law of Iterated Expectations
\begin{equation}
\mathbb{E}\left[Y_{t}'Y_{t - j}'\right] = \beta_{1}^{j}\mathbb{E}\left[Y_{t - j}'^{2}\right]
\end{equation}
As $Y_{t}'$ and $Y_{t - j}'$ are zero mean, this can be rewritten as
\begin{equation}
\operatorname{Cov}\left(Y_{t}', Y_{t - j}'\right) = \beta_{1}^{j}\operatorname{Var}\left(Y_{t - j}\right)
\end{equation}
And finally by $\operatorname{Cov}\left(Y_{t}, Y_{t - j}\right) = \operatorname{Cov}\left(Y_{t}', Y_{t - j}'\right)$ and weak stationarity this becomes
\begin{align}
\operatorname{Cov}\left(Y_{t}, Y_{t - j}\right) &= \beta_{1}^{j}\operatorname{Var}\left(Y_{t}\right) \\
&= \dfrac{\beta_{1}^{j}\omega^{2}}{1 - \beta_{1}^{2}}
\end{align}

\section{Time-Series Analysis}

\chapter{Machine Learning}

\section{Classification Algorithms}

\subsection{Datasets}

\subsection{Performance Metrics in Classification \cite{Goodfellow2016}}

\subsubsection{Accuracy}

The accuracy of a classifier is the proportion of examples that the classifier gets correct.

\subsubsection{Error Rate}

The accuracy of a classifier is the proportion of examples that the classifier gets incorrect, which is the complement of accuracy.

\subsubsection{Bayes Error}

The Bayes error rate is the irreducible part of error rate on new examples, even with infinite training data and knowledge of the true underlying distribution. This may be because the system is inherently stochastic.

\subsubsection{Precision}

Suppose there is a binary classifier that detects an event. The precision is the proportion of detections that were correct. Let $E$ represent the event to be detected and $D$ the event of a detection. Then by Bayes' theorem, the precision can be thought to be ideally estimating
\begin{equation}
\operatorname{Pr}\left(E\middle|D\right) = \dfrac{\operatorname{Pr}\left(D\middle|E\right)\operatorname{Pr}\left(E\right)}{\operatorname{Pr}\left(D\right)}
\end{equation}
where $\operatorname{Pr}\left(D\middle|E\right)$ would be referred as the power.

\subsubsection{Recall}

For a binary classifier that detects an event, recall is the proportion of true events that were detected, ie. it estimates $\operatorname{Pr}\left(D\middle|E\right)$. This is akin to the power of a statistical test.

\subsubsection{Precision/Recall $F$-score}

Given precision $p$ and recall $r$, an `$F$-score' can be calculated by
\begin{equation}
F = \dfrac{2pr}{p + r}
\end{equation}
which is a way to quantitatively trade between precision and recall, where a model with higher $F$-score is preferred.

\subsubsection{Coverage}

A classifier may also be coded so that it can refuse to make a decision (eg. if it is not certain enough about a particular example). The coverage is defined as the fraction of examples for which the classifier is able to make a decision.

\subsection{Confusion Matrices}

\subsection{$k$-Nearest Neighbours}

\subsection{Support Vector Machines}

\subsection{Linear Discriminant Analysis}

\section{Neural Networks}

\subsection{Multilayer Perceptrons}

For an input vector $x\in\mathbb{R}^{Nx}$ with $x=\begin{bmatrix}x_{1} & \dots & x_{Nx}\end{bmatrix}^{\top}$, we have an associated (target/observed) output vector $y\in\mathbb{R}^{Ny}$ with $y=\begin{bmatrix}y_{1} & \dots & y_{Ny}\end{bmatrix}^{\top}$. Let there be $L + 1$ layers in the network, with structure given by
\begin{equation}
\left\{Nx,N1,N2,\dots,N\ell,\dots,NL\right\} \in \mathbb{N}^{L + 1}
\end{equation}
which denotes the number of nodes (size) of each layer. Note that $NL\equiv Ny$. For each layer excluding the input layer, there is an `intermediate output' $\mathstrut^{\ell}z\in\mathbb{R}^{N\ell}$, an activation $\mathstrut^{\ell}a\in\mathbb{R}^{N\ell}$, a bias $\mathstrut^{\ell}b\in\mathbb{R}^{N\ell}$, some weights $\mathstrut_{\ell-1}^{\ell}w\in\mathbb{R}^{N\ell\times N\left(\ell-1\right)}$ and an activation function $\mathstrut^{\ell}\sigma\left(\cdot\right)$. Their relationship in each layer from the one before it is given by
\begin{equation}
\mathstrut^{\ell}z=\mathstrut_{\ell-1}^{\ell}w\mathstrut^{\ell-1}a+\mathstrut^{\ell}b
\end{equation}
and $\mathstrut^{0}a$ can be taken to equal $x$. Denote
\begin{equation}
\mathstrut^{\ell}\boldsymbol{\sigma}\left(\mathstrut^{\ell}z\right)=\begin{bmatrix}\mathstrut^{\ell}\sigma\left(\mathstrut^{\ell} z_{1}\right)\\
\vdots\\
\mathstrut^{\ell}\sigma\left(\mathstrut^{\ell}z_{N}\right)
\end{bmatrix}
\end{equation}
so that
\begin{equation}
\mathstrut^{\ell}a=\mathstrut^{\ell}\boldsymbol{\sigma}\left(\mathstrut^{\ell}z\right)
\end{equation}
The feedforward function is
\begin{equation}
f\left(x\right)=\mathstrut^{L}a=\mathstrut^{L}\boldsymbol{\sigma}\left(\mathstrut_{L-1}^{L}w\mathstrut^{L-1}a+\mathstrut^{L}b\right)
\end{equation}
Suppose we have a training data set indexed by
\begin{gather}
X=\left\{ x\left[1\right],\dots,x\left[n\right]\right\} \\
Y=\left\{ y\left[1\right],\dots,y\left[n\right]\right\} 
\end{gather}
Use the cost function
\begin{equation}
C=\dfrac{1}{2n}\sum_{i=1}^{n}\left\Vert f\left(x\left[i\right]\right)-y\left[i\right]\right\Vert ^{2}
\end{equation}
This can be broken up into a cost for each training point:
\begin{equation}
C_{i}=\dfrac{1}{2}\left\Vert f\left(x\left[i\right]\right)-y\left[i\right]\right\Vert ^{2}=\dfrac{1}{2}\left(f_{1}\left(x\left[i\right]\right)-y_{1}\left[i\right]\right)^{2}+\dots+\dfrac{1}{2}\left(f_{Ny}\left(x\left[i\right]\right)-y_{Ny}\left[i\right]\right)^{2}
\end{equation}
so that
\begin{equation}
C=\dfrac{1}{n}\sum_{i=1}^{n}C_{i}
\end{equation}
Using this cost function, the gradient descent equations for the weights and biases in each layer are
\begin{gather}
\mathstrut_{\ell-1}^{\ell}w\rightarrow\mathstrut_{\ell-1}^{\ell}w-\eta\dfrac{\partial C}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top} \\
\mathstrut^{\ell}b\rightarrow\mathstrut^{\ell}b-\eta\dfrac{\partial C}{\partial\mathstrut^{\ell}b}^{\top}
\end{gather}
where $\eta$ is the learning rate. For ease of notation we will use for a given $i$: $c := C_{i}$, $f := f\left(x\right)\left[i\right]$ and $y:= y\left[i\right]$. For example, instead of using the cumbersome notation for the gradient vector
\begin{equation}
\nabla_{f}C_{i}=\dfrac{\partial C_{i}}{\partial f}^{\top}=\begin{bmatrix}\dfrac{\partial C_{i}}{\partial f_{1}} & \dots & \dfrac{\partial C_{i}}{\partial f_{Ny}}\end{bmatrix}^{\top}=\begin{bmatrix}f_{1}\left(x\left[i\right]\right)-y_{1}\left[i\right]\\
\vdots\\
f_{Ny}\left(x\left[i\right]\right)-y_{Ny}\left[i\right]
\end{bmatrix}
\end{equation}
we can instead write
\begin{equation}
\nabla_{f}c=\begin{bmatrix}f_{1}-y_{1}\\
\vdots\\
f_{Ny}-y_{Ny}
\end{bmatrix}
\end{equation}
To make our way to finding $\dfrac{\partial C}{\partial\mathstrut_{\ell-1}^{\ell}w}$ and $\dfrac{\partial C}{\partial\mathstrut^{\ell}b}$, first consider $\nabla_{\mathstrut^{L}z}c$. We have from the chain rule
\begin{equation}
\nabla_{\mathstrut^{L}z}c=\begin{bmatrix}\dfrac{\partial c}{\partial\mathstrut^{L}z_{1}}\\
\vdots\\
\dfrac{\partial c}{\partial\mathstrut^{L}z_{NL}}
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial f_{1}}\times\dfrac{\partial f_{1}}{\partial\mathstrut^{L}z_{1}}\\
\vdots\\
\dfrac{\partial c}{\partial f_{NL}}\times\dfrac{\partial f_{NL}}{\partial\mathstrut^{L}z_{NL}}
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial f_{1}}\cdot\mathstrut^{L}\sigma'\left(\mathstrut^{L}z\right)\\
\vdots\\
\dfrac{\partial c}{\partial f_{NL}}\cdot\mathstrut^{L}\sigma'\left(\mathstrut^{L}z\right)
\end{bmatrix}=\nabla_{f}c\odot\mathstrut^{L}\boldsymbol{\sigma}'\left(\mathstrut^{L}z\right)
\end{equation}
where $\odot$ is the Hadamard (element-wise) product. Denote the `error' in each layer as $\mathstrut^{\ell}\delta:=\nabla_{\mathstrut^{\ell}z}c$. For the final layer, we have
\begin{equation}
\mathstrut^{L}z=\mathstrut_{L-1}^{L}w\mathstrut^{L-1}a+\mathstrut^{L}b
\end{equation}
It can be easily seen that $\dfrac{\partial\mathstrut^{L}z}{\partial\mathstrut^{L}b}=I$, so this gives
\begin{equation}
\dfrac{\partial C}{\partial\mathstrut^{L}b}^{\top}=\nabla_{\mathstrut^{L}z}c=\mathstrut^{L}\delta
\end{equation}
and in general for any layer
\begin{equation}
\dfrac{\partial C}{\partial\mathstrut^{\ell}b}^{\top}=\nabla_{\mathstrut^{\ell}z}c=\mathstrut^{\ell}\delta
\end{equation}
For the following steps use for ease of notation: $z:=\mathstrut^{\ell}z$, $b:=\mathstrut^{\ell}b$,  $a:=\mathstrut^{\ell-1}a$ and $w:=\mathstrut_{\ell-1}^{\ell}w$. Then write out the following sums:
\begin{equation}
\begin{bmatrix}z_{1}\\
z_{2}\\
\vdots\\
z_{N}
\end{bmatrix}=\begin{bmatrix}w_{11}a_{1}+w_{12}a_{2}+\dots+w_{1N}a_{N}+b_{1}\\
w_{21}a_{1}+w_{22}a_{2}+\dots+w_{2N}a_{N}+b_{2}\\
\vdots\\
w_{N1}a_{1}+w_{N2}a_{2}+\dots+w_{NN}a_{N}+b_{N}
\end{bmatrix}
\end{equation}
This is helpful for seeing that if we compute the derivative of scalar by matrix $\dfrac{\partial z_{1}}{\partial w}$, this gives
\begin{equation}
\dfrac{\partial z_{1}}{\partial w}=\begin{bmatrix}\dfrac{\partial z_{1}}{\partial w_{11}} & \dfrac{\partial z_{1}}{\partial w_{21}} & \dots\\
\dfrac{\partial z_{1}}{\partial w_{12}} & \ddots\\
\vdots
\end{bmatrix}=\begin{bmatrix}a_{1} & 0 & \dots\\
a_{2} & 0 & \dots\\
\vdots & \vdots & \ddots
\end{bmatrix}
\end{equation}
The derivative of the cost with respect to the weights are
\begin{equation}
\dfrac{\partial c}{\partial w}=\begin{bmatrix}\dfrac{\partial c}{\partial w_{11}} & \dfrac{\partial c}{\partial w_{21}} & \dots\\
\dfrac{\partial c}{\partial w_{12}} & \ddots\\
\vdots
\end{bmatrix}
\end{equation}
Consider the first element, this will consist of all the contributions from elements of $z$ as so
\begin{equation}
\dfrac{\partial c}{\partial w_{11}}=\dfrac{\partial c}{\partial z_{1}}\dfrac{\partial z_{1}}{\partial w_{11}}+\dfrac{\partial c}{\partial z_{2}}\green\cancelto{0}{\black\dfrac{\partial z_{2}}{\partial w_{11}}}\black+\green\cancelto{0}{\black\left[\dots\right]}\black=\dfrac{\partial c}{\partial z_{1}}a_{1}
\end{equation}
The element $\dfrac{\partial c}{\partial w_{12}}$ is similarly given by
\begin{equation}
\dfrac{\partial c}{\partial w_{12}}=\dfrac{\partial c}{\partial z_{1}}\dfrac{\partial z_{1}}{\partial w_{12}}+\dfrac{\partial c}{\partial z_{2}}\green\cancelto{0}{\black\dfrac{\partial z_{2}}{\partial w_{12}}}\black+\green\cancelto{0}{\black\left[\dots\right]}\black=\dfrac{\partial c}{\partial z_{1}}a_{2}
\end{equation}
Thus
\begin{equation}
\dfrac{\partial c}{\partial w}^{\top}=\begin{bmatrix}\dfrac{\partial c}{\partial w_{11}} & \dfrac{\partial c}{\partial w_{12}} & \dots\\
\dfrac{\partial c}{\partial w_{21}} & \ddots\\
\vdots
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial z_{1}}a_{1} & \dfrac{\partial c}{\partial z_{1}}a_{2} & \dots\\
\dfrac{\partial c}{\partial z_{2}}a_{1} & \ddots\\
\vdots
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial z_{1}}\\
\dfrac{\partial c}{\partial z_{2}}\\
\vdots
\end{bmatrix}\begin{bmatrix}a_{1} & a_{2} & \dots\end{bmatrix}
\end{equation}
Reverting back to notation which specifies the layers, we have
\begin{equation}
\dfrac{\partial c}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top}=\mathstrut^{\ell}\delta\cdot\mathstrut^{\ell-1}a^{\top}
\end{equation}
Next, consider the relationship between $\mathstrut^{\ell - 1}\delta$ and $\mathstrut^{\ell}\delta$. Computing $\mathstrut^{\ell - 1}\delta$ from $\mathstrut^{\ell}\delta$ is known as backpropogation. First write
\begin{equation}
\mathstrut^{\ell}z=\mathstrut_{\ell-1}^{\ell}w\mathstrut^{\ell}\boldsymbol{\sigma}\left(\mathstrut^{\ell-1}z\right)+\mathstrut^{\ell}b
\end{equation}
For ease of notation, use $\sigma\left(\cdot\right):=\mathstrut^{\ell}\sigma\left(\cdot\right)$. Then writing out the sums arising from the matrix multiplication gives
\begin{equation}
\begin{bmatrix}\mathstrut^{\ell}z_{1}\\
\mathstrut^{\ell}z_{2}\\
\vdots
\end{bmatrix}=\begin{bmatrix}\mathstrut_{\ell-1}^{\ell}w_{11}\sigma\left(\mathstrut^{\ell-1}z_{1}\right)+\mathstrut_{\ell-1}^{\ell}w_{12}\sigma\left(\mathstrut^{\ell-1}z_{2}\right)+\dots+\mathstrut^{\ell}b_{1}\\
\mathstrut_{\ell-1}^{\ell}w_{21}\sigma\left(\mathstrut^{\ell-1}z_{1}\right)+\mathstrut_{\ell-1}^{\ell}w_{22}\sigma\left(\mathstrut^{\ell-1}z_{2}\right)+\dots+\mathstrut^{\ell}b_{2}\\
\vdots
\end{bmatrix}
\end{equation}
We want to find
\begin{equation}
\mathstrut^{\ell-1}\delta=\begin{bmatrix}\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}\\
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{2}}\\
\vdots
\end{bmatrix}
\end{equation}
First consider $\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}$ as the contribution from all $\mathstrut^{\ell}z_{1}$, $\mathstrut^{\ell}z_{2}$, etc.
\begin{equation}
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}=\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot \red \dfrac{\partial\mathstrut^{\ell}z_{1}}{\partial\mathstrut^{\ell-1}z_{1}}\black +\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot \blue \dfrac{\partial\mathstrut^{\ell}z_{2}}{\partial\mathstrut^{\ell-1}z_{1}}\black+\dots
\end{equation}
Calculating some of these terms using the chain rule gives
\begin{gather}
\red\dfrac{\partial\mathstrut^{\ell}z_{1}}{\partial\mathstrut^{\ell-1}z_{1}}\black=\mathstrut_{\ell-1}^{\ell}w_{11}\sigma'\left(\mathstrut^{\ell-1}z_{1}\right) \\
\blue\dfrac{\partial\mathstrut^{\ell}z_{2}}{\partial\mathstrut^{\ell-1}z_{1}}\black=\mathstrut_{\ell-1}^{\ell}w_{21}\sigma'\left(\mathstrut^{\ell-1}z_{1}\right) \\
\vdots
\end{gather}
So then
\begin{equation}
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}=\left(\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot\mathstrut_{\ell-1}^{\ell}w_{11}+\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot\mathstrut_{\ell-1}^{\ell}w_{21}+\dots\right)\sigma'\left(\mathstrut^{\ell-1}z_{1}\right)
\end{equation}
and more generally,
\begin{equation}
\begin{bmatrix}\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}\\
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{2}}\\
\vdots
\end{bmatrix}=\begin{bmatrix}\left(\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot\mathstrut_{\ell-1}^{\ell}w_{11}+\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot\mathstrut_{\ell-1}^{\ell}w_{21}+\dots\right)\sigma'\left(\mathstrut^{\ell-1}z_{1}\right)\\
\left(\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot\mathstrut_{\ell-1}^{\ell}w_{12}+\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot\mathstrut_{\ell-1}^{\ell}w_{22}+\dots\right)\sigma'\left(\mathstrut^{\ell-1}z_{2}\right)\\
\vdots
\end{bmatrix}
\end{equation}
Notice that we may then write this as
\begin{equation}
\mathstrut^{\ell-1}\delta=\left(\mathstrut_{\ell-1}^{\ell}w^{\top}\cdot\mathstrut^{\ell}\delta\right)\odot\mathstrut^{\ell-1}\boldsymbol{\sigma}'\left(\mathstrut^{\ell-1}z\right)
\end{equation}
Now reverting back to notation that is indexed by each training point $i$: $\mathstrut^{\ell}\delta\left[i\right]=\dfrac{\partial C_{i}}{\partial\mathstrut^{\ell}z}$. Since we have found the derivative of each individual cost with respect to the weights, the derivative of the total cost with respect to the weights is
\begin{gather}
\dfrac{\partial C}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top} = \dfrac{1}{n}\sum_{i=1}^{n}\dfrac{\partial C_{i}}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top} = \dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]\cdot\mathstrut^{\ell-1}a\left[i\right]^{\top} \\
\dfrac{\partial C}{\partial\mathstrut^{\ell}b}^{\top}= \dfrac{1}{n}\sum_{i=1}^{n}\dfrac{\partial C_{i}}{\partial\mathstrut^{\ell}b}^{\top} = \dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]
\end{gather}
So the gradient descent equations using the entire data set are
\begin{gather}
\mathstrut_{\ell-1}^{\ell}w\rightarrow\mathstrut_{\ell-1}^{\ell}w-\eta\cdot\dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]\cdot\mathstrut^{\ell-1}a\left[i\right]^{\top} \\
\mathstrut^{\ell}b\rightarrow\mathstrut^{\ell}b-\eta\cdot\dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]
\end{gather}
If the number of training points is very large, then gradient descent can take a long time, so learning occurs slowly. An idea is to use stochastic gradient descent, where a random mini-batch of $m$ inputs is chosen to approximate the gradient of $C$ and update the weights according to
\begin{gather}
\mathstrut_{\ell-1}^{\ell}w\rightarrow\mathstrut_{\ell-1}^{\ell}w-\eta\cdot\dfrac{1}{m}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]\cdot\mathstrut^{\ell-1}a\left[i\right]^{\top} \\
\mathstrut^{\ell}b\rightarrow\mathstrut^{\ell}b-\eta\cdot\dfrac{1}{m}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]
\end{gather}
Then another mini-batch is chosen to update the weights. This occurs until the entirety of the training set is exhausted, in which this is called one training epoch. We may then start over again with a new epoch to continue training.

\subsection{Cross Entropy Loss Function}

In a classification model, we have data with training examples from $1, \dots, n$, and the outputs are the class probabilities for each of $1, \dots, M$ different classes. Denote by $y_{i, j}$ the $i$\textsuperscript{th} class probability of the $j$\textsuperscript{th} training example. Each training example target vector $\mathbf{y}_{j}$ should really be (but in a more general setting is not restricted to) a `one-hot' vector, eg. $\left(0, 1, 0\right)$. Similarly denote the estimates of class probabilities by the model as $\hat{y}_{i, j}$. A choice of loss function $L$ is the cross entropy loss function, given by
\begin{equation}
L\left(\mathbf{y}, \hat{\mathbf{y}}\right) = -\sum_{i = 1}^{M}y_{i}\log\hat{y}_{i}
\end{equation}
which is just the cross entropy between the `true' distribution $\mathbf{y}$ and the approximating distribution $\hat{\mathbf{y}}$. Then because the difference between the cross entropy and the KL divergence is only an additive constant (which is the entropy of $\mathbf{y}$, which we treat as fixed in the data), then minimising the cross entropy is the same as minimising the KL divergence between the estimated distribution and the actual distribution. Over the entire dataset, by assuming independence we can just sum over the cross entropy for each training example, leading to the cost function
\begin{equation}
J = -\sum_{j = 1}^{n}\sum_{i = 1}^{M}y_{i, j}\log\hat{y}_{i, j}
\end{equation}
Now suppose the estimated vector $\hat{y}$ is applied via a softmax of some values $z_{1}, \dots, z_{M}$, ie.
\begin{equation}
\hat{y}_{i} = \dfrac{\exp{\left(z_{i}\right)}}{\sum_{l = 1}^{M}\exp\left(z_{l}\right)}
\end{equation}
Then to derive the derivative of the loss with respect with the values $z_{1}, \dots, z_{M}$ (for use in backpropagation as an example), we get (for a particular $z_{k}$):
\begin{align}
\dfrac{\partial L\left(\mathbf{y}, \hat{\mathbf{y}}\right)}{\partial z_{k}} &= \dfrac{\partial}{\partial z_{k}}\left(-\sum_{i = 1}^{M}y_{i}\log \hat{y}_{i}\right) \\
&= \dfrac{\partial}{\partial z_{k}}\left(-\sum_{i=1}^{M}y_{i}\log\left(\dfrac{\exp\left(z_{i}\right)}{\sum_{l=1}^{M}\exp\left(z_{l}\right)}\right)\right) \\
&= \dfrac{\partial}{\partial z_{k}}\left(-\sum_{i=1}^{M}y_{i}z_{i}+\sum_{i=1}^{M}y_{i}\log\left(\sum_{l=1}^{M}\exp\left(z_{l}\right)\right)\right) \\
&= -y_{k}+\sum_{i=1}^{M}y_{i}\dfrac{\exp\left(z_{k}\right)}{\sum_{l=1}^{M}\exp\left(z_{l}\right)} \\
&= -y_{k}+\hat{y}_{k}\sum_{i=1}^{M}y_{i} \\
&= -y_{k}+\hat{y}_{k}
\end{align}
Hence the gradient vector is
\begin{equation}
\nabla_{\mathbf{z}}L\left(\mathbf{y}, \hat{\mathbf{y}}\right) = - \mathbf{y} + \hat{\mathbf{y}}
\end{equation}
and the gradient vector of the cost function is the sum over the examples
\begin{equation}
\nabla_{\mathbf{z}} J = \sum_{j = 1}^{n}\left(\hat{\mathbf{y}}_{j} - \mathbf{y}_{j}\right)
\end{equation}
We can arrive at the cross entropy cost function by considering a maximum likelihood approach. Suppose $\bar{\mathbf{y}}_{j}$ gives the actual probability distribution for example $j$. The likelihood of the data $\mathbf{y}_{1}, \dots, \mathbf{y}_{n}$ given $\overline{\mathbf{y}}_{j}$ is denoted $\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right)$ and represented by
\begin{equation}
\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = \operatorname{Pr}\left(\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\middle|\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\right)
\end{equation}
In the case $\mathbf{y}_{j}$ are one-hot vectors and all independent, this becomes
\begin{equation}
\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = \prod_{j = 1}^{n}\overline{y}_{\left\{i: y_{i, j} = 1\right\}, j}
\end{equation}
where $\left\{i: y_{i, j} = 1\right\}$ express the index of the class for which was actually observed in the $j$\textsuperscript{th} training example. Hence $\overline{y}_{\left\{i: y_{i, j} = 1\right\}, j}$ is just the probability of the class that was actually observed. Note that if $\mathbf{y}_{j}$ were not one-hot vectors, this still generalises well to
\begin{equation}
\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = \prod_{j = 1}^{n}\prod_{i = 1}^{M}\overline{y}_{i, j}\mathstrut^{y_{i, j}}
\end{equation}
To illustrate, consider a $n = 100$ trials in an i.i.d. multinomial experiment with $M = 3$ for example. If we obtained 40 in class 1, 25 in class 2 and 35 in  class 3, then the likelihood would be written as
\begin{align}
\mathcal{L}\left(\overline{y}_{1}, \overline{y}_{2}, \overline{y}_{3}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{100}\right) &= \overline{y}_{1}^{40}\overline{y}_{2}^{25}\overline{y}_{3}^{35} \\
&= \left(\overline{y}_{1}^{0.4}\overline{y}_{2}^{0.25}\overline{y}_{3}^{0.35}\right)^{100}
\end{align}
Thus we can more generally view the likelihood as the product over all the training examples of the weighted geometric mean of the true class probabilities $\overline{\mathbf{y}}_{j}$ (weighted by the class probabilities for that training example). Taking the negative log likelihood gives
\begin{equation}
-\log\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = -\sum_{j = 1}^{n}\sum_{i = 1}^{M}\overline{y}_{i, j}\log{y_{i, j}}
\end{equation}
If we find the estimates $\hat{\mathbf{y}}_{j}$ (in terms of the parameters of the model) of $\overline{\mathbf{y}}_{j}$ which minimise this negative log likelihood, then it becomes the same problem as minimising the cross entropy.

\section{Convolutional Neural Networks}

\section{Recurrent Neural Networks}

\section{Ensemble Methods}

\subsection{Bagging}

A portmanteau of `bootstrap aggregation', the bagging technique trains several different models separately, and then `votes' for the output on test examples. Consider $k$ different models. In bagging, each of the models has been trained on a dataset which is resampled from the original training data with replacement. In a regression example, suppose the $i$\textsuperscript{th} model has prediction error $\epsilon_{i}$ on the test data, there $\epsilon_{i}$ has zero-mean. Let the expected squared error be $\mathbb{E}\left[\epsilon_{i}^{2}\right] = \operatorname{Var}\left(\epsilon\right) = v$, and let the covariances between errors on different models be $\operatorname{Cov}\left(\epsilon_{i}, \epsilon_{j}\right) = \mathbb{E}\left[\epsilon_{i}\epsilon_{j}\right] = c$. By averaging predictions across the ensemble, the ensemble prediction errors is given by $\dfrac{1}{k}\sum_{i = 1}^{k}\epsilon_{i}$. Then the expected squared error of the ensemble is
\begin{align}
\mathbb{E}\left[\left(\dfrac{1}{k}\sum_{i = 1}^{k}\epsilon_{i}\right)^{2}\right] &= \dfrac{1}{k^{2}}\mathbb{E}\left[\sum_{i = 1}^{k}\epsilon_{i}\sum_{j = 1}^{k}\epsilon_{j}\right] \\
&= \dfrac{1}{k^{2}}\mathbb{E}\left[\sum_{i = 1}^{k}\left(\epsilon_{i}^{2} + \sum_{j: j\neq i}^{k}\epsilon_{i}\epsilon_{j}\right)\right] \\
&= \dfrac{1}{k}\mathbb{E}\left[\epsilon_{i}^{2} + \sum_{j: j\neq i}^{k}\epsilon_{i}\epsilon_{j}\right] \\
&= \dfrac{1}{k}\operatorname{Var}\left(\epsilon\right) + \dfrac{k-1}{k}\operatorname{Cov}\left(\epsilon_{i}, \epsilon_{j}\right) \\
&= \dfrac{1}{k}v + \dfrac{k - 1}{k}c
\end{align}
Suppose errors are perfectly correlated and $c = v$ (eg. the exact same training data is used for each model). Then the mean squared error of the ensemble is $v$. On the other hand if errors are perfectly uncorrelated such that $c = 0$, then the mean squared error by the ensemble is only $\dfrac{1}{k}v$. It shows that in this case, the ensemble can never perform worse than any of its members \cite{Goodfellow2016}.

\subsection{Boosting}

A generic boosting algorithm takes a `weak learner' (eg. only slightly better than a random 50-50 guess on a classification problem), and produces a sequence of weak learners $G_{m}\left(x\right)$ for $m = 1, \dots, M$ to form a `strong' committee of learners that is a weighting of each weak learner. An example is that for a classification problem into classes $\left\{-1, 1\right\}$, this weighting is given by
\begin{equation}
G\left(x\right) = \operatorname{sign}\left(\sum_{m = 1}^{M}\alpha_{m}G_{m}\left(x\right)\right)
\end{equation}
At each step $m$, the training data is reweighted to put emphasis on the examples that the previous learner got wrong. For example, in AdaBoost \cite{Hastie2009, Zhou2012} (short for Adaptive Boosting), the weightings for each observation $\left(x_{i}, y_{i}\right)$ out of $N$ observations total start off at $w_{i} = \dfrac{1}{N}$. At each step $m$, the classifier $G_{m}\left(x\right)$ is fitted from the weighted data and the weighted classification error rate $e_{m}$ is computed. The learner weighted is calculated by
\begin{equation}
\alpha_{m} = \log\left(\dfrac{1 - e_{m}}{e_{m}}\right)
\end{equation}
and then each weight is updated according to
\begin{equation}
w_{i} \gets w_{i}\exp\left[\alpha_{m}\mathbb{I}\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)\right]
\end{equation}
where $\mathbb{I}\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)$ is an indicator function. The weightings can be re-normalised if needed. Thus we can see that the weight for an observation which the learner got wrong is increased, while the weight for an observation which the learner got correct stays the same. Also note that the learner weighting $\alpha_{m}$ is decreasing in the error rate $e_{m}$.

\subsection{Stacking}

In stacking \cite{Hastie2009, Zhou2012}, there are $M$ `first-level learners', and one `second-level learner'. These learners may be not necessarily be from the same class of learning algorithm. Let the estimate from the $m$\textsuperscript{th} learner be denoted $\hat{f}_{m}\left(x\right)$. Then out out $N$ training observations, let $\hat{f}_{m}^{\left(-i\right)}\left(x\right)$ denote the estimate with the $i$\textsuperscript{th} training observation removed. Thus we can view $\hat{f}_{m}^{\left(-i\right)}\left(x_{i}\right)$ as a `prediction' on $x_{i}$ (since $x_{i}$ was not used in training the learner). The second learner (denoted by $\hat{f}\left(z\right)$ is then trained over a dataset created by the first learners. Each observation of the second learner consists of the `leave-one-out' outputs of the first learners as its inputs, and the original label as its output. Concretely, denote the $i$\textsuperscript{th} training observation of the second learner by $\left(z_{i}. y_{i}\right)$, where
\begin{equation}
z_{i} = \left(\hat{f}_{1}^{\left(-i\right)}\left(x_{i}\right), \dots, \hat{f}_{M}^{\left(-i\right)}\left(x_{i}\right)\right)
\end{equation}
The leave-one-out approach is to prevent overfitting. Thus, a stacked prediction $\hat{h}\left(x\right)$ is given by $\hat{h}\left(x\right) = \hat{f}\left(z\right)$ where
\begin{equation}
z = \left(\hat{f}_{1}\left(x\right), \dots, \hat{f}_{M}\left(x\right)\right)
\end{equation}

\section{Decision Trees}

\subsection{Random Forests}

\section{Statistical Learning Theory}

\subsection{Vapnik-Chervonenkis Dimension}

\subsection{Radermacher Complexity}

\subsection{Probably Approximately Correct Learning}

\chapter{Statistical Signal Processing}

\section{Random Signals and Systems}

\subsection{Random Linear Time Invariant Systems}

Consider a linear time invariant (LTI) system in discrete time. For a sequence of inputs $u\left(k\right)$, there is a corresponding sequence of outputs $y\left(k\right)$. The linearity property means that for a superposition of inputs $u_{1}\left(k\right) + u_{2}\left(k\right)$, the output is also a superposition $y_{1}\left(k\right) + y_{2}\left(k\right)$. We can construct any input sequence with a superposition of Kronecker deltas
\begin{equation}
\delta_{ij} = \begin{cases} 1, & i = j \\ 0, & i\neq j\end{cases}
\end{equation}
like so for $\left\{\dots, u\left(0\right), u\left(1\right), u\left(2\right), \dots\right\} =  \left\{\dots, a_{0}, a_{1}, a_{2}, \dots\right\}$:
\begin{equation}
u\left(k\right) = \sum_{j = -\infty}^{\infty}a_{k}\delta_{jk} = a_{k}
\end{equation}
An impulse response $h\left(k\right)$ is defined as the output $y\left(k\right)$ for a input of a unit pulse at $k = 0$, ie. $u\left(k\right) = \delta_{k,0}$. Then for a `lagged' unit pulse input $u\left(k\right) = \delta_{k,1}$, the output will be $h\left(k - 1\right)$ because it is just the  impulse response lagged by $1$. By superposition, if we apply the input $u\left(k\right) = \delta_{k,0} + \delta_{k,1}$, the output should be $y\left(k\right) = h\left(k\right) + h\left(k - 1\right)$. Generalising, if the input is $u\left(k\right) = \sum_{j = -\infty}^{\infty}a_{k}\delta_{jk}$, then the output should be
\begin{align}
y\left(k\right) &= \dots + a_{0}h\left(k\right) + a_{1}h\left(k - 1\right) + \dots \\
&= \dots + u\left(0\right)h\left(k\right) + u\left(1\right)h\left(k - 1\right) + \dots \\
&= \sum_{j = -\infty}^{\infty}h\left(k - j\right)u\left(j\right)
\end{align}
which is the convolution sum. Note that we can rewrite this as
\begin{equation}
y\left(k\right) = \sum_{j = -\infty}^{\infty}h\left(j\right)u\left(k - j\right)
\end{equation}
because this would only change the sequence of summation. It follows that if $U\left(k\right)$ is a random sequence input into an LTI system, the random sequence output $Y\left(k\right)$ would be given by
\begin{equation}
Y\left(k\right) = \sum_{j = -\infty}^{\infty}h\left(j\right)U\left(k - j\right)
\end{equation}
Now if we considered a continuous time LTI system with impulse response $h\left(t\right)$, then by similar arguments (except considering Dirac deltas instead of Kronecker deltas and integrals instead of sums), for a stochastic process input $U\left(t\right)$ the stochastic process output $Y\left(t\right)$ is given by the convolution integral
\begin{equation}
Y\left(t\right) = \int_{-\infty}^{\infty}h\left(\tau\right)U\left(t - \tau\right)d\tau
\end{equation}
Suppose we have a sampled system, where we observe output $Y\left(t\right)$ at time instants separated by interval $T$, and thus define the random sequence $Y_{k} = Y\left(kT\right)$ for $k = \dots, 0, 1, \dots$. Also suppose the random sequence of inputs $U_{k}$ is held constant over the interval $T$ such that
\begin{equation}
U\left(t\right) = U_{k}
\end{equation}
for $kT \leq t < \left(k + 1\right)T$. Then for an LTI system defined by $g\left(t\right)$, the output at instant $t = kT$ is
\begin{align}
Y\left(kT\right) &= \int_{-\infty}^{\infty}h\left(\tau\right)U\left(kT - \tau\right)d\tau \\
&= \sum_{j = -\infty}^{\infty}\int_{\left(j - 1\right)T}^{jT}h\left(\tau\right)U\left(kT - \tau\right)d\tau
\end{align}
Note that by definition $U\left(kT - \tau\right) = U_{k - j}$ for $\left(j - 1\right)T < \tau \leq jT$ so
\begin{equation}
Y\left(kT\right) = \sum_{j = -\infty}^{\infty}\int_{\left(j - 1\right)T}^{jT}h\left(\tau\right)d\tau U_{k - j}
\end{equation}
Define $h_{j} = \int_{\left(j - 1\right)T}^{jT}h\left(\tau\right)d\tau$, then
\begin{equation}
Y_{k} = \sum_{j = -\infty}^{\infty}h_{j}U_{k - j}
\end{equation}
which is a convolution sum in terms of the discrete LTI system defined by $h_{k}$. \\

For a causal system, we should have $h_{k} = 0$ for $k < 0$ (and analogously $h\left(t\right) = 0$ for $t < 0$) otherwise this would imply a response has been `caused' by an input in the future. So a convolution sum can be written as
\begin{equation}
Y_{k} = \sum_{j = 0}^{\infty}h_{j}U_{k - j}
\end{equation}
Define the shift operator $z$ so that $z^{j}U_{k} := U_{k + j}$ and $z^{-j}U_{k} := U_{k - j}$. Thus
\begin{gather}
Y_{k} = \sum_{j = 0}^{\infty}h_{j}z^{-j}U_{k} \\
\dfrac{Y_{k}}{U_{k}} = \sum_{j = 0}^{\infty}h_{j}z^{-j}
\end{gather}
We define $H\left(z\right) := \dfrac{Y_{k}}{U_{k}} = \sum_{j = 0}^{\infty}h_{j}z^{-j}$ and this is the $z$-transform of $h_{k}$. Analogously, a continuous system with impulse response $h\left(t\right)$ has a transfer function which is the Laplace transform $H\left(s\right) = \mathcal{L}\left[h\left(t\right)\right]$. Note that we can also define `strictly causal' systems where $h_{k} = 0$ for $k \leq 0$ and $h\left(t\right) = 0$ for $t \leq 0$, then the transfer function (in the discrete case) becomes
\begin{equation}
H\left(z\right) = \sum_{j = 1}^{\infty}h_{j}z^{-j}
\end{equation}

\subsubsection{Wide Sense Stationarity in LTI Systems}

Suppose the input $X\left(t\right)$ to an LTI system with impulse response $h\left(t\right)$ is a wide sense stationary process. That is, $X\left(t\right)$ has mean function $\mu_{X}$ and autocorrelation function $R_{X}\left(\tau\right)$. Then the output $Y\left(t\right)$ will also be a wide sense stationary process with mean function given by
\begin{align}
\mu_{Y} &= \mathbb{E}\left[Y\left(t\right)\right] \\
&= \mathbb{E}\left[\int_{-\infty}^{\infty}h\left(u\right)X\left(t - u\right)du\right] \\
&= \int_{-\infty}^{\infty}h\left(u\right)\mathbb{E}\left[X\left(t - u\right)\right]du \\
&= \mu_{X}\int_{-\infty}^{\infty}h\left(u\right)du
\end{align}
and autocorrelation function given by
\begin{align}
R_{Y}\left(\tau\right) &= \mathbb{E}\left[Y\left(t\right)Y\left(t-\tau\right)\right] \\
&= \mathbb{E}\left[\int_{-\infty}^{\infty}h\left(u\right)X\left(t-u\right)du\int_{-\infty}^{\infty}h\left(v\right)X\left(t+\tau-v\right)dv\right] \\
&= \mathbb{E}\left[\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h\left(u\right)h\left(v\right)X\left(t-u\right)X\left(t+\tau-v\right)dudv\right] \\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h\left(u\right)h\left(v\right)\mathbb{E}\left[X\left(t-u\right)X\left(t+\tau-v\right)\right]dudv \\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h\left(u\right)h\left(v\right)R_{X}\left(\tau+u-v\right)dudv
\end{align}
The cross-correlation function between $X\left(t\right)$ and $Y\left(t\right)$ is given by
\begin{align}
R_{XY}\left(\tau\right) &= \mathbb{E}\left[X\left(t\right)Y\left(t-\tau\right)\right] \\
&= \mathbb{E}\left[X\left(t\right)\int_{-\infty}^{\infty}h\left(u\right)X\left(t+\tau-u\right)du\right] \\
&= \int_{-\infty}^{\infty}h\left(u\right)\mathbb{E}\left[X\left(t\right)X\left(t+\tau-u\right)\right]du \\
&= \int_{-\infty}^{\infty}h\left(u\right)R_{X}\left(\tau-u\right)du
\end{align}
and the autocorrelation function of $Y\left(t\right)$ can be related to the cross-correlation function by the substitution $w = -u$:
\begin{align}
R_{Y}\left(\tau\right) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h\left(u\right)h\left(v\right)R_{X}\left(\tau+u-v\right)dudv \\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h\left(-w\right)h\left(v\right)R_{X}\left(\tau-w-v\right)dwdv \\
&= \int_{-\infty}^{\infty}h\left(-w\right)\green\underbrace{\black\int_{-\infty}^{\infty}h\left(v\right)R_{X}\left(\tau-w-v\right)dv}_{\green R_{XY}\left(\tau-w\right)}\black dw \\
&= \int_{-\infty}^{\infty}h\left(-w\right)R_{XY}\left(\tau-w\right)dw
\end{align}
By the same arguments, if $X_{n}$ is a wide sense stationary random sequence and $h_{n}$ is the impulse response of a discrete LTI system, then the output sequence $Y_{n}$ is also a wide sense stationary random sequence which satisfies
\begin{gather}
\mu_{Y} = \mu_{X}\sum_{i = -\infty}^{\infty}h_{i} \\
R_{Y}\left(k\right) = \sum_{i = -\infty}^{\infty}\sum_{j = -\infty}^{\infty}h_{i}h_{j}R_{X}\left(k + i - j\right) \\
R_{XY}\left(k\right) = \sum_{i = -\infty}^{\infty}h_{i}R_{X}\left(k - i\right) \\
R_{Y}\left(k\right) = \sum_{i = -\infty}^{\infty}h_{-i}R_{XY}\left(k - i\right)
\end{gather}

\section{Power Spectral Density}

Consider sample functions $x\left(t\right)$ of a stationary process $X\left(t\right)$. The Fourier transform of $x\left(t\right)$ usually does not exist because it is not integrable on an infinite domain. However, if we define the truncated sample function
\begin{equation}
x_{T}\left(t\right) = \begin{cases} x\left(t\right), & -T\leq t\leq T \\ 0, & \mathrm{elsewhere}\end{cases}
\end{equation}
then this truncated sample function has a Fourier transform. Denote this Fourier transform by
\begin{align}
\widetilde{x}_{T}\left(f\right) &= \int_{-\infty}^{\infty}x\left(t\right)e^{j2\pi ft}dt \\
&= \int_{-T}^{T}x\left(t\right)e^{j2\pi ft}dt
\end{align}
where $j = \sqrt{-1}$. The power spectral density of $X\left(t\right)$ is a function of frequency $f$ which may be roughly interpreted as the average density of power of frequency content $f$ for sample functions of $X\left(t\right)$. It is defined by
\begin{align}
S_{X}\left(f\right) &= \lim_{T\to\infty}\dfrac{1}{2T}\mathbb{E}\left[\left|\widetilde{x}_{T}\left(f\right)\right|^{2}\right] \\
&= \lim_{T\to\infty}\dfrac{1}{2T}\mathbb{E}\left[\left|\int_{-T}^{T}x\left(t\right)e^{-j2\pi ft}dt\right|^{2}\right]
\end{align}
The power spectral density $S_{X}\left(\phi\right)$ of a wide-sense stationary random sequence $X_{n}$ is also defined similarly as
\begin{equation}
S_{X}\left(\phi\right) = \lim_{L\to\infty}\dfrac{1}{2L + 1}\mathbb{E}\left[\left|\sum_{n = -L}^{L}X_{n}e^{-j2\pi\phi n}\right|^{2}\right]
\end{equation}

\subsection{Wiener-Khintchine Theorem \cite{Yates2005}}

The Wiener-Khintchine theorem states that the autocorrelation function and power spectral density of a wide sense stationary stochastic process $X\left(t\right)$ are Fourier transform pairs. That is,
\begin{gather}
S_{X}\left(f\right) = \int_{-\infty}^{\infty}R_{X}\left(\tau\right)e^{-j2\pi f\tau}d\tau \\
R_{X}\left(\tau\right) = \int_{-\infty}^{\infty}S_{X}\left(f\right)e^{-j2\pi f\tau}df
\end{gather}
\begin{proof}
Begin by writing the expression for $\mathbb{E}\left[\left|\widetilde{x}_{T}\left(f\right)\right|^{2}\right]$.
\begin{equation}
\mathbb{E}\left[\left|\widetilde{x}_{T}\left(f\right)\right|^{2}\right] = \mathbb{E}\left[\left|\int_{-T}^{T}x\left(t\right)e^{-j2\pi ft}dt\right|^{2}\right]
\end{equation}
The Fourier transform $\widetilde{x}_{T}\left(f\right)$ is a complex valued function of frequency, but since $x\left(t\right)$ is real valued, then the complex conjugate only affects the complex exponential, ie. $\widetilde{x}_{T}^{*}\left(f\right) = \int_{-T}^{T}x\left(t\right)e^{j2\pi ft}dt$. Then use the fact that for complex numbers, $\left|\widetilde{x}_{T}\left(f\right)\right|^{2} = \widetilde{x}_{T}\left(f\right)\widetilde{x}_{T}^{*}\left(f\right)$. Hence
\begin{align}
\mathbb{E}\left[\left|\widetilde{x}_{T}\left(f\right)\right|^{2}\right] &= \mathbb{E}\left[\left(\int_{-T}^{T}x\left(t\right)e^{-j2\pi ft}dt\right)\left(\int_{-T}^{T}x\left(t'\right)e^{j2\pi ft'}dt'\right)\right] \\
&= \mathbb{E}\left[\int_{-T}^{T}\int_{-T}^{T}x\left(t\right)x\left(t'\right)e^{-j2\pi f\left(t-t'\right)}dtdt'\right] \\
&= \int_{-T}^{T}\int_{-T}^{T}\mathbb{E}\left[x\left(t\right)x\left(t'\right)\right]e^{-j2\pi f\left(t-t'\right)}dtdt' \\
&= \int_{-T}^{T}\int_{-T}^{T}R_{X}\left(t-t'\right)e^{-j2\pi f\left(t-t'\right)}dtdt'
\end{align}
Introduce the change of variables $\tau = t - t'$, noting that the integrand then only depends on $\tau$. To determine the limits of integration, we reason that if $-T\leq t \leq T$ and $-T \leq t'\leq T$, then $-2T \leq \tau \leq 2T$. Then a graphical sketch on the $t$-$t'$ plane of the region $\Omega$ for $t'$ given by the intersection
\begin{equation}
\Omega = \left\{t': t - t' = \tau \cap -T\leq t \leq T \cap -T \leq t'\leq T \cap -2T \leq \tau \leq 2T\right\}
\end{equation}
will reveal
\begin{equation}
\Omega = \left\{t': \begin{cases} -T \leq t' \leq T - \tau, & \tau \geq 0 \\ -T - \tau \leq t' \leq T, & \tau < 0\end{cases}\right\}
\end{equation}
Combining these two cases gives $\Omega = \left\{t': -T \leq t' \leq T - \left|\tau\right|\right\}$. Hence the integral can be written as
\begin{align}
\mathbb{E}\left[\left|\widetilde{x}_{T}\left(f\right)\right|^{2}\right] &= \int_{-2T}^{2T}\int_{-T}^{T-\left|\tau\right|}R_{X}\left(\tau\right)e^{-j2\pi f\tau}dt'd\tau \\
&= \int_{-2T}^{2T}\left(2T-\left|\tau\right|\right)R_{X}\left(\tau\right)e^{-j2\pi f\tau}d\tau
\end{align}
This can be visually imagined as integrating along the positive (upward sloping) diagonals for all intercepts $\tau$ in the box bounded by $-T\leq t \leq T$ and $-T \leq t'\leq T$. Then 
\begin{align}
\dfrac{1}{2T}\mathbb{E}\left[\left|\widetilde{x}_{T}\left(f\right)\right|^{2}\right] &= \dfrac{1}{2T}\int_{-2T}^{2T}\left(2T-\left|\tau\right|\right)R_{X}\left(\tau\right)e^{-j2\pi f\tau}d\tau \\
&= \int_{-2T}^{2T}\left(1-\dfrac{\left|\tau\right|}{2T}\right)R_{X}\left(\tau\right)e^{-j2\pi f\tau}d\tau
\end{align}
Define the function
\begin{equation}
h_{T}\left(\tau\right) = \begin{cases} 1 -\dfrac{\left|\tau\right|}{2T}, & \left|\tau\right| \leq 2T \\ 0, & \mathrm{elsewhere}\end{cases}
\end{equation}
This gives $\lim_{T\to\infty}h_{T}\left(\tau\right) = 1$. The power spectral density is therefore
\begin{align}
S_{X}\left(f\right) &= \lim_{T\to\infty}\dfrac{1}{2T}\mathbb{E}\left[\left|\widetilde{x}_{T}\left(f\right)\right|^{2}\right] \\
&= \lim_{T\to\infty}\int_{-2T}^{2T}\left(1-\dfrac{\left|\tau\right|}{2T}\right)R_{X}\left(\tau\right)e^{-j2\pi f\tau}d\tau \\
&= \int_{-\infty}^{\infty}\lim_{T\to\infty}\left(h_{T}\left(\tau\right)\right)R_{X}\left(\tau\right)e^{-j2\pi f\tau}d\tau \\
&= \int_{-\infty}^{\infty}R_{X}\left(\tau\right)e^{-j2\pi f\tau}d\tau
\end{align}
which is the Fourier transform of $R_{X}\left(\tau\right)$.
\end{proof}
Note that one may define average power of a stochastic process as $\mathbb{E}\left[X\left(t\right)^{2}\right] = R_{X}\left(0\right)$, in which case
\begin{equation}
R_{X}\left(0\right) = \int_{-\infty}^{\infty}S_{X}\left(f\right)df
\end{equation}
In this sense, the power spectral density can be viewed as taking the role of a density function. So for a given frequency $f$, the power spectral density gives an indication of the contribution of that frequency to average power.

\subsection{Discrete-Time Wiener-Khintchine Theorem}

The discrete-Ttme Wiener-Khintchine theorem is the discrete-time analogue to the Wiener-Khintchine theorem involving the connection between the autocorrelation and power spectral density of random sequences by the discrete-time Fourier transform. For a wide-sense stationary random sequence $X_{n}$, the autocorrelation function $R_{X}\left(k\right)$ and power spectral density $S_{X}\left(\phi\right)$ form a discrete-time Fourier transform pair:
\begin{gather}
S_{X}\left(\phi\right) = \sum_{k = -\infty}^{\infty}R_{X}\left(k\right)e^{-j2\pi\phi k} \\
R_{X}\left(k\right) = \int_{-1/2}^{1/2}S_{X}\left(\phi\right)e^{j2\pi\phi k}d\phi
\end{gather}
The proof is analogous to continuous time.
\begin{proof}
First write
\begin{align}
\mathbb{E}\left[\left|\sum_{n=-L}^{L}X_{n}e^{-j2\pi\phi n}\right|^{2}\right] &= \mathbb{E}\left[\left(\sum_{n=-L}^{L}X_{n}e^{-j2\pi\phi n}\right)\left(\sum_{n'=-L}^{L}X_{n'}e^{j2\pi\phi n'}\right)\right] \\
&= \mathbb{E}\left[\sum_{n=-L}^{L}\sum_{n'=-L}^{L}X_{n}X_{n'}e^{-j2\pi\phi\left(n-n'\right)}\right] \\
&= \sum_{n=-L}^{L}\sum_{n'=-L}^{L}\mathbb{E}\left[X_{n}X_{n'}\right]e^{-j2\pi\phi\left(n-n'\right)} \\
&= \sum_{n=-L}^{L}\sum_{n'=-L}^{L}R_{X}\left(n-n'\right)e^{-j2\pi\phi\left(n-n'\right)}
\end{align}
Analogous to continuous time, introduce a change of variables $\eta = n - n'$ so that
\begin{align}
\mathbb{E}\left[\left|\sum_{n=-L}^{L}X_{n}e^{-j2\pi\phi n}\right|^{2}\right] &= \sum_{\eta=-2L}^{2L}\sum_{n'=-L}^{L-\left|\eta\right|}R_{X}\left(\eta\right)e^{-j2\pi\phi\eta} \\
&= \sum_{\eta=-2L}^{2L}\left(2L+1-\left|\eta\right|\right)R_{X}\left(\eta\right)e^{-j2\pi\phi\eta}
\end{align}
noting that $\sum_{n'=-L}^{L-\left|\eta\right|} = 2L + 1 - \left|\eta\right|$. So now
\begin{equation}
\dfrac{1}{2L+1}\mathbb{E}\left[\left|\sum_{n=-L}^{L}X_{n}e^{-j2\pi\phi n}\right|^{2}\right]=\sum_{\eta=-2L}^{2L}\left(1-\dfrac{\left|\eta\right|}{2L+1}\right)R_{X}\left(\eta\right)e^{-j2\pi\phi\eta}
\end{equation}
Introduce the function
\begin{equation}
h_{L}\left(\eta\right)= \begin{cases} 1-\dfrac{\left|\eta\right|}{2L+1}, & \left|\eta\right| \leq 2L + 1 \\ 0, & \mathrm{elsewhere}\end{cases}
\end{equation}
where we have that $\lim_{L\to\infty}h_{L}\left(\eta\right) = 1$. Then
\begin{align}
S_{X}\left(\phi\right) &= \lim_{L\to\infty}\dfrac{1}{2L+1}\mathbb{E}\left[\left|\sum_{n=-L}^{L}X_{n}e^{-j2\pi\phi n}\right|^{2}\right] \\
&= \lim_{L\to\infty}\sum_{\eta=-2L}^{2L}h_{L}\left(\eta\right)R_{X}\left(\eta\right)e^{-j2\pi\phi\eta} \\
&= \sum_{\eta=-\infty}^{\infty}R_{X}\left(\eta\right)e^{-j2\pi\phi\eta}
\end{align}
which is the discrete-time Fourier transform of the autocorrelation function of $X_{n}$, $R_{X}\left(\eta\right)$.
\end{proof}

\subsection{Cross Spectral Density}

The cross spectral density $S_{XY}\left(f\right)$ of two jointly wide-sense stationary processes $X\left(t\right)$ and $Y\left(t\right)$ is the Fourier transform of the cross-correlation $R_{XY}\left(\tau\right)$.
\begin{equation}
S_{XY}\left(f\right) = \int_{-\infty}^{\infty}R_{XY}\left(\tau\right)e^{-j2\pi f\tau}d\tau
\end{equation}

The cross spectral density $S_{XY}\left(\phi\right)$ of two jointly wide-sense random sequences $X_{n}$ and $Y_{n}$ is the Fourier transform of the cross-correlation $R_{XY}\left(k\right)$.
\begin{equation}
S_{XY}\left(\phi\right) = \sum_{k = -\infty}^{\infty}R_{XY}\left(\tau\right)e^{-j2\pi \phi k}
\end{equation}

\subsection{Coloured Noise}

\subsection{Spectral Factorisation Theorem \cite{Astrom1970}}

\section{Spectral Density Estimation}

\section{Linear Filtering}

\section{Wiener-Kolgomorov Filtering}

\section{Kalman Filtering}

The Kalman filter is a special case of the Bayes filter when the dynamic model is linear and the noise is Gaussian.

\subsection{Linearised Kalman Filter}

\subsection{Extended Kalman Filter}

\subsection{Unscented Kalman Filter}

\subsection{Kalman-Bucy Filter}

\section{Kalman Smoother}

\section{Viterbi Algorithm}

\section{Particle Filtering}

\section{Wavelets}

\chapter{Stochastic Control}

\section{System Identification}

\subsection{Identification of Linear State Space Systems}

\subsection{Subspace Identification}

\section{Linear Quadratic Gaussian Control}

\section{Stochastic Model Predictive Control}

\section{Stochastic Stability}

\section{Reinforcement Learning}

\chapter{Quantitative Finance}

\section{Portfolio Optimisation}

\subsection{Kelly Criterion}

Suppose an opportunity to place a bet yields $b$ net odds (ie. for every \$1 wagered, a win results in $1 + b$ gross return, while a loss results in no return). The probability of winning the bet is given by $p$. The Kelly Criterion investigates the optimal betting amount for this bet. Assume the bettor has a log utility function of wealth, given by
\begin{equation}
u\left(W\right) = \log W
\end{equation}
The Kelly Criterion maximises the expected utility (hence expected log wealth) from an initial wealth $W_{0}$ and a wager $w$:
\begin{equation}
\mathbb{E}\left[u\left(W\right)\right] = p\log\left(W_{0} + wb\right) + \left(1 - p\right)\log\left(W_{0} - w\right)
\end{equation}
For further simplicity, denote the betting fraction $f = w/W_{0}$ so that
\begin{equation}
\mathbb{E}\left[u\left(W\right)\right] = \dfrac{p\log\left(1 + fb\right) + \left(1 - p\right)\log\left(1 - f\right)}{W_{0}}
\end{equation}
We find the optimal betting fraction $f^{*}$ to maximise expected utility. Note that the expected utility is a concave function. Taking the derivative yields
\begin{equation}
\dfrac{\partial \mathbb{E}\left[u\left(W\right)\right]}{\partial f} = \dfrac{1}{W_{0}}\left(\dfrac{pb}{1 + fb} + \dfrac{p - 1}{1 - f}\right)
\end{equation}
Setting this to zero gives
\begin{gather}
\dfrac{1}{W_{0}}\left(\dfrac{pb}{1 + f^{*}b} + \dfrac{p - 1}{1 - f^{*}}\right) = 0 \\
pb\left(1 - f^{*}\right) = \left(1 + f^{*}b\right)\left(1 - p\right) \\
f^{*} = \dfrac{pb + p - 1}{b}
\end{gather}
Letting $q := 1 - p$, we rewrite this as
\begin{equation}
f^{*} = \dfrac{pb - q}{b}
\end{equation}

\subsection{Markowitz Portfolio Theory}

\section{Black-Scholes Model}

\section{Optimal Stopping}

\section{Ruin Theory}

\chapter{Statistical Physics}

\section{Maxwell-Boltzmann Distribution}

\section{Mean Sojourn Time}

\section{Mean Field Theory}

\section{Fokker-Planck Equations}

%\nocite{*} % adds all entries in the bib file to the bibliography
\bibliography{prob_stats_references}{}
\bibliographystyle{plain}

\end{document}

%\begin{figure}[H]
%\includegraphics[width=1\textwidth]{figures/pred_int_all}\centering
%\caption{A prediction interval is constructed about $\hat{\operatorname{E}}\left[Y\middle|X = x_{d}\right]$ which does contain the new observed value of $y$.}
%\end{figure}
